{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#| default_exp core"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1660686998218
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\r\n",
        "from pathlib import Path\r\n",
        "import tempfile\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "from Semi_ATE import STDF\r\n",
        "\r\n",
        "from fastcore.basics import *"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1660686998570
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| hide\r\n",
        "from nbdev import show_doc"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1660686999188
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities\r\n",
        "\r\n",
        "> utility / helper functions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\r\n",
        "def write_to_file(content, mode='wb', filename='tmp'):\r\n",
        "    dest = Path(tempfile.mkdtemp())\r\n",
        "    with open(dest / filename, mode) as f:\r\n",
        "        f.write(content)\r\n",
        "\r\n",
        "    return dest / filename\r\n",
        "\r\n",
        "def isNotEmpty(r):\r\n",
        "    return len(r) != 0"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1660686999269
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augment existing STDF PTR class\r\n",
        "> These functions are patched to STDF.PTR class to add more functionalities"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\r\n",
        "@patch(as_prop=True)\r\n",
        "def test_type(self: STDF.PTR):\r\n",
        "    return self.fields['TEST_TXT']['Value']\r\n",
        "\r\n",
        "@patch(as_prop=True)\r\n",
        "def test_res(self: STDF.PTR):\r\n",
        "    return self.fields['RESULT']['Value']\r\n",
        "\r\n",
        "@patch(as_prop=True)\r\n",
        "def test_fail_u(self: STDF.PTR):\r\n",
        "    return self.fields['RESULT']['Value'] > self.fields['HI_LIMIT']['Value']\r\n",
        "\r\n",
        "@patch(as_prop=True)\r\n",
        "def test(self: STDF.PTR):\r\n",
        "    return {k: v['Value'] for k, v in self.fields.items()}\r\n",
        "\r\n",
        "@patch(as_prop=True)\r\n",
        "def isEfuse(self: STDF.PTR):\r\n",
        "    return 'eFuse' in self.test_type\r\n",
        "\r\n",
        "@patch(as_prop=True)\r\n",
        "def isX(self: STDF.PTR):\r\n",
        "    return self.isEfuse and ('X_coord' in self.test_type)\r\n",
        "\r\n",
        "@patch(as_prop=True)\r\n",
        "def isW(self: STDF.PTR):\r\n",
        "    return self.isEfuse and ('Wafer_number' in self.test_type)\r\n",
        "\r\n",
        "@patch(as_prop=True)\r\n",
        "def isY(self: STDF.PTR):\r\n",
        "    return self.isEfuse and ('Y_coord' in self.test_type)\r\n",
        "\r\n",
        "@patch(as_prop=True)\r\n",
        "def isFT_checksum(self: STDF.PTR):\r\n",
        "    return self.isEfuse and ('FT_checksum'  in self.test_type)\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1660686999388
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WXY Parser\n",
        "\n",
        "> parse missing wxy from STDF PTR records"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\r\n",
        "class Stdf:\r\n",
        "    \"Stdf content struct\"\r\n",
        "    def __init__(self, \r\n",
        "    file_content=None, #binary file content of stdf file\r\n",
        "    file_name=None): #binary file name of stdf file\r\n",
        "        self._file_content = file_content\r\n",
        "        self._file_name = file_name\r\n",
        "\r\n",
        "    @property\r\n",
        "    def file_content(self):\r\n",
        "        return self._file_content\r\n",
        "\r\n",
        "    @property\r\n",
        "    def file_name(self):\r\n",
        "        return self._file_name\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1660686999524
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_doc(Stdf)\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "<nbdev.showdoc.BasicMarkdownRenderer>",
            "text/markdown": "---\n\n### Stdf\n\n>      Stdf (file_content=None, file_name=None)\n\nStdf content struct\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| file_content | NoneType | None | binary file content of stdf file |\n| file_name | NoneType | None | binary file name of stdf file |"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1660686999659
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\r\n",
        "class Content:\r\n",
        "    \"for extracting wxy\"\r\n",
        "    def __init__(self, \r\n",
        "        stdf=None, # stdf data as Stdf class\r\n",
        "        csv=None): # csv data as Csv class (todo)\r\n",
        "        self._stdf = stdf\r\n",
        "        self._csv = csv\r\n",
        "\r\n",
        "    @property\r\n",
        "    def wxy(self) -> pd.DataFrame:\r\n",
        "        self._f = write_to_file(self._stdf.file_content, filename=self._stdf.file_name)\r\n",
        "        self._parse_records()\r\n",
        "        self._extract_wxy()\r\n",
        "        self._add_cols()\r\n",
        "        return self._dfs\r\n",
        "\r\n",
        "    @property\r\n",
        "    def csv(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "    def _parse_records(self, record_type=STDF.PTR):\r\n",
        "        self._recs = [r for r in STDF.records_from_file(str(self._f)) if isinstance(r, record_type)]\r\n",
        "\r\n",
        "    def _extract_wxy(self):\r\n",
        "        res,N,isHydrated = [],len(self._recs),[]\r\n",
        "        cache,m ={},0\r\n",
        "        dfs = []\r\n",
        "\r\n",
        "        def updateCache():\r\n",
        "            if r.isW: cache['w'] = int(r.test_res)\r\n",
        "            if r.isX: cache['x'] = int(r.test_res)\r\n",
        "            if r.isY: cache['y'] = int(r.test_res)\r\n",
        "\r\n",
        "        for indx,r in enumerate(self._recs[::-1]): #begin from the end\r\n",
        "            res.append((self._f.stem, indx, r.test_type, r.test['RESULT'], r.test['LO_LIMIT'], r.test['HI_LIMIT']))\r\n",
        "            updateCache()\r\n",
        "            if (r.isFT_checksum or indx == N-1) and isNotEmpty(cache):\r\n",
        "                s = slice(m, indx + 1) if indx == N-1 else slice(m, indx) #if last item (reading backwards)\r\n",
        "                wxy = cache['w'], cache['x'], cache['y']\r\n",
        "                res[s] = [_r + wxy for _r in res[s]]\r\n",
        "                cache,m = {},indx\r\n",
        "                isHydrated.append(True)\r\n",
        "\r\n",
        "        if any(isHydrated):\r\n",
        "            columns = ['filename', 'indx', 'test_name','test_value',\r\n",
        "                    'test_lo_limit', 'test_hi_limit', 'wafer_number', 'x_coord', 'y_coord']\r\n",
        "            dfs.append(pd.DataFrame(res, columns=columns) )\r\n",
        "\r\n",
        "        self._dfs = pd.concat(dfs)\r\n",
        "\r\n",
        "\r\n",
        "    def _add_cols(self):\r\n",
        "        def extract():\r\n",
        "            qs_smt = r'.+(SMT\\d).+'  # '\\d' matches only single digit\r\n",
        "            qs_lot = r'.*(K4.+GF).+' # '*' means could be nothing\r\n",
        "            qs_date = r'.+_(220[5-7][0-9]{2}).+' ##very specific date search 2205... or 2206...\r\n",
        "            qs_time = r'.+_220[5-7][0-9]{2}_?([0-9]{6}).+' #'?'=='optional'\r\n",
        "\r\n",
        "            self._dfs['test_insertion'] = self._dfs.filename.str.extract(qs_smt).values\r\n",
        "            self._dfs['lot'] = self._dfs.filename.str.extract(qs_lot).values\r\n",
        "            self._dfs['date'] = self._dfs.filename.str.extract(qs_date).values\r\n",
        "            self._dfs['time'] = self._dfs.filename.str.extract(qs_time).values\r\n",
        "\r\n",
        "        def gen_cols():\r\n",
        "            self._dfs['test_value_minus_test_hi_limit'] = self._dfs['test_value'] - self._dfs['test_hi_limit']\r\n",
        "            self._dfs['test_lo_limit_minus_test_value'] = self._dfs['test_lo_limit'] - self._dfs['test_value']\r\n",
        "            self._dfs['wxy'] = self._dfs['wafer_number'].astype('str') + \\\r\n",
        "                    '/' + self._dfs['x_coord'].astype('str') + '/' + self._dfs['y_coord'].astype('str')\r\n",
        "\r\n",
        "        extract()\r\n",
        "        gen_cols()"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1660686999718
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_doc(Content)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "<nbdev.showdoc.BasicMarkdownRenderer>",
            "text/markdown": "---\n\n### Content\n\n>      Content (stdf=None, csv=None)\n\nfor extracting wxy\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| stdf | NoneType | None | stdf data as Stdf class |\n| csv | NoneType | None | csv data as Csv class (todo) |"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1660686999833
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#| hide\r\n",
        "# Test\r\n",
        "> Test parsing functionality for w,x,y"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#| hide\r\n",
        "def read_from_file(file):\r\n",
        "    with open(file, 'rb') as f:\r\n",
        "        data = f.read()\r\n",
        "    return data\r\n",
        "\r\n",
        "f_test = Path('./testdata')\r\n",
        "file_content = read_from_file(f_test)\r\n",
        "file_name = f_test.stem\r\n"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1660687000147
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| hide\r\n",
        "stdf=Stdf(file_content=file_content, file_name=file_name)\r\n",
        "c = Content(stdf=stdf)\r\n",
        "#c.wxy\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1660687000238
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}