{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCL Performance\n",
    "This notebook focuses on performance aspects of ACCL primitives and collectives. We can run the subsequent cells against an emulator or simulator session but hardware is recommended.\n",
    "\n",
    "There are several factors influencing the duration of an ACCL API call:\n",
    "* the complexity of a call - a copy will be faster than an all-reduce for example\n",
    "* the size (in bytes) of communicated buffers and their location in the memory hierarchy\n",
    "* memory contention between sending and receiving processes. ACCL can be configured in specific ways to minimize this contention\n",
    "* use of blocking or non-blocking variants of the API calls\n",
    "* network performance, which in itself might depend on the size of buffers i.e. very small buffers typically lead to low utilization of Ethernet bandwidth\n",
    "\n",
    "Factors which should not influence runtime are:\n",
    "* data type - API calls on buffers of the same byte size should take the same amount of time, even if the buffers themselves differ in datatype and number of elements \n",
    "* use of compression - ACCL is designed to perform compression at network rate\n",
    "\n",
    "Let's initialize a few ACCL instances and explore two performance-related aspects of the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaccl import accl\n",
    "import numpy as np\n",
    "\n",
    "RUN_ON_HARDWARE = False\n",
    "XCLBIN = \"axis3x.xclbin\"\n",
    "\n",
    "if RUN_ON_HARDWARE:\n",
    "    WORLD_SIZE = 3\n",
    "    RXBUF_SIZE = 16*1024*1024\n",
    "else:\n",
    "    WORLD_SIZE = 4\n",
    "    RXBUF_SIZE = 16*1024\n",
    "\n",
    "assert not RUN_ON_HARDWARE or WORLD_SIZE <= 3\n",
    "\n",
    "accl_instances = []\n",
    "for i in range(WORLD_SIZE):\n",
    "    if RUN_ON_HARDWARE:\n",
    "        accl_instances.append(accl(WORLD_SIZE, i, bufsize=RXBUF_SIZE, xclbin=XCLBIN, cclo_idx=i))\n",
    "    else:\n",
    "        accl_instances.append(accl(WORLD_SIZE, i, bufsize=RXBUF_SIZE, sim_mode=True))\n",
    "\n",
    "def allocate_in_all(count, dtype=np.float32):\n",
    "    op0_buffers = []\n",
    "    op1_buffers = []\n",
    "    res_buffers = []\n",
    "    for i in range(WORLD_SIZE):    \n",
    "        op0_buffers.append(accl_instances[i].allocate((count,)))\n",
    "        op1_buffers.append(accl_instances[i].allocate((count,)))\n",
    "        res_buffers.append(accl_instances[i].allocate((count,)))\n",
    "        op0_buffers[i][:] = [1.0*i for i in range(count)]\n",
    "        op1_buffers[i][:] = [1.0*i for i in range(count)]\n",
    "    return op0_buffers, op1_buffers, res_buffers\n",
    "\n",
    "op0_buf, op1_buf, res_buf = allocate_in_all(RXBUF_SIZE)\n",
    "op0_buf_fp16, op1_buf_fp16, res_buf_fp16 = allocate_in_all(RXBUF_SIZE, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host vs. FPGA buffers\n",
    "\n",
    "Every ACCL primitive or collective assumes your source and destination buffers are in host memory, unless otherwise specified with the `from_fpga` and `to_fpga` optional arguments that most PyACCL calls take. As such, before the operation is initiated, the source data is moved to the FPGA device memory, and after it completes, the resulting data is moved back to host memory. These copies have a performance overhead which typically depends on the size of copied buffers. \n",
    "\n",
    "Let's start by profiling the execution of the copy, the simplest primitive. We will measure across a range of buffer sizes. Feel free to change the `timeit` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 4 -n 10 accl_instances[0].copy(op0_buf[0], res_buf[0], 1)\n",
    "%timeit -r 4 -n 10 accl_instances[0].copy(op0_buf[0], res_buf[0], 1024/4)\n",
    "%timeit -r 4 -n 10 accl_instances[0].copy(op0_buf[0], res_buf[0], RXBUF_SIZE/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the runtime increases with larger message sizes, however it does so from quite a high baseline, caused the by the time required to copy the buffers between host and FPGA memory. However, in many applications the data might have been produced on the FPGA itself, or is subsequently required on the FPGA, and therefore does not require copying to the host. Let's see how the runtime changes if we work on FPGA memory directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 4 -n 10 accl_instances[0].copy(op0_buf[0], res_buf[0], 1, from_fpga=True, to_fpga=True)\n",
    "%timeit -r 4 -n 10 accl_instances[0].copy(op0_buf[0], res_buf[0], 1024/4, from_fpga=True, to_fpga=True)\n",
    "%timeit -r 4 -n 10 accl_instances[0].copy(op0_buf[0], res_buf[0], RXBUF_SIZE/4, from_fpga=True, to_fpga=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if the data type affects runtime (it shouldn't). We'll run the same copy operations again, from FPGA memory, but this time on identically sized FP16 buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 4 -n 10 accl_instances[0].copy(op0_buf_fp16[0], res_buf_fp16[0], 2, from_fpga=True, to_fpga=True)\n",
    "%timeit -r 4 -n 10 accl_instances[0].copy(op0_buf_fp16[0], res_buf_fp16[0], 1024/2, from_fpga=True, to_fpga=True)\n",
    "%timeit -r 4 -n 10 accl_instances[0].copy(op0_buf_fp16[0], res_buf_fp16[0], RXBUF_SIZE/2, from_fpga=True, to_fpga=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous calls\n",
    "Some PyACCL calls take the `async` optional argument. If this is set to true, the function call immediately returns a handle to a Python future object which can be waited on to determine if the processing has actually finished. This enables the program to continue processing on the host while the ACCL call is being executed in the FPGA.\n",
    "\n",
    "We can experiment with this feature by emulating host-side work with calls to `time.sleep()`. As long as the call to ACCL takes longer than the call to `sleep()`, the sleep will be completely hidden behind the ACCL call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def overlap_computation(count):\n",
    "    handle = accl_instances[0].copy(op0_buf[0], res_buf[0], count, from_fpga=True, to_fpga=True, run_async=True)\n",
    "    time.sleep(0.1)\n",
    "    handle.wait()\n",
    "\n",
    "%timeit -r 4 -n 10 overlap_computation(1)\n",
    "%timeit -r 4 -n 10 overlap_computation(1024/4)\n",
    "%timeit -r 4 -n 10 overlap_computation(RXBUF_SIZE/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De-Initialize ACCL instances\n",
    "The `deinit()` function clears all internal data structures in the ACCL instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(WORLD_SIZE):\n",
    "    accl_instances[i].deinit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
