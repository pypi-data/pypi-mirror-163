Metadata-Version: 2.1
Name: vod-tudelft
Version: 1.0.3
Summary: View of Delft dataset devkit
Author: Balazs Szekeres
Author-email: <b.szekeres@student.tudelft.nl>
Keywords: python,video,ai,lidar,radar,camera
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: Unix
Classifier: Operating System :: MacOS :: MacOS X
Classifier: Operating System :: Microsoft :: Windows
Description-Content-Type: text/markdown
Requires-Dist: numpy
Requires-Dist: matplotlib
Requires-Dist: k3d


# The View-of-Delft dataset
<div align="center">
<figure>
<img src="figures/example_frame_2.png" alt="Prius sensor setup" width="800"/>
<figcaption align = "center"><b>Example frame from our dataset with camera, LiDAR, 3+1D radar, and annotations overlaid.</b></figcaption>
<\figure>
</div>

 We present the novel View-of-Delft (VoD) automotive dataset. It contains 10.000 frames of synchronized and calibrated 64-layer LiDAR-, (stereo) camera-, and 3+1D radar-data acquired in complex, urban traffic. It consists of more than 76000 3D bounding box annotations, including more than 15000 pedestrian, 7100 cyclist and 17000 car labels.

## Sensors and data
The LiDAR sensor is a Velodyne 64 operating at 10 Hz. The camera provides colored images of 1936 × 1216 pixels at around 12 Hz. The horizontal field of view is ~64° (± 32°), vertical field of view is ~ 44° (± 22°). Odometry is a filtered combination of several inputs: RTK GPS, IMU, and wheel odometry with a frame rate around 100 Hz. Odometry is given relative to the starting position at the beginning of the current sequence, not in absolute coordinates. Note that while camera and odometry operate at a higher frequency than the LiDAR sensor, timestamps of the LiDAR sensor were chosen as “lead”, and we provide the closest camera frame and odometry information available. 

The provided LiDAR point-clouds are already ego-motion compensated. More specifically, misalignment of LiDAR and camera data originating from ego-motion during the scan (i.e. one full rotation of the LiDAR sensor) and ego-motion between the capture of LiDAR and camera data has been compensated. See an example frame for our data with calibrated and ego-motion compensated LiDAR point-cloud overlaid on the camera image. 

We also provide intrinsic calibration for the camera and extrinsic calibration of all three sensors in a format specified by the annotating party.

<img src="figures/Prius_sensor_setup_5.png" alt="Prius sensor setup" width="800"/>

## Introduction

<img src="figures/sensors.gif" alt="Prius sensor setup" width="600"/>

<img src="figures/labels.gif" alt="Prius sensor setup" width="600"/>

## Goal 

## Use-Cases

## Annotation intstructions
