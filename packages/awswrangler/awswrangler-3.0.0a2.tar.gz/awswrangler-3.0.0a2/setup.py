# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['awswrangler',
 'awswrangler.athena',
 'awswrangler.catalog',
 'awswrangler.data_api',
 'awswrangler.distributed',
 'awswrangler.distributed.datasources',
 'awswrangler.dynamodb',
 'awswrangler.lakeformation',
 'awswrangler.neptune',
 'awswrangler.opensearch',
 'awswrangler.quicksight',
 'awswrangler.s3']

package_data = \
{'': ['*']}

install_requires = \
['backoff>=2.0.0,<3.0.0',
 'boto3>=1.24.7,<2.0.0',
 'botocore>=1.27.7,<2.0.0',
 'gremlinpython>=3.5.2,<4.0.0',
 'jsonpath-ng>=1.5.3,<2.0.0',
 'numpy>=1.22.4,<2.0.0',
 'openpyxl>=3.0.0,<3.1.0',
 'opensearch-py>=2.0.0,<3.0.0',
 'pandas>=1.4.2,<2.0.0',
 'pg8000>=1.29.0,<2.0.0',
 'progressbar2>=4.0.0,<5.0.0',
 'pyarrow>=6.0.0,<7.0.0',
 'pymysql>=1.0.0,<2.0.0',
 'redshift-connector>=2.0.889,<2.1.0',
 'requests-aws4auth>=1.1.1,<2.0.0']

extras_require = \
{'distributed': ['modin>=0.14.0,<0.15.0',
                 'ray[default,data]>=1.13.0,<2.0.0',
                 'psutil>=5.9.0,<6.0.0',
                 'tqdm>=4.64.0,<5.0.0'],
 'oracle': ['oracledb>=1.0.0,<1.1.0'],
 'sparql': ['SPARQLWrapper>=2.0.0,<3.0.0'],
 'sqlserver': ['pyodbc>=4.0.32,<4.1.0']}

setup_kwargs = {
    'name': 'awswrangler',
    'version': '3.0.0a2',
    'description': 'Pandas on AWS.',
    'long_description': '# AWS Data Wrangler\n\n*Pandas on AWS*\n\nEasy integration with Athena, Glue, Redshift, Timestream, OpenSearch, Neptune, QuickSight, Chime, CloudWatchLogs, DynamoDB, EMR, SecretManager, PostgreSQL, MySQL, SQLServer and S3 (Parquet, CSV, JSON and EXCEL).\n\n![AWS Data Wrangler](docs/source/_static/logo2.png?raw=true "AWS Data Wrangler")\n![tracker](https://d3tiqpr4kkkomd.cloudfront.net/img/pixel.png?asset=GVOYN2BOOQ573LTVIHEW)\n\n> An [AWS Professional Service](https://aws.amazon.com/professional-services/) open source initiative | aws-proserve-opensource@amazon.com\n\n[![Release](https://img.shields.io/badge/release-3.0.0a2-brightgreen.svg)](https://pypi.org/project/awswrangler/)\n[![Python Version](https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9%20%7C%203.10-brightgreen.svg)](https://anaconda.org/conda-forge/awswrangler)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n\n[![Checked with mypy](http://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)\n[![Coverage](https://img.shields.io/badge/coverage-91%25-brightgreen.svg)](https://pypi.org/project/awswrangler/)\n![Static Checking](https://github.com/awslabs/aws-data-wrangler/workflows/Static%20Checking/badge.svg?branch=main)\n[![Documentation Status](https://readthedocs.org/projects/aws-data-wrangler/badge/?version=latest)](https://aws-data-wrangler.readthedocs.io/?badge=latest)\n\n| Source | Downloads | Installation Command |\n|--------|-----------|----------------------|\n| **[PyPi](https://pypi.org/project/awswrangler/)**  | [![PyPI Downloads](https://pepy.tech/badge/awswrangler)](https://pypi.org/project/awswrangler/) | `pip install awswrangler` |\n| **[Conda](https://anaconda.org/conda-forge/awswrangler)** | [![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/awswrangler.svg)](https://anaconda.org/conda-forge/awswrangler) | `conda install -c conda-forge awswrangler` |\n\n> ⚠️ **For platforms without PyArrow 3 support (e.g. [EMR](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html#emr-cluster), [Glue PySpark Job](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html#aws-glue-pyspark-jobs), MWAA):**<br>\n➡️ `pip install pyarrow==2 awswrangler`\n\nPowered By [<img src="https://arrow.apache.org/img/arrow.png" width="200">](https://arrow.apache.org/powered_by/)\n\n## Table of contents\n\n- [Quick Start](#quick-start)\n- [Read The Docs](#read-the-docs)\n- [Getting Help](#getting-help)\n- [Community Resources](#community-resources)\n- [Logging](#logging)\n- [Who uses AWS Data Wrangler?](#who-uses-aws-data-wrangler)\n- [What is Amazon Sagemaker Data Wrangler?](#what-is-amazon-sageMaker-data-wrangler)\n\n## Quick Start\n\nInstallation command: `pip install awswrangler`\n\n> ⚠️ **For platforms without PyArrow 3 support (e.g. [EMR](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html#emr-cluster), [Glue PySpark Job](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html#aws-glue-pyspark-jobs), MWAA):**<br>\n➡️`pip install pyarrow==2 awswrangler`\n\n```py3\nimport awswrangler as wr\nimport pandas as pd\nfrom datetime import datetime\n\ndf = pd.DataFrame({"id": [1, 2], "value": ["foo", "boo"]})\n\n# Storing data on Data Lake\nwr.s3.to_parquet(\n    df=df,\n    path="s3://bucket/dataset/",\n    dataset=True,\n    database="my_db",\n    table="my_table"\n)\n\n# Retrieving the data directly from Amazon S3\ndf = wr.s3.read_parquet("s3://bucket/dataset/", dataset=True)\n\n# Retrieving the data from Amazon Athena\ndf = wr.athena.read_sql_query("SELECT * FROM my_table", database="my_db")\n\n# Get a Redshift connection from Glue Catalog and retrieving data from Redshift Spectrum\ncon = wr.redshift.connect("my-glue-connection")\ndf = wr.redshift.read_sql_query("SELECT * FROM external_schema.my_table", con=con)\ncon.close()\n\n# Amazon Timestream Write\ndf = pd.DataFrame({\n    "time": [datetime.now(), datetime.now()],   \n    "my_dimension": ["foo", "boo"],\n    "measure": [1.0, 1.1],\n})\nrejected_records = wr.timestream.write(df,\n    database="sampleDB",\n    table="sampleTable",\n    time_col="time",\n    measure_col="measure",\n    dimensions_cols=["my_dimension"],\n)\n\n# Amazon Timestream Query\nwr.timestream.query("""\nSELECT time, measure_value::double, my_dimension\nFROM "sampleDB"."sampleTable" ORDER BY time DESC LIMIT 3\n""")\n\n```\n\n## [Read The Docs](https://aws-data-wrangler.readthedocs.io/)\n\n- [**What is AWS Data Wrangler?**](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/what.html)\n- [**Install**](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html)\n  - [PyPi (pip)](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html#pypi-pip)\n  - [Conda](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html#conda)\n  - [AWS Lambda Layer](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html#aws-lambda-layer)\n  - [AWS Glue Python Shell Jobs](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html#aws-glue-python-shell-jobs)\n  - [AWS Glue PySpark Jobs](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html#aws-glue-pyspark-jobs)\n  - [Amazon SageMaker Notebook](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html#amazon-sagemaker-notebook)\n  - [Amazon SageMaker Notebook Lifecycle](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html#amazon-sagemaker-notebook-lifecycle)\n  - [EMR](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html#emr)\n  - [From source](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/install.html#from-source)\n- [**Tutorials**](https://github.com/awslabs/aws-data-wrangler/tree/main/tutorials)\n  - [001 - Introduction](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/001%20-%20Introduction.ipynb)\n  - [002 - Sessions](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/002%20-%20Sessions.ipynb)\n  - [003 - Amazon S3](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/003%20-%20Amazon%20S3.ipynb)\n  - [004 - Parquet Datasets](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/004%20-%20Parquet%20Datasets.ipynb)\n  - [005 - Glue Catalog](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/005%20-%20Glue%20Catalog.ipynb)\n  - [006 - Amazon Athena](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/006%20-%20Amazon%20Athena.ipynb)\n  - [007 - Databases (Redshift, MySQL, PostgreSQL, SQL Server and Oracle)](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/007%20-%20Redshift%2C%20MySQL%2C%20PostgreSQL%2C%20SQL%20Server%2C%20Oracle.ipynb)\n  - [008 - Redshift - Copy & Unload.ipynb](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/008%20-%20Redshift%20-%20Copy%20%26%20Unload.ipynb)\n  - [009 - Redshift - Append, Overwrite and Upsert](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/009%20-%20Redshift%20-%20Append%2C%20Overwrite%2C%20Upsert.ipynb)\n  - [010 - Parquet Crawler](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/010%20-%20Parquet%20Crawler.ipynb)\n  - [011 - CSV Datasets](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/011%20-%20CSV%20Datasets.ipynb)\n  - [012 - CSV Crawler](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/012%20-%20CSV%20Crawler.ipynb)\n  - [013 - Merging Datasets on S3](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/013%20-%20Merging%20Datasets%20on%20S3.ipynb)\n  - [014 - Schema Evolution](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/014%20-%20Schema%20Evolution.ipynb)\n  - [015 - EMR](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/015%20-%20EMR.ipynb)\n  - [016 - EMR & Docker](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/016%20-%20EMR%20%26%20Docker.ipynb)\n  - [017 - Partition Projection](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/017%20-%20Partition%20Projection.ipynb)\n  - [018 - QuickSight](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/018%20-%20QuickSight.ipynb)\n  - [019 - Athena Cache](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/019%20-%20Athena%20Cache.ipynb)\n  - [020 - Spark Table Interoperability](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/020%20-%20Spark%20Table%20Interoperability.ipynb)\n  - [021 - Global Configurations](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/021%20-%20Global%20Configurations.ipynb)\n  - [022 - Writing Partitions Concurrently](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/022%20-%20Writing%20Partitions%20Concurrently.ipynb)\n  - [023 - Flexible Partitions Filter](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/023%20-%20Flexible%20Partitions%20Filter.ipynb)\n  - [024 - Athena Query Metadata](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/024%20-%20Athena%20Query%20Metadata.ipynb)\n  - [025 - Redshift - Loading Parquet files with Spectrum](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/025%20-%20Redshift%20-%20Loading%20Parquet%20files%20with%20Spectrum.ipynb)\n  - [026 - Amazon Timestream](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/026%20-%20Amazon%20Timestream.ipynb)\n  - [027 - Amazon Timestream 2](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/027%20-%20Amazon%20Timestream%202.ipynb)\n  - [028 - Amazon DynamoDB](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/028%20-%20DynamoDB.ipynb)\n  - [029 - S3 Select](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/029%20-%20S3%20Select.ipynb)\n  - [030 - Data Api](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/030%20-%20Data%20Api.ipynb)\n  - [031 - OpenSearch](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/031%20-%20OpenSearch.ipynb)\n  - [032 - Lake Formation Governed Tables](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/032%20-%20Lake%20Formation%20Governed%20Tables.ipynb)\n  - [033 - Amazon Neptune](https://github.com/awslabs/aws-data-wrangler/blob/main/tutorials/033%20-%20Amazon%20Neptune.ipynb)\n- [**API Reference**](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html)\n  - [Amazon S3](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#amazon-s3)\n  - [AWS Glue Catalog](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#aws-glue-catalog)\n  - [Amazon Athena](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#amazon-athena)\n  - [AWS Lake Formation](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#aws-lake-formation)\n  - [Amazon Redshift](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#amazon-redshift)\n  - [PostgreSQL](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#postgresql)\n  - [MySQL](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#mysql)\n  - [SQL Server](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#sqlserver)\n  - [Data API Redshift](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#data-api-redshift)\n  - [Data API RDS](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#data-api-rds)\n  - [OpenSearch](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#opensearch)\n  - [Amazon Neptune](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#amazon-neptune)\n  - [DynamoDB](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#dynamodb)\n  - [Amazon Timestream](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#amazon-timestream)\n  - [Amazon EMR](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#amazon-emr)\n  - [Amazon CloudWatch Logs](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#amazon-cloudwatch-logs)\n  - [Amazon Chime](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#amazon-chime)\n  - [Amazon QuickSight](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#amazon-quicksight)\n  - [AWS STS](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#aws-sts)\n  - [AWS Secrets Manager](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#aws-secrets-manager)\n  - [Global Configurations](https://aws-data-wrangler.readthedocs.io/en/3.0.0a2/api.html#global-configurations)\n- [**License**](https://github.com/awslabs/aws-data-wrangler/blob/main/LICENSE.txt)\n- [**Contributing**](https://github.com/awslabs/aws-data-wrangler/blob/main/CONTRIBUTING.md)\n- [**Legacy Docs** (pre-1.0.0)](https://aws-data-wrangler.readthedocs.io/en/0.3.3/)\n\n## Getting Help\n\nThe best way to interact with our team is through GitHub. You can open an [issue](https://github.com/awslabs/aws-data-wrangler/issues/new/choose) and choose from one of our templates for bug reports, feature requests...\nYou may also find help on these community resources:\n* The #aws-data-wrangler Slack [channel](https://join.slack.com/t/aws-data-wrangler/shared_invite/zt-sxdx38sl-E0coRfAds8WdpxXD2Nzfrg)\n* Ask a question on [Stack Overflow](https://stackoverflow.com/questions/tagged/awswrangler)\n  and tag it with `awswrangler`\n\n## Community Resources\n\nPlease [send a Pull Request](https://github.com/awslabs/aws-data-wrangler/edit/main/README.md) with your resource reference and @githubhandle.\n\n- [Optimize Python ETL by extending Pandas with AWS Data Wrangler](https://aws.amazon.com/blogs/big-data/optimize-python-etl-by-extending-pandas-with-aws-data-wrangler/) [[@igorborgest](https://github.com/igorborgest)]\n- [Reading Parquet Files With AWS Lambda](https://aprakash.wordpress.com/2020/04/14/reading-parquet-files-with-aws-lambda/) [[@anand086](https://github.com/anand086)]\n- [Transform AWS CloudTrail data using AWS Data Wrangler](https://aprakash.wordpress.com/2020/09/17/transform-aws-cloudtrail-data-using-aws-data-wrangler/) [[@anand086](https://github.com/anand086)]\n- [Rename Glue Tables using AWS Data Wrangler](https://ananddatastories.com/rename-glue-tables-using-aws-data-wrangler/) [[@anand086](https://github.com/anand086)]\n- [Getting started on AWS Data Wrangler and Athena](https://medium.com/@dheerajsharmainampudi/getting-started-on-aws-data-wrangler-and-athena-7b446c834076) [[@dheerajsharma21](https://github.com/dheerajsharma21)]\n- [Simplifying Pandas integration with AWS data related services](https://medium.com/@bv_subhash/aws-data-wrangler-simplifying-pandas-integration-with-aws-data-related-services-2b3325c12188) [[@bvsubhash](https://github.com/bvsubhash)]\n- [Build an ETL pipeline using AWS S3, Glue and Athena](https://www.linkedin.com/pulse/build-etl-pipeline-using-aws-s3-glue-athena-data-wrangler-tom-reid/) [[@taupirho](https://github.com/taupirho)]\n\n## Logging\n\nEnabling internal logging examples:\n\n```py3\nimport logging\nlogging.basicConfig(level=logging.INFO, format="[%(name)s][%(funcName)s] %(message)s")\nlogging.getLogger("awswrangler").setLevel(logging.DEBUG)\nlogging.getLogger("botocore.credentials").setLevel(logging.CRITICAL)\n```\n\nInto AWS lambda:\n\n```py3\nimport logging\nlogging.getLogger("awswrangler").setLevel(logging.DEBUG)\n```\n\n## Who uses AWS Data Wrangler?\n\nKnowing which companies are using this library is important to help prioritize the project internally.\nIf you would like us to include your company’s name and/or logo in the README file to indicate that your company is using the AWS Data Wrangler, please raise a "Support Data Wrangler" issue. If you would like us to display your company’s logo, please raise a linked pull request to provide an image file for the logo. Note that by raising a Support Data Wrangler issue (and related pull request), you are granting AWS permission to use your company’s name (and logo) for the limited purpose described here and you are confirming that you have authority to grant such permission.\n\n- [Amazon](https://www.amazon.com/)\n- [AWS](https://aws.amazon.com/)\n- [Cepsa](https://cepsa.com) [[@alvaropc](https://github.com/alvaropc)]\n- [Cognitivo](https://www.cognitivo.ai/) [[@msantino](https://github.com/msantino)]\n- [Digio](https://www.digio.com.br/) [[@afonsomy](https://github.com/afonsomy)]\n- [DNX](https://www.dnx.solutions/) [[@DNXLabs](https://github.com/DNXLabs)]\n- [Fortescue Future Industries](https://ffi.com.au/) [[@spencervoorend](https://github.com/spencervoorend)]\n- [Funcional Health Tech](https://www.funcionalcorp.com.br/) [[@webysther](https://github.com/webysther)]\n- [Infomach](https://www.infomach.com.br/)\n- [Informa Markets](https://www.informamarkets.com/en/home.html) [[@mateusmorato]](http://github.com/mateusmorato)\n- [LINE TV](https://www.linetv.tw/) [[@bryanyang0528](https://github.com/bryanyang0528)]\n- [Magnataur](https://magnataur.com) [[@brianmingus2](https://github.com/brianmingus2)]\n- [M4U](https://www.m4u.com.br/) [[@Thiago-Dantas](https://github.com/Thiago-Dantas)]\n- [NBCUniversal](https://www.nbcuniversal.com/) [[@vibe](https://github.com/vibe)]\n- [nrd.io](https://nrd.io/) [[@mrtns](https://github.com/mrtns)]\n- [OKRA Technologies](https://okra.ai) [[@JPFrancoia](https://github.com/JPFrancoia), [@schot](https://github.com/schot)]\n- [Pier](https://www.pier.digital/) [[@flaviomax](https://github.com/flaviomax)]\n- [Pismo](https://www.pismo.io/) [[@msantino](https://github.com/msantino)]\n- [ringDNA](https://www.ringdna.com/) [[@msropp](https://github.com/msropp)]\n- [Serasa Experian](https://www.serasaexperian.com.br/) [[@andre-marcos-perez](https://github.com/andre-marcos-perez)]\n- [Shipwell](https://shipwell.com/) [[@zacharycarter](https://github.com/zacharycarter)]\n- [strongDM](https://www.strongdm.com/) [[@mrtns](https://github.com/mrtns)]\n- [Thinkbumblebee](https://www.thinkbumblebee.com/) [[@dheerajsharma21]](https://github.com/dheerajsharma21)\n- [VTEX](https://vtex.com/us-en/) [[@igorborgest]](https://github.com/igorborgest)\n- [Zillow](https://www.zillow.com/) [[@nicholas-miles]](https://github.com/nicholas-miles)\n\n## What is Amazon SageMaker Data Wrangler?\n\n**Amazon SageMaker Data Wrangler** is a new SageMaker Studio feature that has a similar name but has a different purpose than the **AWS Data Wrangler** open source project.\n\n- **AWS Data Wrangler** is open source, runs anywhere, and is focused on code.\n\n- **Amazon SageMaker Data Wrangler** is specific for the SageMaker Studio environment and is focused on a visual interface.\n',
    'author': 'Amazon Web Services',
    'author_email': None,
    'maintainer': None,
    'maintainer_email': None,
    'url': 'https://aws-data-wrangler.readthedocs.io/',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'extras_require': extras_require,
    'python_requires': '>=3.8,<3.11',
}


setup(**setup_kwargs)
