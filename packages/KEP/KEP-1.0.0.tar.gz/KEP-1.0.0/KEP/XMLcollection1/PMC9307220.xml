<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220802</date><key>pmc.key</key><document><id>9307220</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.1007/s11760-022-02308-x</infon><infon key="article-id_pmc">9307220</infon><infon key="article-id_pmid">35910402</infon><infon key="article-id_publisher-id">2308</infon><infon key="fpage">1</infon><infon key="kwd">COVID-19 Face mask detection Face-hand interaction detection Social distance measurement CNN</infon><infon key="license">Open AccessThis article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.</infon><infon key="lpage">8</infon><infon key="name_0">surname:Eyiokur;given-names:Fevziye Irem</infon><infon key="name_1">surname:Ekenel;given-names:Hazım Kemal</infon><infon key="name_2">surname:Waibel;given-names:Alexander</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="year">2022</infon><offset>0</offset><text>Unconstrained face mask and face-hand interaction datasets: building a computer vision system to help prevent the transmission of COVID-19</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>139</offset><text>Health organizations advise social distancing, wearing face mask, and avoiding touching face to prevent the spread of coronavirus. Based on these protective measures, we developed a computer vision system to help prevent the transmission of COVID-19. Specifically, the developed system performs face mask detection, face-hand interaction detection, and measures social distance. To train and evaluate the developed system, we collected and annotated images that represent face mask usage and face-hand interaction in the real world. Besides assessing the performance of the developed system on our own datasets, we also tested it on existing datasets in the literature without performing any adaptation on them. In addition, we proposed a module to track social distance between people. Experimental results indicate that our datasets represent the real-world’s diversity well. The proposed system achieved very high performance and generalization capacity for face mask usage detection, face-hand interaction detection, and measuring social distance in a real-world scenario on unseen data. The datasets are available at https://github.com/iremeyiokur/COVID-19-Preventions-Control-System.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1331</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1344</offset><text>The COVID-19 pandemic has affected the whole world since the beginning of 2020. In order to decrease the transmission of the COVID-19 disease, many health institutions, particularly the World Health Organization (WHO), have recommended serious constraints and preventions. The essential precautions that individuals can carry out are practicing social distance, wearing a face mask properly (covering mouth and nose), paying attention to personal hygiene, especially hand hygiene, and avoiding touching faces with hands without cleanliness.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1885</offset><text>Convolutional Neural Networks (CNNs), introduced in late 80s, have gained popularity during the last decade. Due to the success of deep learning in computer vision, novel research topics that emerged as a consequence of the COVID-19 pandemic are handled in this context by researchers. These studies focus on diagnosing COVID-19, adjusting the existing surveillance systems to COVID-19 conditions, and building systems to control the preventions. Face detection and recognition systems’ performance deteriorates when subjects wear face masks. Thus, novel face recognition and detection studies try to improve the performance under this condition. Moreover, in order to track the execution of preventions against the spread of COVID-19, several works investigate the detection of face masks and wearing a mask suitably, how people keep physical distancing, and detection of face-hand interaction.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2783</offset><text>To research the effects of COVID-19 regulations, some face mask datasets are introduced. In, a novel masked face recognition dataset is published to improve the face recognition performance in the case of occlusion due to the face masks. In, an artificial masked face dataset, named MaskedFace-Net, is presented. It contains 137,016 images that are generated from the FFHQ dataset using a mask-to-face deformable model. Joshi et. al proposed a framework to detect whether people are wearing a mask or not in public areas. They utilized MTCNN and MobileNetV2 to detect faces and classify them on their own video dataset. In, a one-stage detector based on RetinaFace is proposed to detect faces and classify them whether they contain masks. In, the authors proposed a real-time face mask detector named SSDMNV2, which is composed of SSD face detector and MobileNetV2 mask classifier. In addition to the face mask detection studies, a recent study investigated the face-hand touching behavior. The authors presented 2M non-touching and 74K touching face-hand interaction annotations on 64 video recordings and they evaluated introduced dataset with rule-based, hand-crafted and CNN feature-based models. As a result of evaluations, CNN-based model obtained the best results with 83.76% F1-score.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4076</offset><text>These aforementioned studies show that the face mask detection task is mostly handled in two classes, which are faces with or without a mask. However, this is not sufficient, since this setting omits improper face mask usage that frequently occurs in real-world cases. In, although improper usage of face mask was presented, these images are considered as no mask class when the detection system was developed. Furthermore, in, improper face mask class contains a small amount of images, and in, the images are artificially generated. In contrast to existing studies, we present a novel dataset which contains a larger set of unconstrained real world images. We handle the face mask detection as a multi-class classification task by representing improper face mask usage class as well. Differently from previous studies, we additionally aim to address face-hand interaction detection in order to prevent the spread of airborne viruses. The face-hand interaction task is investigated in for the first time; however, the utilized dataset is not collected for this purpose and does not correspond to the real-world conditions. This motivates us to collect and annotate an unconstrained dataset for face-hand interaction detection as well. Since our objective is to monitor three main COVID-19 protective measures, namely face mask detection, face-hand interaction detection, and social distance measurement tasks, we develop a comprehensive computer vision system that handles these measures jointly for the first time. Moreover, we show the positive effect of using large-scale datasets of diverse facial images on the tasks’ performances and generalization capacity of the trained models.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5766</offset><text>In this work, we collected two novel face datasets, namely Interactive Systems Labs Unconstrained Face Mask Dataset (ISL-UFMD) and Interactive Systems Labs Unconstrained Face-Hand Interaction Dataset (ISL-UFHD). These datasets are collected from the web to provide a significant amount of variation in terms of pose, illumination, resolution, environment, and subjects’ ethnicities. We utilized proposed datasets for the training of presented system which consists of three submodules, face mask detection, face-hand interaction detection, and social distance measurement tasks, respectively. We trained well-known CNN models for the face mask and face-hand interaction detection tasks. While the first model classifies the face image as wearing a mask properly, wearing a mask improperly, or not wearing a mask, the second model classifies face images as touching the face or not. The trained models are evaluated both on the collected datasets and on the existing face mask datasets in the literature without training or fine-tuning on them. We also proposed a rule-based approach to measure the social distance.</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>6883</offset><text>Comparison of the face mask datasets</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Dataset name&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;No mask&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Mask&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Improper Mask&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Face Mask Type&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Ethnicities&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Head Pose&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;ISL-UFMD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10698&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;10618&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;500&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Real&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Various&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Various&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;RMFD &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR10&quot;&gt;10&lt;/xref&gt;*&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;90468&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2203&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Real&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Asian&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Frontal to Profile&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;RWMFD &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR10&quot;&gt;10&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;858&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;4075&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;238&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Real&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Mostly Asian&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Frontal to Profile&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Face mask &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR35&quot;&gt;35&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;718&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;3239&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;123&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Real&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Mostly Asian&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Various&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;MaskedFace-Net &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR16&quot;&gt;16&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;67049&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;66734&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Artificial&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Various&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Mostly Frontal&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>6920</offset><text>Dataset name	No mask	Mask	Improper Mask	Face Mask Type	Ethnicities	Head Pose	 	ISL-UFMD	10698	10618	500	Real	Various	Various	 	RMFD *	90468	2203	–	Real	Asian	Frontal to Profile	 	RWMFD 	858	4075	238	Real	Mostly Asian	Frontal to Profile	 	Face mask 	718	3239	123	Real	Mostly Asian	Various	 	MaskedFace-Net 	–	67049	66734	Artificial	Various	Mostly Frontal	 	</text></passage><passage><infon key="file">Tab1.xml</infon><infon key="id">Tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>7281</offset><text>(*) Although it is stated that RMFD dataset contains 5000 face images with mask, there are only 2203 face images with mask in the publicly available version</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>7438</offset><text>Our contributions can be summarized as follows: (1) We present two novel datasets, ISL-UFMD and ISL-UFHD, for face mask and face-hand interaction detection tasks. ISL-UFMD is one of the largest face mask datasets that includes real-world images with a significant amount of variations and improper face mask usage class. The ISL-UFHD is the first dataset that contains face-hand interaction images from unconstrained real-world scenes. (2) To help people to follow protective measures to avoid spread of COVID-19, we develop a computer vision system that contains all three tasks for the first time. (3) We extensively investigate several CNN models on our datasets to show the efficiency of our unconstrained datasets. We also tested them on publicly available masked face datasets without performing adaptation, e.g. fine-tuning, on them to demonstrate the generalization capacity of our trained models. We achieved very high classification accuracies which indicates the collected datasets’ capability to represent real-world cases. Moreover, to evaluate the overall system, we utilized six different short real-world videos.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>8569</offset><text>The ISL-UFMD &amp; ISL-UFHD datasets</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8602</offset><text>Existing datasets, which are listed in Table 1, mainly focused on collecting face mask images to develop a system that examines whether there is a mask on the face. Most of them contain a limited amount of improper face mask images or include artificially generated masks on the face images using landmark points around the mouth and nose. Besides, the variety of subjects’ ethnicity, environment, resolution, and head-poses are limited. For instance, in these datasets except MaskedFace-Net, Asian people are in the majority. Although MaskedFace-Net includes variation in terms of ethnicity, it consists artificially generated face mask images. Besides, they have limited head-poses mostly from frontal to profile view in yaw axis. Thus, these limitations led us to collect an unconstrained dataset. Additionally, there is only one dataset with face-hand interaction annotations. However, these annotations are limited based on the number of subjects and the dataset is collected under controlled conditions. In contrast, we present a face-hand interaction dataset that is collected from unconstrained real-world scenes.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>9727</offset><text>Data collection</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9743</offset><text>We collected a large amount of face images from several different resources, such as publicly available face datasets, FFHQ, CelebA, LFW, Wider-Face, YouTube videos, and web. These different sources enable us to collect a significant variety of face images in terms of ethnicity, age, and gender. In addition to the subject diversity, we obtained images from indoor and outdoor environments, under different light conditions and resolutions. We also considered ensuring large head pose variations. Moreover, another important key point is to leverage the performance of our COVID-19 prevention system for the combined scenario, e.g., determining mask usage in the case of touching faces or detecting face-hand interaction in the case of wearing a mask. Besides, our images include different sorts of occlusion that make the dataset more challenging. In the end, ISL-UFMD contains 21,816 face images for the face mask detection scenario, 10,618 face images with masks and 10,698 images without a mask. Additionally, we gathered 500 images for improper mask usage. This class has a relatively small number of images compared to no mask and mask classes due to lack of face images with improper mask usage.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>10947</offset><text>The ISL-UFHD is composed of 20,038 samples with and 10,018 samples without face-hand interaction. Please note that, even if the hand is around the face without touching it, we annotated images as a no interaction. Therefore, the model should be able to distinguish whether the hand is touching or being close to the face.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>11269</offset><text>Data annotation</text></passage><passage><infon key="file">11760_2022_2308_Fig1_HTML.jpg</infon><infon key="id">Fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>11285</offset><text>Example images from ISL-UFMD belonging to three different classes; no mask, face mask, improper face mask</text></passage><passage><infon key="file">11760_2022_2308_Fig2_HTML.jpg</infon><infon key="id">Fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>11391</offset><text>Example images from ISL-UFHD that represent face-hand interaction and no interaction</text></passage><passage><infon key="file">11760_2022_2308_Fig3_HTML.jpg</infon><infon key="id">Fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>11476</offset><text>Proposed system for controlling COVID-19 preventions</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>11529</offset><text>For labelling the collected datasets, we designed a web-based image annotation tool. Eleven people from different countries annotated our images using our web tool. After examining annotations from labelers, we decided each image’s final label. Since we formulate our tasks as classification problems, we annotated our images in that manner. While we have three classes—mask, no mask, improper mask—for the mask detection task, we have two classes for the face-hand interaction detection task. The images that include the face without a fully covered nose and mouth by the mask are annotated with the improper mask label. If a person has a mask under the chin, we annotated the image with no mask label. In the face-hand annotation, we aim to identify whether the hand touches the face from RGB images. We considered the direct contact or too close to contact as the existence of face-hand interaction. Many examples of annotated face images for face mask and face-hand interaction detection are shown in Figs. 1 and 2. It is clear that our proposed datasets contain large amount of variations especially for ethnicity and head pose. Also, the examples have diversity in terms of position of hand upon face and usage of face mask.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>12767</offset><text>Methodology</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12779</offset><text>The proposed system, which is illustrated in Fig. 3, consists of three submodules. The system performs person detection and then calculates distances between detected people on input image/video frame. Meanwhile, the same input is used to detect and crop faces of subjects to perform face mask and face-hand interaction detections. While the face mask model decides whether a person wears a mask properly, the face-hand interaction model identifies whether a hand touches the subject’s face. We decided to perform person and face detection separately to eliminate the effect of missing modality. For instance, although a person’s body is occluded and, therefore, social distancing cannot be measured for this person, system can still detect the face of the person to perform other tasks. Similarly, if the subject’s face is occluded or not turned to the camera, system can capture the person’s body to calculate the social distance.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>13721</offset><text>Face mask and face-hand interaction detection</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>13767</offset><text>Face mask detection results on proposed ISL-UFMD dataset for three classes</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;2&quot;&gt;Model&lt;/th&gt;&lt;th align=&quot;left&quot; rowspan=&quot;2&quot;&gt;Accuracy&lt;/th&gt;&lt;th align=&quot;left&quot; colspan=&quot;3&quot;&gt;Precision&lt;/th&gt;&lt;th align=&quot;left&quot; colspan=&quot;3&quot;&gt;Recall&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;No Mask&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Mask&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Improper Mask&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;No Mask&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Mask&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Improper Mask&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Inception-v3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;98.20%&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.985&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.986&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.833&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.988&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.984&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.800&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;ResNet50&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;95.63%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.965&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.954&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.636&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.973&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.973&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.389&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;MobileNetV2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;97.91%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.988&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.975&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.842&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.983&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;0.992&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.640&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;EfficientNet-b0&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;97.82%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.973&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.984&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;0.929&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;0.992&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.986&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.520&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;EfficientNet-b1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;97.91%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.979&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.986&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.800&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.990&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.984&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.711&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;EfficientNet-b2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;97.91%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;0.990&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.977&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.792&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.977&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;0.992&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.760&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;EfficientNet-b3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;98.19%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.988&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;0.990&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.733&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.986&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.982&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;0.880&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>13842</offset><text>Model	Accuracy	Precision	Recall	 	No Mask	Mask	Improper Mask	No Mask	Mask	Improper Mask	 	Inception-v3	98.20%	0.985	0.986	0.833	0.988	0.984	0.800	 	ResNet50	95.63%	0.965	0.954	0.636	0.973	0.973	0.389	 	MobileNetV2	97.91%	0.988	0.975	0.842	0.983	0.992	0.640	 	EfficientNet-b0	97.82%	0.973	0.984	0.929	0.992	0.986	0.520	 	EfficientNet-b1	97.91%	0.979	0.986	0.800	0.990	0.984	0.711	 	EfficientNet-b2	97.91%	0.990	0.977	0.792	0.977	0.992	0.760	 	EfficientNet-b3	98.19%	0.988	0.990	0.733	0.986	0.982	0.880	 	</text></passage><passage><infon key="file">Tab2.xml</infon><infon key="id">Tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>14346</offset><text>Bold values indicate the best scores</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14383</offset><text>In order to obtain face crops, we performed face detection using RetinaFace that was trained on Wider-Face dataset. We used RetinaFace detector since it is robust against tiny faces, challenging head poses, and faces with a mask. Then, we cropped detected faces with a 20% margin for each side, since the face detector’s outputs are quite tight. To perform face mask and face-hand interaction detections, we employed several different CNN architectures, namely ResNet50, Inception-v3, MobileNetV2, and EfficientNet. We decided to use EfficientNet, since it is the state-of-the-art model. We also included MobileNetV2, since it is a light-weight deep CNN model. Finally, we chose ResNet and Inception-v3 models based on their high performances. In the training, we benefited from transfer learning and initialized our networks with the weights of the pretrained models on ImageNet. We employed softmax loss at the end of each network. In EfficientNet and MobileNetV2, we utilized dropout with a 0.2 probability rate to avoid overfitting. For training, we used 0.0001 learning rate and 0.0005 weight decay parameters. We optimized our models with Adam with . The input sizes of the networks are , ,  for MobileNetV2, ResNet50, and Inception-v3, respectively. For the EfficientNet, we employed networks with input sizes between  and . We executed training of our models with mini-batch size of 32 to 128 on the NVIDIA Titan RTX GPU.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>15815</offset><text>Social distance controlling</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15843</offset><text>Keeping the social distance from others is another crucial measurement to avoid spreading of COVID-19. For this, firstly, we detect each person on the image using a pretrained person detection model, DeepHRNet. Thus, we obtain bounding boxes around the people and estimated pose information of each person. Principally, we focus on the shoulders’ coordinates to measure the approximate body width of a person on the image. In many studies, measurements are calculated based on the bounding box around the person. However, when the angle of the body joints and pose are considered, changes on the bounding boxes may reduce the precision of the measurements. To prevent this, we propose to use shoulders’ coordinates to measure the width and identify the middle point of shoulders line as center of the body. After performing detection and pose estimation, we generated pairs  using the combination of each detected persons.  and  are represent each detected person. Then, we calculated the Euclidean distance between the shoulder centers of each pair of persons. In order to decide whether these persons keep social distance between each other, we adaptively calculate a threshold for each pair individually based on the average of their body width. Since the represented measurement of the real world, expressed by pixels in the image domain, constantly changes as depth increases, we overcome this by calculating the average of the body widths of two people. Since the average shoulder width of an adult is around 40-50 cm in the real world and the required social distance between two persons is 1.5-2.0 meters, we empirically decide to select  coefficient as three when calculating the threshold for social distance in the pixel domain as in Eq. 1.Finally, if the Euclidean distance between two persons is lower than the calculated threshold, we decide that these people do not keep sufficient social distance.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>17762</offset><text>Experimental results</text></passage><passage><infon key="file">11760_2022_2308_Fig4_HTML.jpg</infon><infon key="id">Fig4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>17783</offset><text>Class activation map (CAM): a face mask detection task, b face-hand interaction detection task, c misclassified samples of face mask detection task, d misclassified samples of face-hand interaction detection task</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>17996</offset><text>Results for cross-dataset experiments. All models are trained and tested on corresponding dataset. Please note that all experiments are conducted on the 3-class classification setup to perform fair comparison</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;2&quot;&gt;Architecture&lt;/th&gt;&lt;th align=&quot;left&quot; rowspan=&quot;2&quot;&gt;Training Set&lt;/th&gt;&lt;th align=&quot;left&quot; rowspan=&quot;2&quot;&gt;Test Set&lt;/th&gt;&lt;th align=&quot;left&quot; colspan=&quot;2&quot;&gt;# Images&lt;/th&gt;&lt;th align=&quot;left&quot; rowspan=&quot;2&quot;&gt;Accuracy&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Train&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Test&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;MobileNetV2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ISL-UFMD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RMFD &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR10&quot;&gt;10&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;20764&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;92671&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;91.4%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;MobileNetV2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ISL-UFMD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RWMFD &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR10&quot;&gt;10&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;20764&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5171&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;94.7%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;MobileNetV2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ISL-UFMD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;MaskedFace-Net &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR16&quot;&gt;16&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;20764&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;133782&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;88.11%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;MobileNetV2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ISL-UFMD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Face mask &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR35&quot;&gt;35&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;20764&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;4080&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;95.71%&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Inception-v3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ISL-UFMD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RMFD &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR10&quot;&gt;10&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;20764&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;92671&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;95.91%&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Inception-v3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ISL-UFMD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RWMFD &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR10&quot;&gt;10&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;20764&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5171&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;95.9%&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Inception-v3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ISL-UFMD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;MaskedFace-Net &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR16&quot;&gt;16&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;20764&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;133782&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;91.42%&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Inception-v3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ISL-UFMD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Face mask &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR35&quot;&gt;35&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;20764&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;4080&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;94.7%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;MobileNetV2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RMFD + RWMFD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ISL-UFMD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;97842&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;21816&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;86.59%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;MobileNetV2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RMFD + RWMFD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Face mask &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR35&quot;&gt;35&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;97842&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;4080&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;91.07%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;MobileNetV2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;MaskedFace-Net + FFHQ&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ISL-UFMD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;211936&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;21816&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;51.49%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;MobileNetV2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;MaskedFace-Net + FFHQ&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Face mask &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR35&quot;&gt;35&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;211936&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;4080&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;20.4%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Inception-v3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RMFD + RWMFD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ISL-UFMD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;97842&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;21816&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;88.92%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Inception-v3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RMFD + RWMFD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Face mask &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR35&quot;&gt;35&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;97842&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;4080&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;88.4%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Inception-v3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;MaskedFace-Net + FFHQ&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ISL-UFMD&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;211936&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;21816&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;51.39%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Inception-v3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;MaskedFace-Net + FFHQ&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Face mask &lt;xref ref-type=&quot;bibr&quot; rid=&quot;CR35&quot;&gt;35&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;211936&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;4080&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;19.2%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>18205</offset><text>Architecture	Training Set	Test Set	# Images	Accuracy	 	Train	Test	 	MobileNetV2	ISL-UFMD	RMFD 	20764	92671	91.4%	 	MobileNetV2	ISL-UFMD	RWMFD 	20764	5171	94.7%	 	MobileNetV2	ISL-UFMD	MaskedFace-Net 	20764	133782	88.11%	 	MobileNetV2	ISL-UFMD	Face mask 	20764	4080	95.71%	 	Inception-v3	ISL-UFMD	RMFD 	20764	92671	95.91%	 	Inception-v3	ISL-UFMD	RWMFD 	20764	5171	95.9%	 	Inception-v3	ISL-UFMD	MaskedFace-Net 	20764	133782	91.42%	 	Inception-v3	ISL-UFMD	Face mask 	20764	4080	94.7%	 	MobileNetV2	RMFD + RWMFD	ISL-UFMD	97842	21816	86.59%	 	MobileNetV2	RMFD + RWMFD	Face mask 	97842	4080	91.07%	 	MobileNetV2	MaskedFace-Net + FFHQ	ISL-UFMD	211936	21816	51.49%	 	MobileNetV2	MaskedFace-Net + FFHQ	Face mask 	211936	4080	20.4%	 	Inception-v3	RMFD + RWMFD	ISL-UFMD	97842	21816	88.92%	 	Inception-v3	RMFD + RWMFD	Face mask 	97842	4080	88.4%	 	Inception-v3	MaskedFace-Net + FFHQ	ISL-UFMD	211936	21816	51.39%	 	Inception-v3	MaskedFace-Net + FFHQ	Face mask 	211936	4080	19.2%	 	</text></passage><passage><infon key="file">Tab3.xml</infon><infon key="id">Tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>19173</offset><text>Bold values indicate the best scores</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>19210</offset><text>In the experiments, we used our proposed datasets to evaluate our system. We handled 90% of the data for training, the remaining data are reserved equally for validation and testing. However, since the ISL-UFHD dataset contains twice more data for no interaction class than interaction class, we put aside  5,000 images from no face-hand interaction class to avoid class bias in face-hand interaction detection experiments. Further, we utilized published face mask datasets in cross-dataset experiments. We used the publicly available versions1 of RMFD and RWMFD. RMFD includes around 2,203 masked face images. For RWMFD, we executed RetinaFace and obtained 5,171 face images from 4343 images. We used MaskedFace-Net dataset which contains  130,000 face images belongs to correctly worn face masks (CMFD) and incorrectly worn face masks (IMFD) subsets. Face mask dataset (Kaggle) contains 853 images. We used provided annotations to acquire 4,080 crop faces.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>20171</offset><text>Face mask detection</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>20191</offset><text>In Table 2, we presented the results of the trainings on ISL-UFMD. According to the experimental results, although all employed models achieved significantly high performance, the best one is Inception-v3 model with 98.20% classification accuracy. In addition to the classification accuracy, we also presented precision and recall measurements for each class separately. It is also observed that the precision and recall values are very accurate for no mask and mask classes, while the results for improper mask class are slightly lower than these two classes. Even though improper face mask images may confuse with proper face mask images due to visual similarity, the more probable reason behind this outcome is the lack of images for improper mask class.</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>20950</offset><text>Face-hand interaction detection results on proposed ISL-UFHD dataset</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Model&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Accuracy&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Precision&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Recall&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Inception-v3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;93.20%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.932&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.932&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;ResNet50&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;91.76%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.918&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.918&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;MobileNetV2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;92.37%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.924&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.924&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;EfficientNet-b0&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;92.37%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.926&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.924&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;EfficientNet-b1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;92.90%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.929&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.929&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;EfficientNet-b2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;93.35%&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;0.933&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;0.934&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;EfficientNet-b3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;92.44%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.925&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;0.924&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>21019</offset><text>Model	Accuracy	Precision	Recall	 	Inception-v3	93.20%	0.932	0.932	 	ResNet50	91.76%	0.918	0.918	 	MobileNetV2	92.37%	0.924	0.924	 	EfficientNet-b0	92.37%	0.926	0.924	 	EfficientNet-b1	92.90%	0.929	0.929	 	EfficientNet-b2	93.35%	0.933	0.934	 	EfficientNet-b3	92.44%	0.925	0.924	 	</text></passage><passage><infon key="file">Tab4.xml</infon><infon key="id">Tab4</infon><infon key="section_type">TABLE</infon><infon key="type">table_foot</infon><offset>21299</offset><text>Bold values indicate the best scores</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>21336</offset><text>In Fig. 4a, we demonstrated Class Activation Maps (CAM) for the face mask detection task to investigate activation of the model. It is clearly seen that the model focuses on the middle part of the faces, particularly on the nose and mouth. In the second image, the model identified improper mask usage since the nose of the subject is not covered by the face mask even though the mouth is covered. In Fig. 4c, we presented some misclassified images. Although the model classifies the images incorrectly, the prediction probabilities of the model are not as high as in correct predictions. This outcome indicates that the model did not confidently misclassify images. Still, the difficulty in the head pose and illumination causes misclassification in some cases.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>22101</offset><text>Cross-dataset experiments In the first experiment, we evaluated MobileNetV2 and Inception-v3 models, that were trained on our proposed dataset, on four different public face mask datasets. These results are presented in the first part of Table  3. We employed two different architectures to endorse experimental outcome. In the second experiment, we finetuned the MobileNetV2 and Inception-v3 models with two training setups to compare with the models that were trained on our dataset and these results are shown in the second part of Table 3. The first setup contains 97,842 images from the combination of RMFD and RWMFD datasets. We used them together since RMFD dataset has no improper mask class. The second setup includes 211,936 images from the MaskedFace-Net dataset with FFHQ dataset due to absence of no mask class on MaskedFace-Net. While we selected RMFD, RWMFD, MaskedFace-Net, and Face mask (Kaggle) datasets as target for our model, we used the proposed ISL-UFMD dataset and Face mask (Kaggle) dataset as target datasets for other models. Almost all models, that were trained on the ISL-UFMD, achieved more than 90% accuracy. These results indicate that our ISL-UFMD dataset is significantly representative to provide well generalized models for the face mask detection task. The combination of RMFD and RWMFD also provided accurate results, although they are not as high as the ones obtained by training the models on the proposed dataset. The models, that are trained on the MaskedFace-Net, show the worst performance. A possible reason of this outcome could be due to the fact that the artificial data are not as useful as the real data for the training.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>23775</offset><text>Face-hand interaction detection</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>23807</offset><text>In Table 4, we present the face-hand interaction detection results. As in the face mask detection task, all of the employed models have achieved very high performance to discriminate whether there is an interaction with hand. The best classification accuracy is obtained as 93.35% using EfficientNet-b2 model. The best recall and precision results are achieved by EfficientNet-b2 model as well. Almost all results in the table are considerably similar to each other. Precision and recall metrics are balanced and compatible with the accuracies.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24353</offset><text>In Fig. 4b, we provide CAM for the face-hand interaction detection. It is clearly seen that the model focuses on the hand region to decide whether there is an interaction, if hand exists. In Fig. 4d, we demonstrate some misclassified images for the face-hand interaction detection. In the first image, although the model can detect the hand and the face, it cannot identify the depth between them due to the position of the hand. In the second image, the interaction with hands is not correctly classified due to the challenging angles of the head and hands.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>24914</offset><text>Social distance controlling</text></passage><passage><infon key="file">Tab5.xml</infon><infon key="id">Tab5</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>24942</offset><text>Evaluation of the overall system on the test videos</text></passage><passage><infon key="file">Tab5.xml</infon><infon key="id">Tab5</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Video&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;# frames&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;# sub.&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Mask acc.&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Face-hand acc.&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Dist. acc.&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;V1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;179&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;99.16%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;98.32%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;V2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;307&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;99.51%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;96.25%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;V3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;303&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;96.91%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;89.43%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;96.69%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;V4&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;192&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;86.97%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;97.22%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;V5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;207&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;99.03%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;95.45%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;V6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;105&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;87.07%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;99.86%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;74.55%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Total&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;1293&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;22&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;97.95%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;93.84%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;96.51%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>24994</offset><text>Video	# frames	# sub.	Mask acc.	Face-hand acc.	Dist. acc.	 	V1	179	2	100%	99.16%	98.32%	 	V2	307	2	99.51%	96.25%	100%	 	V3	303	3	96.91%	89.43%	96.69%	 	V4	192	3	100%	86.97%	97.22%	 	V5	207	5	99.03%	95.45%	100%	 	V6	105	7	87.07%	99.86%	74.55%	 	Total	1293	22	97.95%	93.84%	96.51%	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>25276</offset><text>We utilized six different videos that we collected from the web to evaluate proposed social distancing module. These videos have different number of frames and they were recorded in various environments with different camera angles. During the calculation of the accuracy of the social distance measurement algorithm, we utilized the annotations that we decided based on the subject pairs and existing distance between each other. Person detector could not detect some of the subjects in the scene, if they are not visible in the camera due to the occlusion by other people or objects. For that reason, we ignored the missing detections when we annotated the videos’ frames and calculated the accuracies. According to the results in Table 5, we achieved very high accuracies on average. However, the fundamental problem, especially occurred in the last video, is caused by the lack of depth information. We project real-world distances to the image pixels with a rule-based approach without using reference points. Therefore, depth perception can be problematic for specific angles.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>26362</offset><text>Overall system performance</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26389</offset><text>We evaluated the overall system performance on the same six videos and presented the results in Table 5. When we examined the face-hand interaction and face mask detection performance of our system, the results on videos that contains various people and cases indicate that system can reach very high performance similar to the ones that are obtained by the models on individual test sets.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>26780</offset><text>Conclusion</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>26791</offset><text>In this paper, we collected and presented unconstrained face mask (ISL-UFMD) and face-hand interaction (ISL-UFHD) datasets to conduct face mask and face-hand interaction detection tasks. Further, we proposed a system to track essential COVID-19 preventions, which are proper face mask usage, avoiding face-hand interaction, and keeping social distance, together for the first time. We employed several different well-known CNN models to perform our system and create benchmark results for our proposed datasets. Additionally, we performed geometric calculation to check the social distance between people. Experimental results showed that trained models achieved significantly high performance with the help of our proposed datasets, since they contain a large amount of variation which represents various cases in the real world. The cross-dataset experiments indicate the generalization capacity of trained models on unseen data. The proposed system can be effectively utilized to track all preventions against the transmission of COVID-19. As a future work, we will focus on to collect more improper face mask usage images to improve the performance as well as contribute to the literature by providing more data.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>28008</offset><text>https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>28071</offset><text>Publisher's Note</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">footnote</infon><offset>28088</offset><text>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">title</infon><offset>28207</offset><text>Funding</text></passage><passage><infon key="section_type">ACK_FUND</infon><infon key="type">paragraph</infon><offset>28215</offset><text>Open Access funding enabled and organized by Projekt DEAL.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>28274</offset><text>References</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>28285</offset><text>Coronavirus disease advice for the public. https://www.who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public. Accessed: 2021-05-01</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>28432</offset><text>Covid-19: physical distancing. https://www.who.int/westernpacific/emergencies/covid-19/information/physical-distancing. Accessed: 2021-05-01</text></passage><passage><infon key="fpage">328</infon><infon key="issue">3</infon><infon key="lpage">339</infon><infon key="name_0">surname:Waibel;given-names:A</infon><infon key="name_1">surname:Hanazawa;given-names:T</infon><infon key="name_2">surname:Hinton;given-names:G</infon><infon key="name_3">surname:Shikano;given-names:K</infon><infon key="name_4">surname:Lang;given-names:KJ</infon><infon key="pub-id_doi">10.1109/29.21701</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Acoust. Speech Signal Process.</infon><infon key="type">ref</infon><infon key="volume">37</infon><infon key="year">1989</infon><offset>28573</offset><text>Phoneme recognition using time-delay neural networks</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>28626</offset><text>Le Cun, Y., et al.: Handwritten digit recognition with a back-propagation network. In: NeurIPS (1989)</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">11</infon><infon key="name_0">surname:Chen;given-names:J</infon><infon key="pub-id_doi">10.1038/s41598-019-56847-4</infon><infon key="pub-id_pmid">31913322</infon><infon key="section_type">REF</infon><infon key="source">Sci. Rep.</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2020</infon><offset>28730</offset><text>Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography</text></passage><passage><infon key="fpage">E65</infon><infon key="issue">2</infon><infon key="lpage">E71</infon><infon key="name_0">surname:Li;given-names:L</infon><infon key="pub-id_doi">10.1148/radiol.2020200905</infon><infon key="pub-id_pmid">32191588</infon><infon key="section_type">REF</infon><infon key="source">Radiology</infon><infon key="type">ref</infon><infon key="volume">296</infon><infon key="year">2020</infon><offset>28842</offset><text>Using artificial intelligence to detect covid-19 and community-acquired pneumonia based on pulmonary ct: evaluation of the diagnostic accuracy</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>28985</offset><text>Farooq, M., Hafeez, A.: Covid-resnet: A deep learning framework for screening of covid19 from radiographs. arXiv preprint arXiv:2003.14395 (2020)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29131</offset><text>Narin, A., Kaya, C., Pamuk, Z.: Automatic detection of coronavirus disease (covid-19) using x-ray images and deep convolutional neural networks. arXiv preprint arXiv:2003.10849 (2020)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29315</offset><text>Jiang, M., Fan, X.: Retinamask: a face mask detector. arXiv preprint arXiv:2005.03950 (2020)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29408</offset><text>Wang, Z., et al.: Masked face recognition dataset and application. arXiv preprint arXiv:2003.09093 (2020)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29515</offset><text>Anwar, A., Raychowdhury, A.: Masked face recognition for secure authentication. arXiv preprint arXiv:2008.11104 (2020)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29634</offset><text>Damer, N., et al.: The effect of wearing a mask on face recognition performance: an exploratory study. In: BIOSIG (2020)</text></passage><passage><infon key="fpage">209688</infon><infon key="lpage">209698</infon><infon key="name_0">surname:Chen;given-names:S</infon><infon key="name_1">surname:Liu;given-names:W</infon><infon key="name_2">surname:Zhang;given-names:G</infon><infon key="pub-id_doi">10.1109/ACCESS.2020.3039862</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">8</infon><infon key="year">2020</infon><offset>29756</offset><text>Efficient transfer learning combined skip-connected structure for masked face poses classification</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29855</offset><text>Boutros, F., Damer, N., et al.: Mfr 2021: Masked face recognition competition. In: IJCB, pp. 1–10. IEEE (2021)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>29969</offset><text>Erakn, M.E., Demir, U., Ekenel, H.K.: On recognizing occluded faces in the wild. In: BIOSIG, pp. 1–5. IEEE (2021)</text></passage><passage><infon key="fpage">100144</infon><infon key="name_0">surname:Cabani;given-names:A</infon><infon key="pub-id_doi">10.1016/j.smhl.2020.100144</infon><infon key="pub-id_pmid">33521223</infon><infon key="section_type">REF</infon><infon key="source">Smart Health</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">2021</infon><offset>30085</offset><text>Maskedface-net-a dataset of correctly/incorrectly masked face images in the context of covid-19</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>30181</offset><text>Joshi, A.S., Joshi, S.S., Kanahasabai, G., Kapil, R., Gupta, S.: Deep learning framework to detect face masks from video footage. In: CICN, pp. 435–440. IEEE (2020)</text></passage><passage><infon key="fpage">102692</infon><infon key="name_0">surname:Nagrath;given-names:P</infon><infon key="name_1">surname:Jain;given-names:R</infon><infon key="name_2">surname:Madan;given-names:A</infon><infon key="name_3">surname:Arora;given-names:R</infon><infon key="name_4">surname:Kataria;given-names:P</infon><infon key="name_5">surname:Hemanth;given-names:J</infon><infon key="pub-id_doi">10.1016/j.scs.2020.102692</infon><infon key="pub-id_pmid">33425664</infon><infon key="section_type">REF</infon><infon key="source">Sustain. Cities Soc</infon><infon key="type">ref</infon><infon key="volume">66</infon><infon key="year">2021</infon><offset>30348</offset><text>Ssdmnv2: a real time DNN-based face mask detection system using single shot multibox detector and mobilenetv2</text></passage><passage><infon key="fpage">2070</infon><infon key="issue">5</infon><infon key="name_0">surname:Batagelj;given-names:B</infon><infon key="name_1">surname:Peer;given-names:P</infon><infon key="name_2">surname:Štruc;given-names:V</infon><infon key="name_3">surname:Dobrišek;given-names:S</infon><infon key="pub-id_doi">10.3390/app11052070</infon><infon key="section_type">REF</infon><infon key="source">Appl. Sci.</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2021</infon><offset>30458</offset><text>How to correctly detect face-masks for covid-19 from visual information?</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>30531</offset><text>Chowdary, G.J., Punn, N.S., Sonbhadra, S.K., Agarwal, S.: Face mask detection using transfer learning of inceptionv3. In: International Conference on Big Data Analytics (2020)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>30707</offset><text>Wang, Z., Wang, P., Louis, P.C., Wheless, L.E., Huo, Y.: Wearmask: Fast in-browser face mask detection with serverless edge computing for covid-19. arXiv preprint arXiv:2101.00784 (2021)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>30894</offset><text>Petrović, N., Kocić, Đ.: Iot-based system for covid-19 indoor safety monitoring. preprint), IcETRAN (2020)</text></passage><passage><infon key="fpage">102600</infon><infon key="name_0">surname:Loey;given-names:M</infon><infon key="name_1">surname:Manogaran;given-names:G</infon><infon key="name_2">surname:Taha;given-names:MHN</infon><infon key="name_3">surname:Khalifa;given-names:NEM</infon><infon key="pub-id_doi">10.1016/j.scs.2020.102600</infon><infon key="pub-id_pmid">33200063</infon><infon key="section_type">REF</infon><infon key="source">Sustain. Cities Soc.</infon><infon key="type">ref</infon><infon key="volume">65</infon><infon key="year">2021</infon><offset>31004</offset><text>Fighting against covid-19: a novel deep learning model based on yolo-v2 with resnet-50 for medical face mask detection</text></passage><passage><infon key="fpage">108288</infon><infon key="name_0">surname:Loey;given-names:M</infon><infon key="name_1">surname:Manogaran;given-names:G</infon><infon key="name_2">surname:Taha;given-names:MHN</infon><infon key="name_3">surname:Khalifa;given-names:NEM</infon><infon key="pub-id_doi">10.1016/j.measurement.2020.108288</infon><infon key="pub-id_pmid">32834324</infon><infon key="section_type">REF</infon><infon key="source">Measurement</infon><infon key="type">ref</infon><infon key="volume">167</infon><infon key="year">2021</infon><offset>31123</offset><text>A hybrid deep transfer learning model with machine learning methods for face mask detection in the era of the covid-19 pandemic</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>31251</offset><text>Sathyamoorthy, A.J., et al.: Covid-robot: Monitoring social distancing constraints in crowded scenarios. arXiv preprint arXiv:2008.06585 (2020)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>31396</offset><text>Yang, D., Yurtsever, E., Renganathan, V., Redmill, K.A., Özgüner, Ü.: A vision-based social distancing and critical density detection system for covid-19. arXiv preprint arXiv:2007.03578 pp. 24–25 (2020)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>31605</offset><text>Rezaei, M., Azarmi, M.: Deepsocial: social distancing monitoring and infection risk assessment in covid-19 pandemic. Appl. Sci. 10(21), 7514 (2020)</text></passage><passage><infon key="fpage">102571</infon><infon key="name_0">surname:Ahmed;given-names:I</infon><infon key="name_1">surname:Ahmad;given-names:M</infon><infon key="name_2">surname:Rodrigues;given-names:JJ</infon><infon key="name_3">surname:Jeon;given-names:G</infon><infon key="name_4">surname:Din;given-names:S</infon><infon key="pub-id_doi">10.1016/j.scs.2020.102571</infon><infon key="pub-id_pmid">33163330</infon><infon key="section_type">REF</infon><infon key="source">Sustain. Cities Soc.</infon><infon key="type">ref</infon><infon key="volume">65</infon><infon key="year">2021</infon><offset>31753</offset><text>A deep learning-based social distance monitoring framework for covid-19</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>31825</offset><text>Beyan, C., et al.: Analysis of face-touching behavior in large scale social interaction dataset. In: ICMI (2020)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>31939</offset><text>Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial networks. In: CVPR, pp. 4401–4410 (2019)</text></passage><passage><infon key="fpage">1499</infon><infon key="issue">10</infon><infon key="lpage">1503</infon><infon key="name_0">surname:Zhang;given-names:K</infon><infon key="name_1">surname:Zhang;given-names:Z</infon><infon key="name_2">surname:Li;given-names:Z</infon><infon key="name_3">surname:Qiao;given-names:Y</infon><infon key="pub-id_doi">10.1109/LSP.2016.2603342</infon><infon key="section_type">REF</infon><infon key="source">IEEE Signal Proc. Lett.</infon><infon key="type">ref</infon><infon key="volume">23</infon><infon key="year">2016</infon><offset>32079</offset><text>Joint face detection and alignment using multitask cascaded convolutional networks</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>32162</offset><text>Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted residuals and linear bottlenecks. In: CVPR, pp. 4510–4520 (2018)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>32312</offset><text>Deng, J., Guo, J., Ververas, E., Kotsia, I., Zafeiriou, S.: Retinaface: Single-shot multi-level face localisation in the wild. In: CVPR, pp. 5203–5212 (2020)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>32472</offset><text>Liu, W., et al.: Ssd: Single shot multibox detector. In: ECCV, pp. 21–37. Springer (2016)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>32565</offset><text>Face mask detection. https://www.kaggle.com/andrewmvd/face-mask-detection. Accessed: 2021-05-01</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>32661</offset><text>Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In: ICCV, pp. 3730–3738 (2015)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>32775</offset><text>Huang, G.B., Learned-Miller, E.: Labeled faces in the wild: Updates and new reporting procedures. Dept. Comput. Sci., Univ. Massachusetts Amherst, Amherst, MA, USA, Tech. Rep 14(003) (2014)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>32965</offset><text>Yang, S., Luo, P., Loy, C.C., Tang, X.: Wider face: A face detection benchmark. In: CVPR, pp. 5525–5533 (2016)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>33078</offset><text>He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR, pp. 770–778 (2016)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>33192</offset><text>Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: CVPR, pp. 2818–2826 (2016)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>33346</offset><text>Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural networks. In: ICML (2019)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>33453</offset><text>Deng, J., et al.: Imagenet: A large-scale hierarchical image database. In: CVPR, pp. 248–255. IEEE (2009)</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>33562</offset><text>Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014)</text></passage><passage><infon key="fpage">3349</infon><infon key="lpage">3364</infon><infon key="name_0">surname:Wang;given-names:J</infon><infon key="pub-id_doi">10.1109/TPAMI.2020.2983686</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. PAMI</infon><infon key="type">ref</infon><infon key="volume">43</infon><infon key="year">2020</infon><offset>33666</offset><text>Deep high-resolution representation learning for visual recognition</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>33734</offset><text>Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D.: Grad-cam: Visual explanations from deep networks via gradient-based localization. In: ICCV, pp. 618–626 (2017)</text></passage></document></collection>
