<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220803</date><key>pmc.key</key><document><id>9338857</id><infon key="license">CC BY</infon><passage><infon key="article-id_doi">10.1155/2022/7833516</infon><infon key="article-id_pmc">9338857</infon><infon key="article-id_pmid">35915789</infon><infon key="elocation-id">7833516</infon><infon key="license">This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</infon><infon key="name_0">surname:Badr;given-names:Malek</infon><infon key="name_1">surname:Al-Otaibi;given-names:Shaha</infon><infon key="name_2">surname:Alturki;given-names:Nazik</infon><infon key="name_3">surname:Abir;given-names:Tanvir</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">2022</infon><infon key="year">2022</infon><offset>0</offset><text>Deep Learning-Based Networks for Detecting Anomalies in Chest X-Rays</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>69</offset><text>X-ray images aid medical professionals in the diagnosis and detection of pathologies. They are critical, for example, in the diagnosis of pneumonia, the detection of masses, and, more recently, the detection of COVID-19-related conditions. The chest X-ray is one of the first imaging tests performed when pathology is suspected because it is one of the most accessible radiological examinations. Deep learning-based neural networks, particularly convolutional neural networks, have exploded in popularity in recent years and have become indispensable tools for image classification. Transfer learning approaches, in particular, have enabled the use of previously trained networks' knowledge, eliminating the need for large data sets and lowering the high computational costs associated with this type of network. This research focuses on using deep learning-based neural networks to detect anomalies in chest X-rays. Different convolutional network-based approaches are investigated using the ChestX-ray14 database, which contains over 100,000 X-ray images with labels relating to 14 different pathologies, and different classification objectives are evaluated. Starting with the pretrained networks VGG19, ResNet50, and Inceptionv3, networks based on transfer learning are implemented, with different schemes for the classification stage and data augmentation. Similarly, an ad hoc architecture is proposed and evaluated without transfer learning for the classification objective with more examples. The results show that transfer learning produces acceptable results in most of the tested cases, indicating that it is a viable first step for using deep networks when there are not enough labeled images, which is a common problem when working with medical images. The ad hoc network, on the other hand, demonstrated good generalization with data augmentation and an acceptable accuracy value. The findings suggest that using convolutional neural networks with and without transfer learning to design classifiers for detecting pathologies in chest X-rays is a good idea.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>2141</offset><text>1. Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2157</offset><text>Advances in data acquisition, storage, and processing allow for cheap, large-scale data collection. They have improved the ability to process data into useful information and advance knowledge. This caused a substantial increase in available information in medical imaging, leaving behind the days when health data was scarce. This poses a great challenge when it comes to developing tools for its analysis and interpretation that aid in decision-making. Many modern hospitals' computer systems store a large volume of chest X-rays and radiological reports.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>2715</offset><text>Digital image processing (DIP) allows segmentation and classification of medical images. In this context, segmentation defines a partition so that the obtained regions correspond to anatomical structures, processes, or regions of special interest. Its results are used to compare volumes, morphologies, and characteristics with other studies or other regions of the same image; study tissue distribution; detect lesions; understand anatomy; plan surgeries; plan radiation therapies; and detect abnormal tissue, among other tasks. The classification requires global image analysis and helps with diagnosis and treatment.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3335</offset><text>Neural networks based on deep learning (DNN, from the English Deep Neural Network), specifically convolutional ones (CNN, from the English Convolutional Neural Networks), have seen a huge boom in recent years due to the increased capacity and availability of specific graphics processing units (GPU), the significant reduction in hardware cost, and the recent advances in machine learning. In medical imaging, the number of successful DNN applications is growing, including organ and substructure segmentation, tumor detection, sample classification (complete images), and registration.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3922</offset><text>The automatic detection of anomalies in chest X-rays is an important application of deep learning networks in medical images that has gained momentum in the last year, due to COVID-19 detection. In this work, CNN automatically detects anomalies in chest X-rays from the ChestX-ray14 database, which contains more than 100,000 images with 14 possible pathologies. The work includes a deep study of the problem and CNN's and the development of programs for the design, training, and evaluation of networks using the Keras API on Python and GPU processing. Study, propose, implement, and validate DNN for chest X-ray anomaly detection.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>4555</offset><text>2. Material and Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>4579</offset><text>2.1. ChestX-ray14 database</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>4606</offset><text>The ChestX-ray14 database (Figure 1) was compiled by the NIH (National Institute of Health), the main US government agency responsible for biomedical and public health research. It comprises more than 100,000 chest X-ray images with multilabels of common diseases from more than 30,000 anonymous patients, accumulated from the year 2010 to 2020. The data was extracted from the texts of radiology reports using language processing techniques. The data set contains the following 14 varieties of abnormalities: infiltration, effusion, atelectasis, nodule, mass, pneumothorax, consolidation, pleural swelling, cardiomegaly, emphysema, edema, fibrosis, pneumonia, and hernia. Radiographs are tagged with one or more pathology keywords, resulting in single or multitags. On the other hand, the images not associated with any of the 14 classes of pathologies are labeled as “No Finding,” which is not equivalent to an image of a healthy person but could contain patterns of disease other than the 14 listed or findings uncertain within the 14 possible categories. The existence of more than one pathology associated with certain radiographs allows to analyze of the correlation between them.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>5797</offset><text>2.2. CNNs Used in the Work</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>5824</offset><text>Having introduced the basic concepts of the CNNs, this subsection aims to briefly describe the architectures of the pretrained CNNs used during the development of the work; Table 1 shows some of their properties.  Figure 2 shows a general diagram of the implemented models based on transfer learning.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>6125</offset><text>The proposed CNN has five convolution blocks that constitute the feature extraction phase, where 11 × 11, 5 × 5, and 3 × 3 filters are used. For its part, the qualifying phase has 2 fully connected layers, two layers of dropout, and an output layer with the same number of neurons as classes, adjusted according to the problem explored. It was decided to reduce the size of the input images to 224 × 224 pixels. Finally, the built network has 10,721,190 trainable parameters.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>6605</offset><text>2.2.1. VGGNet</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>6619</offset><text>VGGNet was built and trained by Karen Simonyan and Andrew Zisserman, belonging to the Vision Geometry Group (VGG) of the University of Oxford, being the winner in the location task and obtaining the second place in the classification task of the ILSVRC competition in the year 2014.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>6902</offset><text>This network uses an architecture with very small convolution filters (3 × 3) achieving a significant improvement over previous CNN configurations, where large convolution filters (9 × 9 or 11 × 11 from AlexNet) were used. The 3 × 3 filter is the smallest filter that can be used without losing track of left/right, top/bottom, and center between neighboring pixels.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7273</offset><text>There are variants of the VGG depending on the number of hidden layers. In this work the VGG19 is used.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>7377</offset><text>2.2.2. Inception (GoogLeNet)</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7406</offset><text>GoogLeNet, also known as Inception, is a CNN developed by Google researchers. The GoogLeNet architecture presented at the ILSVRC in 2014 won first place in the image classification task, beating VGG. The depth of GoogLeNet is greater than that of VGGNet. However, the number of parameters is much less, making it a better option to optimize computational costs when available resources are limited. The input data to the network are images of dimension 224 × 224 pixels, preprocessed with an average of zero.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>7916</offset><text>2.2.3. ResNet</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7930</offset><text>ResNet introduced the concept of residual learning to address the gradient fading problem, by adding direct connections between neurons, without adding additional parameters or additional computational complexity. In 2015, it was a winner at the ILSVRC in the image classification, detection, and localization categories and the MS COCO in the detection and segmentation categories.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8313</offset><text>ResNet introduced the residual block concept with the idea of information flowing across connections, allowing it to build much deeper networks. The residual block consists of various network layers and a direct access connection (Figure 3).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>8555</offset><text>2.3. Regularization</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8575</offset><text>When developing DNN, it is common to see overfitting, where the model works well with training data but has poor generalization. A neural network learns latent data patterns and adjusts model weights to fit them during training. This becomes complicated when the pattern he finds is just noise. Real-world training data may be affected by noise and differ greatly from real data. If a neural network is overtrained, that is, it overlearns the training data, and these contain noise, then the internal parameters probably move in their adjustment with respect to the values they would obtain in noise-free data, which means the network is less robust to noise and cannot generalize well. The pseudocolor images are segmentation results from the gray PD, T1, and T2 input images. MLP stands for multilayer perceptron. You cannot train a generalizable model when you have too few samples to learn from. If the data were infinite, the model would be exposed to all data distribution aspects, preventing overfitting Regularization decreases overfitting. Dropout and data augmentation are two DNN regularization methods.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>9690</offset><text>2.3.1. Dropout</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9705</offset><text>The dropout technique is one of the most effective and used techniques. This method, applied to a layer, consists of arbitrarily “deactivating” (zeroing) some neurons of a layer during each training iteration. A number of output features of the layer are randomly discarded. The fraction of the features that are reduced to zero is a parameter of choice known as the dropout rate.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10090</offset><text>It can be seen in Figure 4 how the regular network (left) has all the neurons and connections between two successive layers intact, whereas, with dropout, each iteration induces some defined degree of randomness by arbitrarily deactivating or discarding some neurons and their associated connections.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>10391</offset><text>2.3.2. Data Augmentation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10416</offset><text>Data augmentation consists of generating more training data (in quantity and/or diversity) from the existing training samples, increasing the samples through a series of random transformations that generally consist of rotations, transformations related, translations, and scaling, among others. At the training time, the goal is that the model never sees exactly the same image twice and thus generalizes better, known as dynamic data augmentation (Figure 5).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>10877</offset><text>2.4. Model Validation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10899</offset><text>Generalizability is used to evaluate a neural network's quality. To estimate generalization capacity, a suitable metric and mechanism must be defined. This work uses accuracy as the metric and hold-out as the estimation method when working with DNN.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11149</offset><text>Accuracy is the ratio of correct predictions to total predictions. It measures model generalizability. Correct generalizability estimation requires evaluating the model with untrained data. The hold-out validation method randomly creates two disjoint partitions of the original data set.  Figure 6 depicts hold-out.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11465</offset><text>Given that different subsampling of the data set or random initializations of the internal parameters of the DNNs can usually be used, it is common to carry out more than one entry/test cycle defining a different hold-out for each one. In these cases, the metric is obtained as the average of the values obtained over the different cycles.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>11805</offset><text>2.5. Development Environment and Implemented Programs</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11859</offset><text>Python 3.8.5, Keras 2.4.3, and TensorFlow were used to implement classification models. Python was chosen because it has many data analysis and deep learning libraries, extensive documentation, and a large programming community.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12088</offset><text>The code was structured in well-defined functions to create clear and readable code. Data loading and preprocessing functions were defined to obtain the necessary data sets for each proposed experimental arrangement, separate training and test data, generate tensor image batches with real-time data augmentation, create classification models and adjust them, graph results, and analyze them. The code was designed to be reused in each experiment with adequate documentation, saving time and reducing redundancy.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12601</offset><text>Jupyter Notebook, a 2015 client-server application, was used to develop the codes. A Jupyter Notebook is a “computational narrative” that publishes code and data with analyses, hypotheses, and conjectures. “.ipynb” files are simple, documented JSON files. It was chosen because it was accessed through a web browser, allowing the same interface to run locally as a desktop application and on a remote server.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13018</offset><text>Jupyter Notebook was run on a remote server at the ICyTE Image Processing Laboratory using a local web browser and SSH. SSH encrypts client-server connections. Authentication, commands, output, and file transfers are all encrypted.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13250</offset><text>Conda was a package manager and environment manager. Conda lets you run different versions of Python and its libraries in virtual environments. The work was done in a virtual environment separate from other projects.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13467</offset><text>Since deep learning training takes a long time, Screen or GNU Screen was used to keep remote sessions active. It is possible to start a screen session, open multiple virtual terminals, and then log out while the processes continue. By establishing a new connection, terminals can be accessed without interrupting processes.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>13791</offset><text>2.5.1. Obtaining New Data Sets from the Original Data Set ChestX-ray14</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13862</offset><text>Different experimental designs required new data sets from the original database. The open-source libraries NumPy, which manages vectors and matrices in Python, and Pandas were used to obtain these sets.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14066</offset><text>In this case, the ChestX-ray14 CSV file containing class labels, image paths, radiograph information, and patient information was used. We used the radiograph paths and labels for the proposed analysis, discarding unnecessary data. Only in experiment #1 was radiograph orientation preserved. The main challenge was creating functions for each problem.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14418</offset><text>The labels had to be pure in #2 and #4; the radiographs had to show a single pathology. In this case, functions were programmed to filter the database by label. Fix #1 used this function to filter images by orientation. To fix #3, all pathological X-ray images (whether pure or not) had to be relabeled as “finding,” generating a new data set with the categories “no finding” and “finding.” A program was developed for this.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14855</offset><text>Unbalanced data was one of the work's greatest challenges, so experimental arrangement #2 was proposed. A data balancing function was programmed to obtain the number of pure pathological samples and subsample the majority class until the minority class number was equal. Reusing the same function for “pure pathology” and “no finding” In array #4, the same logic was used to obtain X-ray images labeled with a single pathology by subsampling the set without substitution.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15335</offset><text>All cases required a single data set. Multiple sets were obtained in cases where filtering was used, each group belonging to each filtered category (15 categories for array #2 and pure labels with a frequency of 500 or more for fix #4). Therefore, the sets are needed to be concatenated.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15623</offset><text>The Pandas library's concatenation function created a new data set with the labels in the same order as the individual sets. Balancing cannot be ensured in partitions performed after a concatenation using this library, which harms the generalization capacity of the models and, consequently, the quality of the experiment and conclusions drawn. Programming a function to partition and concatenate data sets preserved class balance.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>16055</offset><text>2.5.2. Image Batch Generation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16085</offset><text>The preprocessing API in the Keras library has classes and functions for working with images that help convert raw data on disc into an object that can be used to train a model. It was used to both preprocess the input data and apply data augmentation to it.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16344</offset><text>The ImageDataGenerator class, also known as a generator, allows you to set parameters for preprocessing input data before feeding it to a model. On the one hand, this class was used in the experiments with pretrained CNNs, and the same preprocessing that was used in ResNet, VGG, and Inception was applied to the set of radiographs. The gray levels of the radiographs were normalized to the interval [0.1] for the proposed architecture without transfer learning. Specific functions were programmed in each case.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16856</offset><text>Most deep-learning classification data sets organize all of the images into separate directories labeled with the names of the classes, making it simple to read the images from the disc. This is not the case with ChestX-ray14, where all images are stored in a single directory and their tags are mapped to a CSV file, as described in the previous subsection.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17215</offset><text>The method flow from the data frame of the ImageDataGenerator class was used to generate the image batches, which allows the data frames generated with the data set generation functions to be used as input parameters. The method returns an iterator that generates tuples (x, y), where x is an array of batches of tensor images and y is a vector containing the labels. The batch size refers to the number of images used in each training step before a model's trainable parameters are updated. Because the GPU's memory is limited, a batch size of 128 images was chosen, with a validation set of 256 images. Specific functions were created as a result of this stage to define batches to be used in the training and evaluation of the models.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17953</offset><text> (1) Augmenting Data with the ImageDataGenerator Class. The ImageDataGenerator class also allows you to apply data augmentation. Keras augmentation is focused on generating diversity in the data shown to the model from epoch to epoch, maintaining the original amount of data. By instantiating the ImageDataGenerator class, transformations are defined, and then, during training, batches with images transformed in real time are assembled. It seeks to make the most of the training examples so that the model never sees exactly the same image twice; this helps to avoid overfitting and the model generalizes better.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18568</offset><text>A random rotation within the range of -5/+5 degrees</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18620</offset><text>A random horizontal and vertical shift of 5%</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18665</offset><text>A random increase range of 15%</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18696</offset><text>A random distortion along an axis of 0.1 degrees creates or rectifies the angles of perception</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18791</offset><text>A padding to keep the size of the input images constant, mirroring neighboring pixels to fill in the gaps with missing pixels</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18917</offset><text>To implement data augmentation, a function was defined in such a way that it would return the instance to the ImageDataGenerator class configured to perform transformations on the data. Taking into account previous articles that analyzed chest X-ray images, the following transformations were defined: </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19220</offset><text>The programmed function makes use of the flow_from_dataframe method to apply the transformations defined by the generator and assemble the image batches. Dynamic data augmentation was applied during model training by calling the function at each epoch.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19473</offset><text>In order to correctly evaluate the generalization capacity of the model, data augmentation was not applied to the validation data set, so another function had to be programmed with another generator without including the transformation parameters mentioned above. In this case, the implementation uses the flow_from_dataframe method prior to classifier training to generate the image batches.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>19866</offset><text>2.5.3. Implementation of Classification Models Using Keras</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19925</offset><text>The classification models were built with the Keras functional API, allowing for nonlinear topology models and creating more complex networks than the Keras sequential model. This API was chosen because it is extremely useful for implementing transfer learning. It can handle shared layers, which means you can access and reuse a model's middle layer activations, allowing you to use its features to create new feature extraction models that return the activation values. The weights of the convolutional layers were frozen (to avoid their adjustment during training), and the feature extraction phase was extracted for later use in transfer learning-based models.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20590</offset><text>Specific programs were written to implement both the architecture proposed and the classification phase of the models obtained from pretrained CNNs, in which the model class was used to create the model and the layers class was used to instantiate the necessary layers and define their parameters (number of neurons, activation functions, filters, filter size, type of pooling, dropout, and so on), both of which are part of the Keras library. Layers in Keras are unfrozen by default, allowing them to adjust their weights during network training.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21138</offset><text>A function was programmed to return the model. The base model and the classification scheme were used as parameters; and the number of classes according to the experimental arrangement, for the creation of the proposed architectures using transfer learning and fine-tuning. A specific function, on the other hand, was created to return the proposed architecture without transfer learning.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>21527</offset><text>3. Results and Discussion</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>21553</offset><text>This section has the objective of detailing the results obtained for the experiments carried out in the work, using tables and graphs, and also to carry out an analysis of them.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>21731</offset><text>First, the experimentation and analysis carried out for the selection of the base model (CNN pretrained) for the transfer learning tests are explained. Second, the results of experimental arrangements #1 to #4 for transfer learning are shown. Finally, the ad hoc CNN architecture is evaluated on the experimental arrangement that showed the best results.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>22086</offset><text>3.1. Selection of the Base Model and Classification Scheme for Transfer Learning</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>22167</offset><text>In the first instance of the development of this work, a study of the classification capacity of CNN chest radiographs defined from transfer learning was carried out. Starting from the ResNet50, VGG19 and InceptionV3 pre-trained CNNs, described in the previous sections and considering the two multilayer perceptron structures of the classification phase (schemes #1 and #2).  Table 2 shows a summary of the architectures studied in the first stage of the work.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>22629</offset><text>It is important to note that the experimentation was carried out equally for each proposed architecture, seeking that the results obtained are comparable. For this reason, the same hyperparameters were used for training in each of the experiments, which can be seen in Table 3.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>22907</offset><text>In the search to select the best base model and the best classification scheme, the classifiers were trained by adjusting their trainable parameters with the data sets obtained for experimental arrangement #2, the classification between no finding and a specific pathology which are shown in Figure 7.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>23209</offset><text>The best accuracy results were shown for architecture #1, consisting of the ResNet50 pretrained CNN and the first proposed classification scheme, as shown in Figure 8.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>23377</offset><text>The differences in accuracy observed with respect to the rest of the pre-trained architectures and proposed classification schemes were not very significant, but they were minor, so it was decided to continue with the following transfer-learning experiments using architecture #1, that is, combining the ResNet50 qualifying phase and the #1 qualifying phase scheme. It should be noted that all cases presented overfitting, which will be analyzed in more detail later in this section.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>23861</offset><text>3.2. Transfer Learning Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>23892</offset><text>3.2.1. Results of Experimental Setup #1: Evaluation of Robustness in the Classification according to Orientation</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24005</offset><text>After evaluating and comparing the proposed models, it was determined that ResNet50 and the multilayer perceptron #1 produced the best results in identifying pure pathologies in chest X-rays.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24197</offset><text>The problem defined in experimental arrangement #1, namely the need or not to separate the radiographs according to their orientation for subsequent classification, was investigated using ResNet50 and the classification phase scheme #1. As described in the previous chapter, the database images were filtered according to anteroposterior (AP) and posteroanterior (PA) orientation. The decision was made to perform the analysis only for the classification between “no find” and “mass” due to the lengthy computation times required for CNN training. This filtering produced two subsets, with 1544 images in the AP data set and 2734 images in the PA data set as a result of the filtering. Five training/test cycles were used to define the validation scheme.  Figure 9 summarizes the findings.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>24995</offset><text>As shown in Figure 9, there is no evidence of any improvement in the average accuracy achieved. The behavior varies a lot with respect to the results obtained for the cases evaluated without filtering by orientation; although there is overfitting in both experiments, the model does not seem to improve in generalization. But on the contrary, based on the results obtained, it was decided to continue with the proposed experiments without separating the sets by the orientation of the radiographs.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>25493</offset><text>3.2.2. Results of Experimental Setup #2: Evaluation of Robustness in the Classification according to Orientation</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>25606</offset><text>Tests were carried out with the different architectures from pretrained CNNs on the data sets shown in Table 2. The results obtained in this stage of the work made evident the need to work with regularization techniques to improve the generalization of the models.  Figure 10 shows the average accuracies obtained for each case of pure pathology vs. “no finding” with and without dynamic data augmentation over 5 training/test cycles.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26045</offset><text>When using dynamic data augmentation, it can be seen that the average validation accuracy improves by more than 5% in relative terms, less for the “nodule” vs. “no finding” classification, which shows a performance degradation. The experimental results show that the use of data augmentation decreases the overfitting that was very marked from the epoch 10.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>26411</offset><text>3.2.3. Results of Experimental Setup #3: Classification between “Find” and “No Find”</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>26504</offset><text>A new data set called “Find + No Find” was created, which contains all available examples and has a total number of images of 112,120. For this experiment, the “finding” class includes all radiographs that show one or more pathologies, taking into account both simple labels and multilabels. The results obtained with and without dynamic data augmentation are 63.9 percent without data augmentation and 69.5 percent with data augmentation. There was only one training/test cycle because the data set was unique (no subsampling).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27041</offset><text>When using data augmentation for the classification between “find” and “no find,” there is no improvement in invalidation accuracy for the set. However, the results obtained in both cases are close to 70% accuracy, which is excellent given the problem's high complexity. However, when comparing the results, there is a significant improvement in overfitting when using dynamic data augmentation.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_3</infon><offset>27445</offset><text>3.2.4. Results of Experimental Setup #4: Classification between Different Pure Pathologies</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27536</offset><text>The validation accuracy results were obtained for the average of the five training cycles of average accuracy without (27.2%) and with (28.1%) data augmentation using a set of 12 pure pathologies. The results of this experimentation were not encouraging, possibly due to the complexity of the problem posed. The use of regularization techniques did not result in any improvement in the model's lack of generalization. In terms of the validation accuracy obtained from the average of the five training cycles completed, there was a 1% improvement. However, the value obtained is close to 30% in both cases, implying that only about 3 out of every 10 radiographs shown in the model will be correctly classified. As can be seen from the results, increasing the amount of dynamic data improves the model's generalizability while reducing premature overfitting. The average validation accuracy results obtained, on the other hand, show that the proposed approach is not feasible in this experimental setup.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>28538</offset><text>3.3. CNN Results without Transfer Learning</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>28581</offset><text>Based on the results obtained with transfer learning for the different experimental arrangements, it was decided to use the data set of experimental arrangement #3, composed of the labels “no finding” and “finding,” which allowed having a large number of images necessary for the training of a complete CNN.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>28897</offset><text>It was decided to train the model for 300 epochs, using dynamic data augmentation and the same training hyperparameters as in the transfer learning experiments. Hold-out was performed with 80% of the data for training and the remaining 20% for testing. The training and test accuracy values for each training epoch are shown in the graph of Figure 11. As can be seen in Figure 11, the model generalizes very well, beginning to converge between epochs 150 and 200. Compared to the architecture proposed with ResNet50 and the multilayer perceptron, it shows a drastic improvement with overfitting. Likewise, the accuracy value achieved remains close to 70%, as with transfer learning.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>29580</offset><text>4. Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>29595</offset><text>This work studied deep learning models for identifying chest radiography pathologies. Scarcity of databases of this type of medical image in the public domain limits the state of the art. ChestX-ray14 has the most chest X-rays and the most research, so it was chosen for this work.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>29877</offset><text>The work began with a thorough study of state-of-the-art and the adopted database's characteristics, from which different experiments were designed to test the models.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>30045</offset><text>ResNet50 showed the best performance for pathology detection using CNN-based transfer learning approaches, followed by VGG19 and Inceptionv3. Transfer learning approaches allow us to affirm that this can be a valid initial strategy for using CNN in cases where there are not enough labeled images, which is common in medicine. Even with chest radiograph classification, considering the complexity of chest radiograph classification, most experiments show acceptable accuracy. Dynamic data augmentation reduced overfitting in all tests without increasing computational cost or classifier stability. The ad hoc architecture proposed to evaluate a CNN without transfer learning showed acceptable accuracy and good generalization using data augmentation, indicating it is a valid strategy to follow when available. Images are enough. DNN models with and without transfer learning proved adequate for classifying chest X-ray pathologies. The total of the programs developed in this work for the design, training, and validation of CNN, along with its documentation, is an adequate basis for the future development of a library of CNN image functions that will be added to other techniques under development in the laboratory.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>31266</offset><text>Data Availability</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>31284</offset><text>The data used to support the findings of this study are included within the article.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>31369</offset><text>Conflicts of Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>31391</offset><text>There is no potential conflict of interest in our paper, and all authors have seen the manuscript and approved to submit to your journal.</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">title_1</infon><offset>31529</offset><text>Authors' Contributions</text></passage><passage><infon key="section_type">AUTH_CONT</infon><infon key="type">paragraph</infon><offset>31552</offset><text>All authors have seen the manuscript and approved to submit to your journal.</text></passage><passage><infon key="name_0">surname:Stead;given-names:M.</infon><infon key="name_1">surname:Bower;given-names:M.</infon><infon key="name_2">surname:Brinkmann;given-names:B.</infon><infon key="name_3">surname:Warren;given-names:C.</infon><infon key="name_4">surname:Worrell;given-names:G. A.</infon><infon key="pub-id_doi">10.1201/b10866-37</infon><infon key="section_type">REF</infon><infon key="source">
In Epilepsy: The Intersection of Neurosciences, Biology, Mathematics, Engineering, and Physics
</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>31629</offset><text>Large-Scale Electrophysiology: Acquisition, Storage and Analysis</text></passage><passage><infon key="fpage">2097</infon><infon key="lpage">2106</infon><infon key="name_0">surname:Wang;given-names:X.</infon><infon key="name_1">surname:Peng;given-names:Y.</infon><infon key="name_2">surname:Lu;given-names:L.</infon><infon key="name_3">surname:Lu;given-names:Z.</infon><infon key="name_4">surname:Bagheri;given-names:M.</infon><infon key="name_5">surname:Summers;given-names:R. M.</infon><infon key="pub-id_doi">10.1109/CVPR.2017.369</infon><infon key="pub-id_other">2-s2.0-85042155331</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>31694</offset><text>ChestX-Ray8: hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</text></passage><passage><infon key="fpage">278</infon><infon key="lpage">282</infon><infon key="name_0">surname:Chauhan;given-names:R.</infon><infon key="name_1">surname:Ghanshala;given-names:K. K.</infon><infon key="name_2">surname:Joshi;given-names:R. C.</infon><infon key="pub-id_doi">10.1109/ICSCCC.2018.8703316</infon><infon key="pub-id_other">2-s2.0-85065666076</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>31837</offset><text>Convolutional neural network (CNN) for image detection and recognition</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">15</infon><infon key="name_0">surname:Madiajagan;given-names:M.</infon><infon key="name_1">surname:Raj;given-names:S.</infon><infon key="pub-id_doi">10.1016/B978-0-12-816718-2.00008-7</infon><infon key="section_type">REF</infon><infon key="source">
In Deep learning and parallel computing environment for bioengineering systems
</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>31908</offset><text>Parallel computing, graphics processing unit (GPU) and new hardware for deep learning in computational intelligence research</text></passage><passage><infon key="name_0">surname:Alazzam;given-names:M. B.</infon><infon key="name_1">surname:Tayyib;given-names:N.</infon><infon key="name_2">surname:Alshawwa;given-names:S. Z.</infon><infon key="name_3">surname:Ahmed;given-names:M.</infon><infon key="pub-id_doi">10.1155/2022/1959371</infon><infon key="pub-id_pmid">35310193</infon><infon key="section_type">REF</infon><infon key="source">
Journal of Healthcare Engineering
</infon><infon key="type">ref</infon><infon key="volume">2022</infon><infon key="year">2022</infon><offset>32033</offset><text>Nursing care systematization with case-based reasoning and artificial intelligence</text></passage><passage><infon key="name_0">surname:Musolu;given-names:M.</infon><infon key="name_1">surname:Sadeghi Darvazeh;given-names:S.</infon><infon key="name_2">surname:Raeesi Vanani;given-names:I.</infon><infon key="pub-id_doi">10.1007/978-981-15-4112-4_7</infon><infon key="section_type">REF</infon><infon key="source">
In Internet of Things for Healthcare Technologies
</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>32116</offset><text>Deep learning and its applications in medical imaging</text></passage><passage><infon key="fpage">p. 2054</infon><infon key="name_0">surname:Jemioło;given-names:P.</infon><infon key="name_1">surname:Storman;given-names:D.</infon><infon key="name_2">surname:Orzechowski;given-names:P.</infon><infon key="pub-id_doi">10.3390/jcm11072054</infon><infon key="section_type">REF</infon><infon key="source">
Journal of Clinical Medicine
</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2022</infon><offset>32170</offset><text>Artificial intelligence for COVID-19 detection in medical imaging diagnostic measures and wasting a systematic umbrella review</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">11</infon><infon key="name_0">surname:Jwaid;given-names:M. F.</infon><infon key="section_type">REF</infon><infon key="source">
Scientific Journal Al-Imam University College
</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2022</infon><offset>32297</offset><text>An efficient technique for image forgery detection using local binary pattern (Hessian and center symmetric) and transformation method</text></passage><passage><infon key="fpage">1771</infon><infon key="lpage">1776</infon><infon key="name_0">surname:Sorić;given-names:M.</infon><infon key="name_1">surname:Pongrac;given-names:D.</infon><infon key="name_2">surname:Inza;given-names:I.</infon><infon key="pub-id_doi">10.23919/MIPRO48935.2020.9245376</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>32432</offset><text>Using convolutional neural network for chest X-ray image classification</text></passage><passage><infon key="fpage">72</infon><infon key="issue">1</infon><infon key="lpage">84</infon><infon key="name_0">surname:Al-Obeidi;given-names:A. S.</infon><infon key="name_1">surname:Al-Azzawi;given-names:S. F.</infon><infon key="section_type">REF</infon><infon key="source">
International Journal of Computing Science and Mathematics
</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2022</infon><offset>32504</offset><text>A novel six-dimensional hyperchaotic system with self-excited attractors and its chaos synchronization</text></passage><passage><infon key="name_0">surname:Abdullah Hamad;given-names:A.</infon><infon key="name_1">surname:Thivagar;given-names:M. L.</infon><infon key="name_2">surname:Bader Alazzam;given-names:M.</infon><infon key="name_3">surname:Alassery;given-names:F.</infon><infon key="name_4">surname:Hajjej;given-names:F.</infon><infon key="name_5">surname:Shihab;given-names:A. A.</infon><infon key="pub-id_doi">10.1155/2022/4569879</infon><infon key="pub-id_pmid">35222627</infon><infon key="section_type">REF</infon><infon key="source">
Computational Intelligence and Neuroscience
</infon><infon key="type">ref</infon><infon key="volume">2022</infon><infon key="year">2022</infon><offset>32607</offset><text>Applying dynamic systems to social media by using controlling stability</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">22</infon><infon key="name_0">surname:Al-Baldawi;given-names:A. A. A. R.</infon><infon key="section_type">REF</infon><infon key="source">
Scientific Journal Al-Imam University College
</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2022</infon><offset>32679</offset><text>The possibility of implementing industrial incubators and their role in the development of small industry and medium in Iraq</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">19</infon><infon key="name_0">surname:Muhammad;given-names:B. A. M.</infon><infon key="section_type">REF</infon><infon key="source">
Scientific Journal Al-Imam University College
</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2022</infon><offset>32804</offset><text>The role of universities in developing societies by accreditation on scientific research</text></passage><passage><infon key="issue">10</infon><infon key="name_0">surname:Mangeri;given-names:L.</infon><infon key="name_1">surname:Gnana Prakasi;given-names:O. S.</infon><infon key="name_2">surname:Puppala;given-names:N.</infon><infon key="name_3">surname:Kanmani;given-names:P.</infon><infon key="pub-id_doi">10.14569/IJACSA.2021.0121026</infon><infon key="section_type">REF</infon><infon key="source">
International Journal of Advanced Computer Science and Applications
</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2021</infon><offset>32893</offset><text>Chest diseases prediction from X-ray images using CNN models: a study</text></passage><passage><infon key="fpage">422</infon><infon key="lpage">428</infon><infon key="name_0">surname:Stirenko;given-names:S.</infon><infon key="name_1">surname:Kochura;given-names:Y.</infon><infon key="name_2">surname:Alienin;given-names:O.</infon><infon key="pub-id_doi">10.1109/ELNANO.2018.8477564</infon><infon key="pub-id_other">2-s2.0-85055792857</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>32963</offset><text>Chest X-ray analysis of tuberculosis by deep learning with segmentation and augmentation</text></passage><passage><infon key="name_0">surname:Hamad;given-names:A. A.</infon><infon key="name_1">surname:Thivagar;given-names:M. L.</infon><infon key="name_2">surname:Alazzam;given-names:M. B.</infon><infon key="pub-id_doi">10.1155/2021/8148772</infon><infon key="pub-id_publisher-id">8148772</infon><infon key="section_type">REF</infon><infon key="source">
Advances in Materials Science and Engineering
</infon><infon key="type">ref</infon><infon key="volume">2021</infon><infon key="year">2021</infon><offset>33052</offset><text>Dynamic systems enhanced by electronic circuits on 7D</text></passage><passage><infon key="fpage">7</infon><infon key="name_0">surname:Enbeyle;given-names:W.</infon><infon key="name_1">surname:Hamad;given-names:A. A.</infon><infon key="name_2">surname:Al-Obeidi;given-names:A. S.</infon><infon key="pub-id_doi">10.1155/2022/3294954</infon><infon key="section_type">REF</infon><infon key="source">
Journal of Nanomaterials
</infon><infon key="type">ref</infon><infon key="volume">2022</infon><infon key="year">2022</infon><offset>33106</offset><text>Trend analysis and prediction on water consumption in southwestern Ethiopia</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">18</infon><infon key="name_0">surname:Ahmed;given-names:I.</infon><infon key="name_1">surname:Jeon;given-names:G.</infon><infon key="name_2">surname:Chehri;given-names:A.</infon><infon key="pub-id_doi">10.1007/s00607-021-00992-0</infon><infon key="section_type">REF</infon><infon key="source">
Computing
</infon><infon key="type">ref</infon><infon key="volume">12</infon><infon key="year">2022</infon><offset>33182</offset><text>An IoT-enabled smart health care system for screening of COVID-19 with multi layers features fusion and selection</text></passage><passage><infon key="name_0">surname:Alazzam;given-names:M. B.</infon><infon key="name_1">surname:Mansour;given-names:H.</infon><infon key="name_2">surname:Alassery;given-names:F.</infon><infon key="name_3">surname:Almulihi;given-names:A.</infon><infon key="pub-id_doi">10.1155/2021/5759184</infon><infon key="pub-id_pmid">35003245</infon><infon key="section_type">REF</infon><infon key="source">
Computational Intelligence and Neuroscience, Volume
</infon><infon key="type">ref</infon><infon key="volume">2021</infon><infon key="year">2021</infon><offset>33296</offset><text>Machine learning implementation of a diabetic patient monitoring system using interactive E-App</text></passage><passage><infon key="fpage">12</infon><infon key="name_0">surname:Ji;given-names:D.</infon><infon key="name_1">surname:Zhang;given-names:Z.</infon><infon key="name_2">surname:Zhao;given-names:Y.</infon><infon key="name_3">surname:Zhao;given-names:Q.</infon><infon key="pub-id_doi">10.1155/2021/6799202</infon><infon key="section_type">REF</infon><infon key="source">
Journal of Healthcare Engineering
</infon><infon key="type">ref</infon><infon key="volume">2021</infon><infon key="year">2021</infon><offset>33392</offset><text>Research on classification of COVID-19 chest X-ray image modal feature fusion based on deep learning</text></passage><passage><infon key="fpage">225</infon><infon key="issue">1</infon><infon key="lpage">236</infon><infon key="name_0">surname:Rahmat;given-names:T.</infon><infon key="name_1">surname:Ismail;given-names:A.</infon><infon key="name_2">surname:Aliman;given-names:S.</infon><infon key="pub-id_doi">10.24191/mjoc.v4i1.6095</infon><infon key="section_type">REF</infon><infon key="source">
Malaysian Journal of Computing
</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">2019</infon><offset>33493</offset><text>Chest X-ray image classification using faster R-CNN</text></passage><passage><infon key="name_0">surname:Qi;given-names:X.</infon><infon key="name_1">surname:Brown;given-names:L.</infon><infon key="name_2">surname:Foran;given-names:D.</infon><infon key="name_3">surname:Nosher;given-names:J.</infon><infon key="name_4">surname:Hacihaliloglu;given-names:I.</infon><infon key="pub-id_doi">10.1007/s11548-020-02305-w</infon><infon key="section_type">REF</infon><infon key="source">
International Journal of Computer Assisted Radiology and Surgery
</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2021</infon><offset>33545</offset><text>Chest X-ray image phase features for improved diagnosis of COVID-19 using convolutional neural network</text></passage><passage><infon key="name_0">surname:Alazzam;given-names:M. B.</infon><infon key="name_1">surname:Al-Radaideh;given-names:A. T.</infon><infon key="name_2">surname:Alhamarnah;given-names:R. A.</infon><infon key="name_3">surname:Alassery;given-names:F.</infon><infon key="name_4">surname:Hajjej;given-names:F.</infon><infon key="name_5">surname:Halasa;given-names:A.</infon><infon key="pub-id_doi">10.1155/2021/1220374</infon><infon key="pub-id_pmid">35047026</infon><infon key="section_type">REF</infon><infon key="source">
Computational Intelligence and Neuroscience
</infon><infon key="type">ref</infon><infon key="volume">2021</infon><infon key="year">2021</infon><offset>33648</offset><text>A survey research on the willingness of gynecologists to employ mobile health applications</text></passage><passage><infon key="fpage">415</infon><infon key="lpage">420</infon><infon key="name_0">surname:Moradi;given-names:M.</infon><infon key="name_1">surname:Madani;given-names:A.</infon><infon key="name_2">surname:Karargyris;given-names:A.</infon><infon key="name_3">surname:Syeda-Mahmood;given-names:T.</infon><infon key="pub-id_doi">10.1117/12.2293971</infon><infon key="pub-id_other">2-s2.0-85047349208</infon><infon key="section_type">REF</infon><infon key="source">
Medical Imaging 2018: Image Processing
</infon><infon key="type">ref</infon><infon key="volume">10574</infon><infon key="year">2018</infon><offset>33739</offset><text>Chest X-ray generation and data augmentation for cardiovascular abnormality classification</text></passage><passage><infon key="fpage">1</infon><infon key="issue">7</infon><infon key="lpage">10</infon><infon key="name_0">surname:Hammoudi;given-names:K.</infon><infon key="name_1">surname:Benhabiles;given-names:H.</infon><infon key="name_2">surname:Melkemi;given-names:M.</infon><infon key="pub-id_doi">10.1007/s10916-021-01745-4</infon><infon key="section_type">REF</infon><infon key="source">
Journal of Medical Systems
</infon><infon key="type">ref</infon><infon key="volume">45</infon><infon key="year">2021</infon><offset>33830</offset><text>Deep learning on chest X-ray images to detect and evaluate pneumonia cases at the era of COVID-19</text></passage><passage><infon key="file">BMRI2022-7833516.001.jpg</infon><infon key="id">fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>33928</offset><text>Chest X-ray images, with their respective labels, belonging to the ChestX-ray14 data set.</text></passage><passage><infon key="file">BMRI2022-7833516.002.jpg</infon><infon key="id">fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34018</offset><text>General architecture of the model based on transfer learning used.</text></passage><passage><infon key="file">BMRI2022-7833516.003.jpg</infon><infon key="id">fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34085</offset><text>Comparison between a typical CNN block and a ResNet residual block.</text></passage><passage><infon key="file">BMRI2022-7833516.004.jpg</infon><infon key="id">fig4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34153</offset><text>Illustrative figure of the dropout technique.</text></passage><passage><infon key="file">BMRI2022-7833516.005.jpg</infon><infon key="id">fig5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34199</offset><text>Dynamic data augmentation.</text></passage><passage><infon key="file">BMRI2022-7833516.006.jpg</infon><infon key="id">fig6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34226</offset><text>Simple hold-out partition of the data set.</text></passage><passage><infon key="file">BMRI2022-7833516.007.jpg</infon><infon key="id">fig7</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34269</offset><text>Subsets made up of pure pathology and “without finding.”</text></passage><passage><infon key="file">BMRI2022-7833516.008.jpg</infon><infon key="id">fig8</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34330</offset><text>Average accuracy results for architecture #1.</text></passage><passage><infon key="file">BMRI2022-7833516.009.jpg</infon><infon key="id">fig9</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34376</offset><text>Comparison of results obtained by filtering by orientation and without filtering.</text></passage><passage><infon key="file">BMRI2022-7833516.010.jpg</infon><infon key="id">fig10</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34458</offset><text>Average accuracy without and with data augmentation for the data sets made up of one pathology and “no finding.”</text></passage><passage><infon key="file">BMRI2022-7833516.011.jpg</infon><infon key="id">fig11</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34575</offset><text>Accuracy vs. epoch for the CNN without transfer learning, trained with the data set composed of the labels “no finding” and “finding.”</text></passage><passage><infon key="file">tab1.xml</infon><infon key="id">tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>34718</offset><text>Pretrained CNN properties used in work.</text></passage><passage><infon key="file">tab1.xml</infon><infon key="id">tab1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Architecture&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Input image size (pixels)&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Depth (layers)&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Parameters (millions)&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;VGG19&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;224 × 224&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;19&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;144&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Inception V3&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;299 × 299&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;48&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;23.9&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ResNet50&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;224 × 224&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;50&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;25.6&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>34758</offset><text>Architecture	Input image size (pixels)	Depth (layers)	Parameters (millions)	 	VGG19	224 × 224	19	144	 	Inception V3	299 × 299	48	23.9	 	ResNet50	224 × 224	50	25.6	 	</text></passage><passage><infon key="file">tab2.xml</infon><infon key="id">tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>34927</offset><text>Proposed architectures from pretrained CNNs.</text></passage><passage><infon key="file">tab2.xml</infon><infon key="id">tab2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Proposed architecture base model&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Scheme&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Multilayer perceptron&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;#1&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ResNet50&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;#1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;#2&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;ResNet50&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;#2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;#3&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;VGG19&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;#1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;#4&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;VGG19&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;#2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;#5&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;InceptionV3&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;#1&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;#6&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;InceptionV3&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;#2&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>34972</offset><text>Proposed architecture base model	Scheme	Multilayer perceptron	 	#1	ResNet50	#1	 	#2	ResNet50	#2	 	#3	VGG19	#1	 	#4	VGG19	#2	 	#5	InceptionV3	#1	 	#6	InceptionV3	#2	 	</text></passage><passage><infon key="file">tab3.xml</infon><infon key="id">tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>35139</offset><text>Training hyperparameters used for the experiments.</text></passage><passage><infon key="file">tab3.xml</infon><infon key="id">tab3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Hyperparameter&lt;/th&gt;&lt;th align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Value&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Optimization algorithm&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Stochastic gradient descent&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Loss algorithm&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Cross-entropy&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Times&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;150&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Training batch size&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;128&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;Validation lot size&lt;/td&gt;&lt;td align=&quot;center&quot; rowspan=&quot;1&quot; colspan=&quot;1&quot;&gt;256&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>35190</offset><text>Hyperparameter	Value	 	Optimization algorithm	Stochastic gradient descent	 	Loss algorithm	Cross-entropy	 	Times	150	 	Training batch size	128	 	Validation lot size	256	 	</text></passage></document></collection>
