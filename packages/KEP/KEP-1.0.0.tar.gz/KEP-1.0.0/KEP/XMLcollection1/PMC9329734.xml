<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220803</date><key>pmc.key</key><document><id>9329734</id><infon key="license">NO-CC CODE</infon><passage><infon key="article-id_doi">10.1016/j.compeleceng.2022.108236</infon><infon key="article-id_pii">S0045-7906(22)00472-4</infon><infon key="article-id_pmc">9329734</infon><infon key="article-id_pmid">35915590</infon><infon key="article-id_publisher-id">108236</infon><infon key="fpage">108236</infon><infon key="kwd">Multi-class classification Clinical dataset COVID-19 Machine learning Class balancing techniques</infon><infon key="license">Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</infon><infon key="lpage">108236</infon><infon key="name_0">surname:Kumar;given-names:Vinod</infon><infon key="name_1">surname:Lalotra;given-names:Gotam Singh</infon><infon key="name_2">surname:Kumar;given-names:Ravi Kant</infon><infon key="section_type">TITLE</infon><infon key="title">Keywords</infon><infon key="type">front</infon><infon key="volume">102</infon><infon key="year">2022</infon><offset>0</offset><text>Improving performance of classifiers for diagnosis of critical diseases to prevent COVID risk</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>94</offset><text>The risk of developing COVID-19 and its variants may be higher in those with pre-existing health conditions such as thyroid disease, Hepatitis C Virus (HCV), breast tissue disease, chronic dermatitis, and other severe infections. Early and precise identification of these disorders is critical. A huge number of patients in nations like India require early and rapid testing as a preventative measure. The problem of imbalance arises from the skewed nature of data in which the instances from majority class are classified correct, while the minority class is unfortunately misclassified by many classifiers. When it comes to human life, this kind of misclassification is unacceptable. To solve the misclassification issue and improve accuracy in such datasets, we applied a variety of data balancing techniques to several machine learning algorithms. The outcomes are encouraging, with a considerable increase in accuracy. As an outcome of these proper diagnoses, we can make plans and take the required actions to stop patients from acquiring serious health issues or viral infections.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>1182</offset><text>Graphical abstract</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1201</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1214</offset><text>As the number of people infected with Corona Virus Disease (COVID-19) began to rapidly rise, even amongst adults, the entire world was concerned about human life. Because the coronavirus (COVID-19) being a novel virus at the time, it was difficult to determine whether it was more infectious in children, young individuals, or the old, or whether it was equally infectious in all. Scientists later discovered that people who are already sick with a variety of diseases have a higher risk of contracting COVID and experiencing catastrophic complications if they become infected. Following this understanding, it has been beneficial to establish plans and decisions to provide work safety rules or immunization plans. As a result, detecting other diseases in people is vital so that they are aware of their present health status and can take the necessary measures, treatments, and immunization regimens. In a country with such a large population as India, keeping proper information of all patients' histories of critical diseases has become a top priority for the government. Various machine learning algorithms are now accessible for categorization in order to test a big population and assess these massive records of patients' ailments. The key issue is finding the right dataset to train these machine learning algorithms on. The majority of datasets in the medical area are imbalanced. When machine learning algorithms are trained using this skewed data, the outcomes are skewed. When the amount of data in one class is significantly less than the amount of data in the other classes, this is known as class imbalance. As a result, in critical fields, like as medicine, erroneous outcomes due to class imbalanced problems cannot be tolerated. Finally, due to the severity of their current disorders, we are unable to achieve our standards for the appropriate implementation of the plan to protect or save persons from COVID-19. We have included several small variants in our research to solve this key problem and improve the accuracy of machine learning systems. A significant enhancement in the accuracy of machine learning algorithms for the multi-classification of diseases is found, even with imbalanced data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3434</offset><text>It is extremely difficult to cure diseases after they have progressed to a critical stage. As a result, obtaining prior information about the sickness is crucial, and machine learning algorithms can be extremely helpful in detecting diseases in less time and with more accuracy. Consequently, we are attempting to improve several machine learning algorithms employing different smote versions in order to address class imbalanced difficulties in the dataset connected to a number of crucial issues.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3933</offset><text>Hepatitis C is a global disease that affects millions of individuals. The Hepatitis C Virus (HCV), a blood-borne virus that is a major cause of liver cancer, causes hepatitis C, a liver illness. Both acute and chronic hepatitis can be caused by HCV. Chronic HCV infection affects an estimated 71 million persons worldwide. According to the World Health Organization, around 399,000 people died from hepatitis C in 2016. Antiviral drugs can treat hepatitis C in 95% of instances, however diagnosis and therapy are tough to come by.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4464</offset><text>Carcinoma (car) is a type of cancer that begins in the cells that make up the skin or the tissue that lines the insides of the organs. Carcinomas are cancerous cells that divide uncontrollably.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4658</offset><text>A fibroadenoma (fad) is a benign (noncancerous) tumor. They normally do not hurt and feel like stones in the breast.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4775</offset><text>Mastopathy (mas) is a benign, hormone-dependent change in the glandular tissue of the breast.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4869</offset><text>Glandular tissue (gla) is the component of the breast that produces milk, and ducts are the tubes that carry milk from the breast to the nipple.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5014</offset><text>The breast's connective (con) tissue gives it shape and support.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5079</offset><text>Breast adipose (adi) tissue in the breast is an endocrine system that secretes a variety of growth hormones and enzymes.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5200</offset><text>Thyroid disease is another frequent condition that affects people all around the world. The gland is a butterfly-shaped organ located in the front and base of the neck. Hormones produced by the thyroid gland play an important role in regulating the functions of various systems throughout the body. Thyroid illness is caused by the thyroid's overproduction or underproduction of hormones. Thyroid disease is divided into four types: hyperthyroidism, hypothyroidism, thyroiditis, and Hashimoto's thyroiditis. Infants, teenagers, men, women, and the elderly are all susceptible to this condition. This is a condition that affects people all throughout the world; roughly 20 million Americans have more than one man. Thyroid illness is diagnosed five to eight times more frequently in women than in men, it can be diagnosed by blood tests, imaging tests, or physical exams. In the last two decades, the number of people diagnosed with cancer has nearly doubled, from an estimated 10 million in 2000 to 19.3 million in 2020. The number of cancer-related deaths has also increased, rising from 6.2 million in 2000 to 10 million by 2020. Breast cancer is one of the most lethal cancers in humans, and it is particularly common in women. </text></passage><passage><infon key="file">gr1_lrg.jpg</infon><infon key="id">fig0001</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>6432</offset><text>Categories of classification process.</text></passage><passage><infon key="file">gr1_lrg.jpg</infon><infon key="id">fig0001</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>6470</offset><text>Fig 1</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6476</offset><text>Skin illnesses are one of the most common medical issues in the world. Skin disease is a multifaceted notion that encompasses the psychological, social, and financial problems of patients. Prevention is better than cure. Medical datasets are the gold standard for developing machine learning-based illness diagnosis systems that are quick, cheap, and accurate. However, medical datasets of patients with class imbalance problems, which means that a greater number of instances may belong to one class called the majority class and another called the minority class, are considered imbalanced data. The issue of class imbalance persists and is ubiquitous, posing issues in the field of machine learning. Because the overall accuracy and decision-making are biased towards the majority class, any machine learning technique built atop class imbalanced datasets suffers from this imbalance problem, which leads to misclassifying minority class instances or viewing them as noise. Furthermore, the medical dataset may include binary or multi-class labels. Building a machine learning-based efficient diagnosis system for disease prediction is more difficult with a multi-label class imbalanced dataset than with a binary class imbalanced dataset. The categories of the classification process are depicted in Fig. 1 . The four datasets-thyroid disease, Hepatitis C Virus (HCV), breast tissue disease, and skin disease are multi-class datasets. Therefore, these four multi-class datasets have been taken up and empirically studied by devising a prediction system based on machine learning methods and class balancing approaches. Classification is supervised learning in which target labels or classes are pre-defined and the classification model allocates items in a collection to those labels or classes. For example, deciding whether or not an email is spam. Predicting one of two classes is known as binary classification. A classification task having more than two classifications is referred to as multi-classification. Multi-label classification originated from the investigation of text categorization problems, where each document may be referred to many specified subjects at the same time. Although a little discrepancy is acceptable, most classification datasets do not have the same number of cases in each class.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>8797</offset><text>The presented research effort is one of the solutions for selecting an appropriate model and a class balancing strategy to maximize the model's performance on multiclass clinical datasets. Diagnostic procedures in the medical industry are both time-consuming and costly. The proposed research output would serve as a tool for categorizing patient disease kinds based on test results and assisting doctors in the clinical decision-making process.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9243</offset><text>The remainder of the paper is laid out as follows: - Section 2 describes the related works and some important research papers in this field. Section 3 introduces applied data balancing and classification techniques. Section 5 explains the experimental analysis, Section 7 discusses the results and discussion, and Section 8 discusses the conclusion and future work.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>9614</offset><text>Related works</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>9628</offset><text>Using re-sampling techniques, you may manage a class imbalance in a simple, easy-to-implement, and understandable way. Data is both over- and under-sampled when it is re-sampled. Samples from the majority group are reduced as a result of under-sampling. The process of creating new data from the minority group to match the size of the majority group is known as over-sampling. The complexity of the dataset rises as the number of classes increases. Linear discriminant analysis is a straightforward and effective method for obtaining accurate and timely findings from multi-class data. This study used benchmark datasets, which included a variety of medical datasets. The Random Forest (RF) outperforms the other two Decision Tree techniques used to diagnose heart arrhythmia, namely Classification and Regression Tree (CART) and C4.5, across many databases using 10-fold cross-validation. The MIT-BIH database and the St. Petersburg Institute of Cardiological Technics 12-lead arrhythmia database were used to test the suggested system, which is a combination model of discrete wavelet transformation (DWT), multiscale principal component analysis (MSPCA) de-noising, and RF classifier. Subhani and Anjum combined clinical and gene expression data in their study and used multiple machine learning algorithms on the consolidated dataset. This method, which involves building learning classifiers from large-scale inter domain integrated datasets, produces remarkably accurate clinical predictions. The k-nearest neighbor (k-NN) model has outperformed the other methods on this integrated dataset in terms of accuracy and performance for multi-class classification. To detect one of the sixteen classes, support vector machine (SVM) based approaches such as one versus all, one against one, and error-correcting codes are used to the normalized data. SVM, Neural network (NN), and Logistic Regression (LR) were used to implement over the heart disease dataset in their comparison study, and the performance was measured using F1-score, receiver operating characteristic (ROC) curve, and area under the curve. In their comparison analysis, Khanna et al. concluded that SVM is superior to the other methods under consideration. The UCI repository has been used to pull fifteen real-world medical datasets. Six algorithms are implemented (Naive Bayes, Logistic Regression, K star, Decision Tree: J48, Neural Network: Multilayer Perceptron, and ZeroR), and accuracy is calculated by averaging the results of ten 10-fold cross-validation runs. The Naive Bayes algorithm outperforms the other algorithms on eight of the fifteen datasets.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>12262</offset><text>Data balancing techniques</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>12288</offset><text>Several methods for improving classifiers trained from imbalanced data have already been proposed. The algorithmic level approach and the data level approach are two approaches to the problem of class imbalance. The algorithmic approach tries to modify current classifier learning methods, such as cost-sensitive learning algorithms, to favor the minority group. The data-level strategy aims to rebalance class dispersion by resampling the data space, which involves over-sampling minority class instances and under-sampling majority class instances. Some data-level methods for class balancing operations are listed below. Fig. 9 illustrates the undersampling and oversampling tasks.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>12974</offset><text>Random oversampling</text></passage><passage><infon key="file">gr2_lrg.jpg</infon><infon key="id">fig0002</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>12994</offset><text>Pseudo code for oversampling.</text></passage><passage><infon key="file">gr2_lrg.jpg</infon><infon key="id">fig0002</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>13024</offset><text>Fig 2</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13030</offset><text>In random oversampling, samples are picked at random from minority classes and then replaced in the training dataset. This means that in random oversampling, examples from minority classes in the training dataset are duplicated, potentially leading to overfitting of machine learning algorithms. The pseudo-code for random oversampling is shown in Fig. 2 .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>13388</offset><text>Under sampling</text></passage><passage><infon key="file">gr3_lrg.jpg</infon><infon key="id">fig0003</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>13403</offset><text>Pseudo code for undersampling.</text></passage><passage><infon key="file">gr3_lrg.jpg</infon><infon key="id">fig0003</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>13434</offset><text>Fig 3</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13440</offset><text>The samples are chosen at random and the training datasets are removed. Random under-sampling, on the other hand, eliminates potentially enormous numbers of samples. Defining the decision border between minority instances and majority instances could be difficult. Because of the rejected samples, which lower classification performance. The pseudo-code for random undersampling is shown in Fig. 3 .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>13841</offset><text>SMOTE</text></passage><passage><infon key="file">gr4_lrg.jpg</infon><infon key="id">fig0004</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>13847</offset><text>Pseudo Code of SMOTE.</text></passage><passage><infon key="file">gr4_lrg.jpg</infon><infon key="id">fig0004</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>13869</offset><text>Fig 4</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>13875</offset><text>A vector space is a collection of feature vectors that represent each sample in an imbalanced dataset with a small number of minority samples compared to a high number of majority samples. For each minority sample, k nearest neighbors are chosen from the minority sample, and then minority sample is chosen at random. Between  and (), a point is chosen at random. () is the name of a newly synthesised sample that is contributed to the dataset. The balance parameter B is used to govern the synthesised samples. B = 1 denotes that the minority and majority categories each have an equal number of samples. F all refers to the overall number of samples to be synthesised, whereas F refers to the number of samples to be synthesised from a single minority sample. The minority sample synthesis is repeated F times. The pseudo code for SMOTE is shown in Fig. 4 .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>14738</offset><text>ADASYN</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>14745</offset><text>More samples from minority samples are generated along the borderline in ADASYN. The ratio of majority samples in the k nearest neighbors of a minority sample (ti) is h[i]). It determines the likelihood of being close to the borderline. It is then normalized in order to calculate h[i] and F[i], the number of samples to synthesize from (ti). The ADAYSN method is implemented from.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>15127</offset><text>SVM SMOTE</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15137</offset><text>The borderline area is determined using this way after the SVM algorithm has been trained. Support vectors were generated from the original training set. Artificial data will be generated at random along the borderline connecting each 'minority class support vector' to a few of its nearest neighbors. As a result, it creates an obvious distinction between the minority and majority classes, SVM-SMOTE is implemented from the work the paper.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>15579</offset><text>Borderline SMOTE</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15596</offset><text>Start.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15603</offset><text>In the minority class Q, for each qi(i = 1, 2, …., qnum), we compute its g-close neighbors from the whole training set S, the number of majority instances amongst the g-nearest neighbors is represented by g′(0 ≤ g′ ≤ g).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>15836</offset><text>IF gi = g, it represents all the k nearest neighbor qi are majority cases, qi is noise and it is not used in the subsequent steps. If ; the number of qi′s widely held nearest neighbors is higher than the number of its minorities, qi is easily mis-classified and placed into a set DANGER. If , qi is all right and it is not required to take part in the following stages.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16210</offset><text>The data on the margins of the minority class Q are used as examples in DANGER, and we can find that DANGER ⊆ Q, We place DANGER = , 0 ≤ dnum ≤ num. From Q, we determine the k closest neighbors of each case in DANGER.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16438</offset><text>Now, we create s ×  dnum artificial positive samples based on the information in DANGER, where s is an integer within (1, k). We choose s nearest neighbor from the k nearest neighbors in Q at random for each ,. First, we determine the differences, diffj(j = 1, 2…..s) between  and its s closest neighbor from, then multiply, diffj by a random number rj (1,2….s) between 0 and 1. After all s new artificial marginal cases are produced between  and its closest neighbors:</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>16919</offset><text>Consider the entire training set is S, Q is the minority class (Q = q 1,q 2,……….qnum) R is the majority class (R = r 1,r 2,……….rnum) where, qnum and rnum are the number of minority and majority instances. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17141</offset><text>, j = 1, 2………s</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17164</offset><text>It can be repeated the above process for each  in DANGER and it can reach s ×  dnum synthetical examples.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>17275</offset><text>SMOTENC</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17283</offset><text>Start.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17290</offset><text>Median computation: The median of standard deviations for all continuous characteristics is estimated for the minority class. If a sample's minimal features deviate from its likely nearest neighbors, the median is already included in the Euclidean distance. The median is used to adjust the difference in nominal features by an amount proportional to the difference in continuous feature values.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>17689</offset><text>Nearest neighbor computation: The Euclidean distance between the feature vector for which k-nearest neighbors are being found (in the minority class sample) and the other feature vectors (of minority class samples) is evaluated using the continuous feature space. The median of the standard deviations estimated before, in the Euclidean distance, for each different nominal feature between the considered feature vector and its possible nearest-neighbor.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18144</offset><text>Populate the synthetic sample: The new synthetic minority class sample's continuous features are created with the same SMOTE method as previously shown. The value found in the majority of the k-nearest neighbors is used as the nominal feature.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18388</offset><text>Stop.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18394</offset><text>Below are the steps of the 'Synthetic Minority Over-sampling Technique-Nominal Continuous [SMOTE-NC]'. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>18498</offset><text>SMOTEEN</text></passage><passage><infon key="file">gr5_lrg.jpg</infon><infon key="id">fig0005</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>18506</offset><text>Pseudo code of SMOTEEN.</text></passage><passage><infon key="file">gr5_lrg.jpg</infon><infon key="id">fig0005</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>18530</offset><text>Fig 5</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>18536</offset><text>At first, SMOTE specifies the k-nearest Neighbors (k-NNs) (denoted by ) for each minority sample xi ∈ αmin. For generating a synthetic data model xnew for xi; SMOTE arbitrarily selects an element  and  in αmin. The feature vector of xnew is the sum of the feature vectors of xi and the value which is obtain by multiplying ( − xi) with a random value δ, where 0 &lt; δ &lt; 1. In this way, we achieve a synthetic point along the line segment join up xi and . Furthermore, the Edited Nearest neighbor (ENN) is utilized to clear up class overlap. The pseudo code for SMOTEEN is shown in Fig. 5 .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>19140</offset><text>SMOTETOMEK</text></passage><passage><infon key="file">gr6_lrg.jpg</infon><infon key="id">fig0006</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>19151</offset><text>Pseudo code of SMOTETOMEK.</text></passage><passage><infon key="file">gr6_lrg.jpg</infon><infon key="id">fig0006</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>19178</offset><text>Fig 6</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19184</offset><text>It is a revised version of SMOTE in which the TOMEK links are employed to filter out the noisy data. The TOMEK links are specified as; Instance ‘l’ is the nearest neighbor of instance m where, m is the nearest neighbor of l and l and m belong to different classes. Pseudo code has been mentioned in the Fig. 6 .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>19501</offset><text>Classification techniques</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19527</offset><text>A concise description of the algorithm of each classification techniques has been described here with the goal of presenting essential insights into six different classifiers.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>19703</offset><text>Linear Discriminant Analysis (LDA)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19738</offset><text>Mean of each input value.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19764</offset><text>Probability of an instance.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19792</offset><text>Covariance for the input data.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>19823</offset><text>The LDA is the preferred linear classification approach when there are more than two classes. The description of LDA is quite simple. It contains statistical data attributes. The mean and variance for each class are determined for a single input variable (x). The LDA model needs the estimation of statistics from the training data for each class. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20172</offset><text>LDA model figures out the mean and variance of data for each class and it is easy to compute in case of univariate (single input variable) scenario with two classes. The mean value of each input (x) for each class (k) can be computed as below-</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20416</offset><text>Where, for the class k, meank is the mean value of x, nk is the number of instances of class k. The variance is computed through all classes taking the average squared difference of each value from the mean.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20628</offset><text>Where, sigma2 denotes the variance of all inputs (n), ‘k’ is the number of classes and meank is the mean of x for the class to which xi belongs, n is the number of instances.</text></passage><passage><infon key="file">gr7_lrg.jpg</infon><infon key="id">fig0007</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>20807</offset><text>Undersampling v/s Oversampling task.</text></passage><passage><infon key="file">gr7_lrg.jpg</infon><infon key="id">fig0007</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>20844</offset><text>Fig 7</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>20850</offset><text>LDA yields estimates by calculating the likelihood that a given set of inputs will fall into each of the classes. The output class with the highest probability stands as the final prediction. The Bayes Theorem is used to calculate the model's probabilities (Fig. 7 ).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>21119</offset><text>Decision Tree (DT)</text></passage><passage><infon key="file">gr8_lrg.jpg</infon><infon key="id">fig0008</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>21138</offset><text>Decision making in COVID-19.</text></passage><passage><infon key="file">gr8_lrg.jpg</infon><infon key="id">fig0008</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>21167</offset><text>Fig 8</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21173</offset><text>It is a flowchart-like tree structure in which each internal node represents a test on an attribute, each branch represents the test's outcome, and class labels have been symbolized by a leaf node, making it a Decision Tree. The root node is the highest node in a tree. It should be acknowledged that no root-to- leaf path should get the same discrete attribute more than once A Decision Tree can quickly create comprehensible rules and do classification tasks with less computation. It can be illustrated simply as in Fig. 8 .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>21702</offset><text>Support vector machine (SVM)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>21731</offset><text>V. Vapnik produced a very effective and well-known categorization system in 1992, called SVM. In this the hyperplane mechanism is applied to split two data points that are on opposite sides of the hyperplane. The purpose is to raise the margin so that there is a significant deciding gap between the instances and the hyperplane on both sides. The segregating hyperplane is depicted in Eq. (3). </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22133</offset><text>Where, W = {w 1, w 2,w 3……wn} designates the weight vector, ‘n’: number of attributes, X is n dimensional vector; and ‘b’ stances for a scalar (a bias).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22302</offset><text>D= {(xi,yi /xi ∈ Rn, yi ∈ ( − 1, 1)), Where D stands for dataset-  </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>22382</offset><text>Gaussian Naive Bayes (GNB)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22409</offset><text>If the majority of the attributes in the instances are continuous, Gaussian Naive Bayes is applied. The conditional probability is computed using the equation in Eq. (6). The Bayesian theorem is used for the Naive Bayes classifier technique. When coping with complex classification, the Gaussian Naive Bayes method is preferable. </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22741</offset><text>Where x = variable, c = Class, μ = mean and σ is variance of predictor distribution</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>22835</offset><text>k-nearest neighbor (k-NN)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>22861</offset><text>The k-Nearest Neighbor (k-NN) prediction model is widely recognized as a lazy learning (no learning) approach. The prediction is based on the k-nearest numbers supplied to it as input. In general, the Euclidian distance formula is used to calculate the neighborhood. Other distance measures, such as Minkowski, Hamming, and Manhattan distances, are used as needed. The neighborhood distance dist(x,y) from points ‘x’ to ‘y’ is decided using the formula as given in Eq. (7). </text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_2</infon><offset>23346</offset><text>Artificial Neural Network (ANN)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23378</offset><text>Artificial Neural Networks (ANNs) abridge and mimic the activities of brain. An ANN is a set of modules identified as artificial neurons that admit input, modify their inner state (activation) in reply to the input, and produce output based on the activation function and input supplied.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23666</offset><text>Weights (W): {w 1, w 2,w 3……wn} signifies the neuron strength.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23735</offset><text>Input layer: this integrates inputs and corresponding weights.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23798</offset><text>Bias (b): The curve of the activation function is adjusted by Bias.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23866</offset><text>Hidden layer: The hidden layer has summation as well as activation function. The ANN may be composed of many hidden layers.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>23990</offset><text>Activation function: The activation function, which offers neural networks non-linear features, is a crucial component. It primarily transforms any artificial neuron (AN) input into output. Following that, the acquired output is sent into the next layer of AN. Many activation functions are available such as Radial basis function, SoftMax, Tanh (), etc. The widely used sigmoid function has been mentioned in Eq. (8).</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24409</offset><text>(8)</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>24413</offset><text>Output layer: The set of outcomes generated by the preceding layer is given to Output layer.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>24506</offset><text>Experimental design</text></passage><passage><infon key="file">gr9_lrg.jpg</infon><infon key="id">fig0009</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>24526</offset><text>Prediction System for multi classification over medical datasets.</text></passage><passage><infon key="file">gr9_lrg.jpg</infon><infon key="id">fig0009</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>24592</offset><text>Fig 9</text></passage><passage><infon key="file">gr10_lrg.jpg</infon><infon key="id">fig0010</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>24598</offset><text>Step-by-step procedure of proposed architecture.</text></passage><passage><infon key="file">gr10_lrg.jpg</infon><infon key="id">fig0010</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>24647</offset><text>Fig 10</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24654</offset><text>In this segment, we report a wide-ranging experimental setup that aim to appraise the performance of different multi-classification models with different sets of data balancing techniques as accurately as possible. The whole experiment was carried out by building multi-classification models in “Scikit-learn” a python-based machine learning library. To achieve a class balanced dataset, another python-based python package - imbalanced-learn which offers a number of re-sampling methods normally used in datasets for class balancing. The pictorial and algorithmic representation of the proposed prediction system for multi classification over medical datasets are given in Fig. 9, Fig. 10 , respectively.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25364</offset><text> Datasets </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25375</offset><text>Thyroid disease</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25391</offset><text>This research is conducted on four clinical datasets: Thyroid disease, HCV, breast tissue disease, and skin disease and these were downloaded from the UCI machine learning repository. A brief discussion of these datasets is given as follows- </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25634</offset><text>Hepatitis C Virus (HCV) for the Egyptian patients</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>25684</offset><text>The thyroid dataset was composed of 7200 samples comprising 21 attributes and was classified into three different classes. Out of 21 attributes, 15 attributes contain real values and the other 6 attributes have categorical values. The first class represents Hyperthyroid which has 166 instances in it. The second class belongs to Hypothyroid having 368 instances, and the third label is normal (not hypothyroid) and possesses 6666 instances. The research problem is to determine whether a patient referred to the clinic is hypothyroid or not. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26228</offset><text>Breast tissue disease</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26250</offset><text>This dataset was collected from Egyptian patients who underwent treatment dosages for HCV for nearly 18 months. It contains 1385 instances with 29 features and four target classes, namely: &quot;Portal Fibrosis’, ‘Few Septa', 'Many septa’, ‘Many septa’ with a number of instances of 336, 332, 355, and 362, respectively. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26577</offset><text>Dermatology/ skin disease dataset</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26611</offset><text>This breast tissue disease dataset was recorded as electrical impedance measurements of newly excised tissue instances from the breast. The dataset contains 106 numbers of instances and consists of 10 features with real values. The dataset can be used for predicting the classification into six classes namely- ‘car’, ‘fad’, ‘mas’, ‘gla’, ‘con’, and ‘adi’. </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26994</offset><text>This multiclass dataset contains 366 Number of Instances with 33 features of categorical nature and have six target disease classes namely- ‘Psoriasis’, ‘Lichen planus’, ‘Seboreic dermatitis’, ‘Pityriasis rosea’, ‘Cronic dermatitis’ and ‘Pityriasis rubra pilaris’. The main aim is to categorize the type of Eryhemato-Squamous (i.e., common skin disease) from this dataset.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>27391</offset><text>Performance evaluation criteria</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27423</offset><text>For classification purposes, there exist various supervised machine learning models. But for a particular problem, which model is appropriate, it can only be known by the comparative assessments. Some of the most popular and widely used performance evaluation metrics for classifiers are as follows:</text></passage><passage><infon key="file">tbl0001.xml</infon><infon key="id">tbl0001</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>27723</offset><text>Confusion matrix of a classifier.</text></passage><passage><infon key="file">tbl0001.xml</infon><infon key="id">tbl0001</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot;&gt;Class&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;1&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;2&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;K&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;11&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;12&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;1k&lt;/sub&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;2&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;21&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;22&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;2k&lt;/sub&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;K&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;k1&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;k2&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;kk&lt;/sub&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>27757</offset><text>Table 1	 	</text></passage><passage><infon key="file">tbl0001.xml</infon><infon key="id">tbl0001</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot;&gt;Class&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;1&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;2&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;K&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;11&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;12&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;1k&lt;/sub&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;2&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;21&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;22&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;2k&lt;/sub&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;K&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;k1&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;k2&lt;/sub&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;&lt;bold&gt;–&lt;/bold&gt;&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;m&lt;sub&gt;kk&lt;/sub&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>27768</offset><text>Class	1	2	–	K	 	1	m11	m12	–	m1k	 	2	m21	m22	–	m2k	 	–	–	–	–	–	 	K	mk1	mk2	–	mkk	 	</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27869</offset><text>Contingency / Confusion Matrix: A table that shows how well a model acts on test data for which the target values are known. As illustrated in Table 1 , the diagonal indicates successfully identified occurrences and the off-diagonal shows classification errors.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28132</offset><text>Accuracy (TN +TP/ (TN + TP+FN+FP)) computes a classifier's ability to identify only the truthful cases for each class.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28253</offset><text>Precision computes a classifier's ability to identify only the truthful cases for each class.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28348</offset><text>Recall (TP / (TN + FP)) is a classifier's ability to find all truthful cases in a class.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28440</offset><text>F1-score {2*((precision*recall) / (precision + recall))}: This shows a perfect balance because precision and recall are inversely related. When both recall and precision are important, then a high F1-score is useful.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>28659</offset><text>A macro-average finds the metric independently for each class in order to calculate the average, whereas a micro-average computes the metric from the aggregate contributions of all classes. A micro-average is used for imbalanced datasets because it considers the frequency of each class. Micro-averaging and macro-averaging are methods for calculating a single number for each of the precision, recall, and other metrics across multiple classes.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>29107</offset><text>Results and discussion</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>29130</offset><text>The proposed work investigates and compares the applicability of six different types of classifiers and nine significant sampling techniques (1. Random Oversampling, 2. Random Under sampling, 3. SMOTE, 4. ADASYN, 5.SVM SMOTE, 6. Borderline SMOTE, 7. SMOTENC, 8. SMOTEEN, and 9. SMOTETOMEK) to predict over four publicly available clinical datasets. Resampled datasets and original datasets were fed to six different classifiers one by one and assessed in terms of accuracy, precision, recall, and F1-score. During the model execution, the total size of the dataset was split into 60% instances as training data and 40% instances as test data. Moreover, k-fold cross validation is used to improve performance. Where k is the number of iterations, a model will be trained and validated wherein a different fold of data is held out for validation for each iteration and the leftover k-1 fold is often used for learning.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30048</offset><text>Each classifier has been tested under the following parameter settings-</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30120</offset><text>LDA: Solver:'svd' (Singular value decomposition). </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30171</offset><text>SVM: Kernel: 'linear’, max_iter: −1 (for no limit), gamma:'scale’ (kernel coefficient), decision_function_shape: 'ovr' (one-vs-rest), Degree: 3 (Degree of the polynomial kernel function ('poly')). </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30375</offset><text>GNB:(Prior probabilities of the classes) priors: None, var_smoothing:1e-9. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30451</offset><text>MLP: random_state:1,  hidden_layer_sizes:(10, 10, 10), max_iter: 103, solver: adam, activation: relu, learning_rate_init:10−3, learning_rate: constant, alpha: 10−4 (Regularization). </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30641</offset><text>k-NN: n_neighbors:3, weights: uniform, leaf_size:30, p:2, metric:'minkowski', algorithm: ’auto’. </text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>30745</offset><text>Decision Tree (DT) random_state:None, ccp_alpha:0.0, splitter:'best', criterion:'gini', max_depth:None, min_samples_leaf:1, min_samples_split:2 min_impurity_decrease:0.0, min_impurity_split: None, class_weight:None, max_features:None,  min_weight_fraction_leaf:0.0, max_leaf_nodes:None. </text></passage><passage><infon key="file">tbl0002.xml</infon><infon key="id">tbl0002</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>31040</offset><text>Thyroid disease dataset: Samples distribution of each Label in Class.</text></passage><passage><infon key="file">tbl0002.xml</infon><infon key="id">tbl0002</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot;&gt;SNo.&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;Balancing Method&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;Samples distribution of each Label in Class [Hyperthyroid →1; Hypothyroid→2; Normal →3]&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;None&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 166, 2: 368, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;2&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Under Sampling&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 166, 2: 166, 3: 166&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;3&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ROS&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6666, 2: 6666, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6666, 2: 6666, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ADASYN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6644, 2: 6704, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;6&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SVM SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6666, 2: 6666, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;7&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Borderline SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6666, 2: 6666, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;8&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTENC&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6666, 2: 6666, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;9&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTEEN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6650, 2: 6200, 3: 5314&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;10&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTETOMEK&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6665, 2: 6616, 3: 6615&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>31110</offset><text>Table 2	 	</text></passage><passage><infon key="file">tbl0002.xml</infon><infon key="id">tbl0002</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot;&gt;SNo.&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;Balancing Method&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;Samples distribution of each Label in Class [Hyperthyroid →1; Hypothyroid→2; Normal →3]&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;None&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 166, 2: 368, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;2&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Under Sampling&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 166, 2: 166, 3: 166&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;3&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ROS&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6666, 2: 6666, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6666, 2: 6666, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ADASYN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6644, 2: 6704, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;6&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SVM SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6666, 2: 6666, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;7&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Borderline SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6666, 2: 6666, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;8&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTENC&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6666, 2: 6666, 3: 6666&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;9&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTEEN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6650, 2: 6200, 3: 5314&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;10&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTETOMEK&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 6665, 2: 6616, 3: 6615&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>31121</offset><text>SNo.	Balancing Method	Samples distribution of each Label in Class [Hyperthyroid →1; Hypothyroid→2; Normal →3]	 	1	None	1: 166, 2: 368, 3: 6666	 	2	Under Sampling	1: 166, 2: 166, 3: 166	 	3	ROS	1: 6666, 2: 6666, 3: 6666	 	4	SMOTE	1: 6666, 2: 6666, 3: 6666	 	5	ADASYN	1: 6644, 2: 6704, 3: 6666	 	6	SVM SMOTE	1: 6666, 2: 6666, 3: 6666	 	7	Borderline SMOTE	1: 6666, 2: 6666, 3: 6666	 	8	SMOTENC	1: 6666, 2: 6666, 3: 6666	 	9	SMOTEEN	1: 6650, 2: 6200, 3: 5314	 	10	SMOTETOMEK	1: 6665, 2: 6616, 3: 6615	 	</text></passage><passage><infon key="file">tbl0003.xml</infon><infon key="id">tbl0003</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>31627</offset><text>HCV disease dataset (Samples distribution of each Label in Class).</text></passage><passage><infon key="file">tbl0003.xml</infon><infon key="id">tbl0003</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot;&gt;S.No.&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;Balancing Method&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;Samples distribution of each Label in Class [Portal Fibrosis→1; Few Septa→2; Many septa→3, Many septa→4]&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;None&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 336, 2: 332, 3: 355, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;2&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Under Sampling&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 332, 2: 332, 3: 332, 4: 332&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;3&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ROS&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 362, 2: 362, 3: 362, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 362, 2: 362, 3: 362, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ADASYN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 362, 2: 362, 3: 362, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;6&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SVM SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 362, 2: 362, 3: 362, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;7&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Borderline SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 362, 2: 362, 3: 362, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;8&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTENC&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 362, 2: 362, 3: 362, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;9&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTEEN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 12, 2: 13, 3: 10, 4: 7&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;10&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTETOMEK&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 233, 2: 238, 3: 224, 4: 223&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>31694</offset><text>Table 3	 	</text></passage><passage><infon key="file">tbl0003.xml</infon><infon key="id">tbl0003</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot;&gt;S.No.&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;Balancing Method&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;Samples distribution of each Label in Class [Portal Fibrosis→1; Few Septa→2; Many septa→3, Many septa→4]&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;None&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 336, 2: 332, 3: 355, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;2&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Under Sampling&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 332, 2: 332, 3: 332, 4: 332&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;3&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ROS&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 362, 2: 362, 3: 362, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 362, 2: 362, 3: 362, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ADASYN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 362, 2: 362, 3: 362, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;6&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SVM SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 362, 2: 362, 3: 362, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;7&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Borderline SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 362, 2: 362, 3: 362, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;8&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTENC&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 362, 2: 362, 3: 362, 4: 362&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;9&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTEEN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 12, 2: 13, 3: 10, 4: 7&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;10&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTETOMEK&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 233, 2: 238, 3: 224, 4: 223&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>31705</offset><text>S.No.	Balancing Method	Samples distribution of each Label in Class [Portal Fibrosis→1; Few Septa→2; Many septa→3, Many septa→4]	 	1	None	1: 336, 2: 332, 3: 355, 4: 362	 	2	Under Sampling	1: 332, 2: 332, 3: 332, 4: 332	 	3	ROS	1: 362, 2: 362, 3: 362, 4: 362	 	4	SMOTE	1: 362, 2: 362, 3: 362, 4: 362	 	5	ADASYN	1: 362, 2: 362, 3: 362, 4: 362	 	6	SVM SMOTE	1: 362, 2: 362, 3: 362, 4: 362	 	7	Borderline SMOTE	1: 362, 2: 362, 3: 362, 4: 362	 	8	SMOTENC	1: 362, 2: 362, 3: 362, 4: 362	 	9	SMOTEEN	1: 12, 2: 13, 3: 10, 4: 7	 	10	SMOTETOMEK	1: 233, 2: 238, 3: 224, 4: 223	 	</text></passage><passage><infon key="file">tbl0004.xml</infon><infon key="id">tbl0004</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>32281</offset><text>Breast tissue disease dataset (Samples distribution of each Label in Class).</text></passage><passage><infon key="file">tbl0004.xml</infon><infon key="id">tbl0004</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot;&gt;S.No.&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;Balancing Method&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;Samples distribution of each Label in Class [car→1; fad→2; mas→3; gla→4; con→5; adi→6]&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;None&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 42, 2: 30, 3: 36, 4: 32, 5: 28, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;2&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Under Sampling&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 28, 2: 28, 3: 28, 4: 28, 5: 28, 6: 28&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;3&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ROS&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ADASYN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;6&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SVM SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;7&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Borderline SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;8&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTENC&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;9&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTEEN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 34, 2: 20, 3: 14, 4: 27, 6: 34, 5: 30&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;10&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTETOMEK&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>32358</offset><text>Table 4	 	</text></passage><passage><infon key="file">tbl0004.xml</infon><infon key="id">tbl0004</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot;&gt;S.No.&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;Balancing Method&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;Samples distribution of each Label in Class [car→1; fad→2; mas→3; gla→4; con→5; adi→6]&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;None&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 42, 2: 30, 3: 36, 4: 32, 5: 28, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;2&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Under Sampling&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 28, 2: 28, 3: 28, 4: 28, 5: 28, 6: 28&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;3&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ROS&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ADASYN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;6&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SVM SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;7&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Borderline SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;8&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTENC&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;9&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTEEN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 34, 2: 20, 3: 14, 4: 27, 6: 34, 5: 30&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;10&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTETOMEK&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>32369</offset><text>S.No.	Balancing Method	Samples distribution of each Label in Class [car→1; fad→2; mas→3; gla→4; con→5; adi→6]	 	1	None	1: 42, 2: 30, 3: 36, 4: 32, 5: 28, 6: 44	 	2	Under Sampling	1: 28, 2: 28, 3: 28, 4: 28, 5: 28, 6: 28	 	3	ROS	1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44	 	4	SMOTE	1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44	 	5	ADASYN	1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44	 	6	SVM SMOTE	1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44	 	7	Borderline SMOTE	1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44	 	8	SMOTENC	1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44	 	9	SMOTEEN	1: 34, 2: 20, 3: 14, 4: 27, 6: 34, 5: 30	 	10	SMOTETOMEK	1: 44, 2: 44, 3: 44, 4: 44, 5: 44, 6: 44	 	</text></passage><passage><infon key="file">tbl0005.xml</infon><infon key="id">tbl0005</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>33036</offset><text>Skin disease dataset (Samples distribution of each Label in Class).</text></passage><passage><infon key="file">tbl0005.xml</infon><infon key="id">tbl0005</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot;&gt;&lt;bold&gt;S.No.&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;&lt;bold&gt;Balancing Method&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;&lt;bold&gt;Samples distribution of each Label in Class&lt;/bold&gt; [Psoriasis→1; Seboreic dermatitis→2, Lichen planus→3, Pityriasis rosea→4, Cronic dermatitis→5, Pityriasis rubra pilaris→6]&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;None&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 61, 3: 72, 4: 49, 5: 52, 6: 20&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;2&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Under Sampling&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 20, 2: 20, 3: 20, 4: 20, 5: 20, 6: 20&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;3&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ROS&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 112, 3: 112, 4: 112, 5: 112, 6: 112&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 112, 3: 112, 4: 112, 5: 112, 6: 112&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ADASYN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1:112, 2: 104, 3: 112, 4: 120, 5: 113, 6: 113&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;6&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SVM SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 112, 3: 91, 4: 112, 5: 112, 6: 69&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;7&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Borderline SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 112, 3: 72, 4: 112, 5: 112, 6: 20&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;8&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTENC&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 112, 3: 112, 4: 112, 5: 112, 6: 112&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;9&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTEEN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 96, 2: 79, 3: 112, 4: 89, 5: 101, 6: 111&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;10&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTETOMEK&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 112, 3: 112, 4: 112, 5: 112, 6: 112&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>33104</offset><text>Table 5	 	</text></passage><passage><infon key="file">tbl0005.xml</infon><infon key="id">tbl0005</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th valign=&quot;top&quot;&gt;&lt;bold&gt;S.No.&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;&lt;bold&gt;Balancing Method&lt;/bold&gt;&lt;/th&gt;&lt;th valign=&quot;top&quot;&gt;&lt;bold&gt;Samples distribution of each Label in Class&lt;/bold&gt; [Psoriasis→1; Seboreic dermatitis→2, Lichen planus→3, Pityriasis rosea→4, Cronic dermatitis→5, Pityriasis rubra pilaris→6]&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;1&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;None&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 61, 3: 72, 4: 49, 5: 52, 6: 20&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;2&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Under Sampling&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 20, 2: 20, 3: 20, 4: 20, 5: 20, 6: 20&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;3&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ROS&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 112, 3: 112, 4: 112, 5: 112, 6: 112&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;4&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 112, 3: 112, 4: 112, 5: 112, 6: 112&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;5&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;ADASYN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1:112, 2: 104, 3: 112, 4: 120, 5: 113, 6: 113&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;6&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SVM SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 112, 3: 91, 4: 112, 5: 112, 6: 69&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;7&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;Borderline SMOTE&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 112, 3: 72, 4: 112, 5: 112, 6: 20&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;8&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTENC&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 112, 3: 112, 4: 112, 5: 112, 6: 112&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;9&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTEEN&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 96, 2: 79, 3: 112, 4: 89, 5: 101, 6: 111&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot;&gt;10&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;SMOTETOMEK&lt;/td&gt;&lt;td valign=&quot;top&quot;&gt;1: 112, 2: 112, 3: 112, 4: 112, 5: 112, 6: 112&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>33115</offset><text>S.No.	Balancing Method	Samples distribution of each Label in Class [Psoriasis→1; Seboreic dermatitis→2, Lichen planus→3, Pityriasis rosea→4, Cronic dermatitis→5, Pityriasis rubra pilaris→6]	 	1	None	1: 112, 2: 61, 3: 72, 4: 49, 5: 52, 6: 20	 	2	Under Sampling	1: 20, 2: 20, 3: 20, 4: 20, 5: 20, 6: 20	 	3	ROS	1: 112, 2: 112, 3: 112, 4: 112, 5: 112, 6: 112	 	4	SMOTE	1: 112, 2: 112, 3: 112, 4: 112, 5: 112, 6: 112	 	5	ADASYN	1:112, 2: 104, 3: 112, 4: 120, 5: 113, 6: 113	 	6	SVM SMOTE	1: 112, 2: 112, 3: 91, 4: 112, 5: 112, 6: 69	 	7	Borderline SMOTE	1: 112, 2: 112, 3: 72, 4: 112, 5: 112, 6: 20	 	8	SMOTENC	1: 112, 2: 112, 3: 112, 4: 112, 5: 112, 6: 112	 	9	SMOTEEN	1: 96, 2: 79, 3: 112, 4: 89, 5: 101, 6: 111	 	10	SMOTETOMEK	1: 112, 2: 112, 3: 112, 4: 112, 5: 112, 6: 112	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33903</offset><text>Table 2, Table 3, Table 4, Table 5 contains the sample distribution of each class label before and after the application of class balancing techniques in the thyroid disease dataset, HCV disease dataset, breast tissue disease dataset, and skin disease dataset.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>34164</offset><text>Performance comparison of different classifiers with different smote variants over thyroid disease dataset</text></passage><passage><infon key="file">gr11_lrg.jpg</infon><infon key="id">fig0011</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34271</offset><text>Accuracy of six classifiers with different balancing techniques for thyroid disease dataset.</text></passage><passage><infon key="file">gr11_lrg.jpg</infon><infon key="id">fig0011</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>34364</offset><text>Fig 11</text></passage><passage><infon key="file">gr12_lrg.jpg</infon><infon key="id">fig0012</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34371</offset><text>Precision of six classifiers with different balancing techniques for thyroid disease dataset.</text></passage><passage><infon key="file">gr12_lrg.jpg</infon><infon key="id">fig0012</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>34465</offset><text>Fig 12</text></passage><passage><infon key="file">gr13_lrg.jpg</infon><infon key="id">fig0013</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34472</offset><text>Recall of six classifiers with different balancing techniques for thyroid disease dataset.</text></passage><passage><infon key="file">gr13_lrg.jpg</infon><infon key="id">fig0013</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>34563</offset><text>Fig 13</text></passage><passage><infon key="file">gr14_lrg.jpg</infon><infon key="id">fig0014</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34570</offset><text>F1-score of six classifiers with different balancing techniques for thyroid disease dataset.</text></passage><passage><infon key="file">gr14_lrg.jpg</infon><infon key="id">fig0014</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>34663</offset><text>Fig 14</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>34670</offset><text>In this section, we have expressed the results of the experiments performed over thyroid disease dataset. From Fig. 11, Fig. 12, Fig. 13, Fig. 14 , shows the six classifier's performance measures in terms of accuracy, precision, recall and F1-score value with and without applying class balancing techniques over the thyroid dataset.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>35004</offset><text>The Decision Tree (DT) with SMOTEEN was found superior in accuracy results, where it could classify above 96.9% of all the instances correctly, and so this combination has the highest measures in the thyroid dataset. In some cases, the accuracy may be the misleading claim, therefore, a better way to assess the performance of the classifier is to confirm it by precision (97.3%), recall (96.9%) and F1-Score (96.9%) also. Alternatively, sampling techniques ROS and ADSYN with DT also show comparable results (accuracy-94.3%, precision-95.2%, recall-94.3%, and F1-score of 94.3% approximately same) in the prediction of thyroid disease.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>35641</offset><text>The classifier's performance with the SMOTEEN class balancing technique in descending order came as: DT&gt;SVM&gt;GNB&gt;k-NN&gt;LDA&gt;ANN. Experimental result projected Decision Tree (DT) with SMOTEEN could distinguish (Hyperthyroid, Hypothyroid and Normal) 97.3% correctly and stood as the best performer whereas ANN could classify only 33.3% correctly and fell as the poorest performer in the case of thyroid disease prediction. Moreover, there is a significant 9% performance improvement in prediction when the prediction was done by DT with SMOTEEN and without class sampling technique.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>36219</offset><text>Performance comparison of different classifiers with different smote variants over Hepatitis C Virus (HCV) dataset</text></passage><passage><infon key="file">gr15_lrg.jpg</infon><infon key="id">fig0015</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>36334</offset><text>Accuracy of six classifiers with different balancing techniques for HCV disease dataset.</text></passage><passage><infon key="file">gr15_lrg.jpg</infon><infon key="id">fig0015</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>36423</offset><text>Fig 15</text></passage><passage><infon key="file">gr16_lrg.jpg</infon><infon key="id">fig0016</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>36430</offset><text>Precision of six classifiers with different balancing techniques for HCV disease dataset.</text></passage><passage><infon key="file">gr16_lrg.jpg</infon><infon key="id">fig0016</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>36520</offset><text>Fig 16</text></passage><passage><infon key="file">gr17_lrg.jpg</infon><infon key="id">fig0017</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>36527</offset><text>Recall of six classifiers with different balancing techniques for HCV disease dataset.</text></passage><passage><infon key="file">gr17_lrg.jpg</infon><infon key="id">fig0017</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>36614</offset><text>Fig 17</text></passage><passage><infon key="file">gr18_lrg.jpg</infon><infon key="id">fig0018</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>36621</offset><text>F1-score of six classifiers with different balancing techniques for HCV disease dataset.</text></passage><passage><infon key="file">gr18_lrg.jpg</infon><infon key="id">fig0018</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>36710</offset><text>Fig 18</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>36717</offset><text>This experimental demonstration was done to classify all four class labels (Portal Fibrosis→1; Few Septa→2; Many septa→3, Many septa→4) in Hepatitis C Virus (HCV) disease to identify the correct type of HCV disease. Based on the experimental results as presented in Fig. 15, Fig. 16, Fig. 17, Fig. 18 , the following conclusion was drawn- None of the classifiers could achieve acceptable classification performance in terms of precision, accuracy, recall and F1-score. The highest accuracy 41.2% is achieved by k-NN with SMOTEEN class balancing technique. This means its misclassification is 59.8% which recommends not to take the help of any of these models (DT, SVM, GNB, k-NN, LDA, and ANN) for prediction of HCV disease in patients. The possible reasons are hidden in the format of the dataset which contains 1385 instances with 29 features and four target classes. It has a diverse and large number of attribute values. The attribute value ranges have wide differences in minimum and maximum values (Features like RNA Base, RNA 4, RNA 12, RNA EOT, and RNA EF). In addition to this, range value fluctuation amongst different attributes is also affective learning by different models. This has caused overfitting in the model while learning over the HCV dataset. For the reasons stated earlier all the models underperformed. In other words, models are not able to generalize on a new dataset. Moreover, approximately a 17.6% increase in the performance of the k-NN model with and without using SMOTEEN class balancing techniques was observed.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>38271</offset><text>Performance comparison of different classifiers with different smote variants over breast tissue dataset</text></passage><passage><infon key="file">gr19_lrg.jpg</infon><infon key="id">fig0019</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>38376</offset><text>Accuracy of six classifiers with different balancing techniques for breast tissue dataset.</text></passage><passage><infon key="file">gr19_lrg.jpg</infon><infon key="id">fig0019</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>38467</offset><text>Fig 19</text></passage><passage><infon key="file">gr20_lrg.jpg</infon><infon key="id">fig0020</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>38474</offset><text>Precision of six classifiers with different balancing techniques for breast tissue dataset.</text></passage><passage><infon key="file">gr20_lrg.jpg</infon><infon key="id">fig0020</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>38566</offset><text>Fig 20</text></passage><passage><infon key="file">gr21_lrg.jpg</infon><infon key="id">fig0021</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>38573</offset><text>Recall of six classifiers with different balancing techniques for breast tissue dataset.</text></passage><passage><infon key="file">gr21_lrg.jpg</infon><infon key="id">fig0021</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>38662</offset><text>Fig 21</text></passage><passage><infon key="file">gr22_lrg.jpg</infon><infon key="id">fig0022</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>38669</offset><text>F1-score of six classifiers with different balancing techniques for breast tissue dataset.</text></passage><passage><infon key="file">gr22_lrg.jpg</infon><infon key="id">fig0022</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>38760</offset><text>Fig 22</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>38767</offset><text>In the task of distinguishing six class labels (car→1; fad→2; mas→3; gla→4; con→5; adi→6) in breast tissue disease. The experimental evaluation has been conducted in terms of accuracy, precision, recall, and F1-score results are presented in Fig. 19, Fig. 20, Fig. 21, Fig. 22 , respectively. In combination with the SMOTEEN sampling technique, the highest accuracy achieved by different classifiers-DT, SVM, GNB, k-NN, LDA, and ANN is 99.9%,92.2%, 90%, 89.1%, 82.8%, and 33.3%, respectively. Alternatively, ROS with different classifiers- DT, SVM, LDA, k-NN, GNB, and ANN obtained the second highest accuracies −94.3%, 84%, 72.6%, 71.7%, 61.3%, and 40.6%, respectively. Evidently, these classification models have also shown similar trends in the cases of precision (Fig. 16), recall (Fig. 17), and F1-score (Fig. 18) measurements.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39617</offset><text>SMOTEEN improved the Decision Tree (DT) classifier's accuracy by 10% when compared to the Decision Tree without any sampling technique.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>39753</offset><text>When we compare the contributions of all nine class balancing techniques to improving accuracy, we discovered the following ordering from best to worst: SMOTEEN &gt; ROS &gt; ADASYN &gt; BorderlineSMOTE &gt; SMOTE &gt; SMOTENC &gt; Undersampling &gt; SVM-SMOTE &gt;SMOTETOMEK.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40006</offset><text>In this study, the Decision Tree (DT) classifier proved to be a promising classification model, with 96.9% ability to correctly classify and 97.3% trustworthiness in predicting multiple disease types in breast disease. In contrast to Decision Tree (DT) classifier, the ANN proved as the poorest classification model with 42.5% ability to correctly classify and 43.5% trustable in making multiple disease types in breast disease prediction.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>40446</offset><text>Performance comparison of different classifiers with different smote variants skin disease dataset</text></passage><passage><infon key="file">gr23_lrg.jpg</infon><infon key="id">fig0023</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>40545</offset><text>Accuracy of six classifiers with different balancing techniques for skin disease dataset.</text></passage><passage><infon key="file">gr23_lrg.jpg</infon><infon key="id">fig0023</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>40635</offset><text>Fig 23</text></passage><passage><infon key="file">gr24_lrg.jpg</infon><infon key="id">fig0024</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>40642</offset><text>Precision of six classifiers with different balancing techniques for skin disease dataset.</text></passage><passage><infon key="file">gr24_lrg.jpg</infon><infon key="id">fig0024</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>40733</offset><text>Fig 24</text></passage><passage><infon key="file">gr25_lrg.jpg</infon><infon key="id">fig0025</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>40740</offset><text>Recall of six classifiers with different balancing techniques for skin disease dataset.</text></passage><passage><infon key="file">gr25_lrg.jpg</infon><infon key="id">fig0025</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>40828</offset><text>Fig 25</text></passage><passage><infon key="file">gr26_lrg.jpg</infon><infon key="id">fig0026</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>40835</offset><text>F1-score of six classifiers with different balancing techniques for skin disease dataset.</text></passage><passage><infon key="file">gr26_lrg.jpg</infon><infon key="id">fig0026</infon><infon key="section_type">FIG</infon><infon key="type">fig</infon><offset>40925</offset><text>Fig 26</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>40932</offset><text>In the process of identifying six class labels (Psoriasis→1; Seboreic dermatitis→2, Lichen planus→3, Pityriasis rosea→4, Cronic dermatitis→5, Pityriasis rubra pilaris→6) in the skin disease dataset, the various classification models with different class balancing techniques have been trained and tested for their performance. Experimental evaluation of different models is demonstrated in terms of accuracy, precision, recall, and F1-score in Fig. 23, Fig. 24, Fig. 25, Fig. 26 , respectively.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>41439</offset><text>In combination with the SMOTE sampling technique, the highest accuracy achieved by different classifiers-SVM, LDA, DT, ANN, k-NN, GNB, is 99.3%, 97.4%, 97%, 95.5%, 92.2%, and 88.5%, respectively. SMOTENC with LDA, SVM, and DT attained promising accuracies of 99.3%, 98.1, and 98.1%, respectively. In addition to this, SMOTENC with LDA, SVM, and DT also shows correspondingly better values of precision (Fig. 24), recall (Fig. 25), and F1-score (Fig. 26).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>41897</offset><text>In the case of the skin disease dataset, all six classification models proved promising with different sampling techniques, and the overall performance of the model reached up to 99.3% accuracy, which is positively supported by precision, recall, and F1-score values. The reason for high performance lies in the multiclass dataset, which contains only 366 instances with 33 features of categorical nature. In general, due to the application of sampling technique, a 2% improvement in accuracy was observed in distinguishing the six class labels in the skin disease dataset. Except for the undersampling technique, all the other eight class balancing techniques gave similar influences on the performance improvement of all six classifiers under the study.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>42653</offset><text>ROS (Random Over Sampling) improves binary classification by generating synthetic balanced cases from continuous and categorical data using conditional density estimates of the two classes. SMOTE is a data augmentation approach that generates synthetic data points by slightly moving a data item in the direction of its sibling. It ensures that the synthetic data point is not a perfect replica but also not too distinct from known minority class observations. For different minority classes that are more difficult to learn, ADASYN uses a weighted distribution. As a result, the categorization decision boundary is improved. The reason for adopting SVM-SMOTE to improve classification is because it is more concerned with data separation. Synthetic data is generated at random along the support vectors. Because Borderline-SMOTE only generates synthetic data along the decision boundary of two classes, it is more suited to binary classification. For datasets with mixed (categorical and continuous) features, SMOTE-NC (Nominal and Continuous) is more appropriate. SMOTE-ENN combines SMOTE's ability to generate synthetic instances for minority groups with ENN's ability to delete some extraneous features, resulting in improved classification accuracy. SMOTE Tomek is a hybrid approach that combines Tomek's under-sampling with Tomek's oversampling (SMOTE). Oversampling adds data to the minority class, whereas undersampling removes data from the majority class that is unnecessary or unimportant. As a result, a well-balanced dataset is formed, which increases machine learning algorithms' classification accuracy.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>44272</offset><text>Conclusion and future work</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>44299</offset><text>People with pre-existing health issues such as like thyroid disease, Hepatitis C Virus (HCV), breast tissue disease, Chronic Dermatitis etc., may be at a higher risk of developing deadly COVID-19 and its mutant variants. So, early, and correct identification of these diseases is essential. Machine learning algorithms can assist in the early and quick diagnosis of numerous diseases in a huge population like India. However, machine learning algorithms perform poorly when a dataset does contain a class imbalance issue. Smote versions are used to overcome the class imbalanced problem. The experimental work revealed that combining class balancing strategies with classification models enhanced the overall performance of all six classifiers over clinical datasets. The combination of SMOTEEN and Decision Tree often gave better results in distinguishing the class labels in clinical datasets (Thyroid, Hepatitis C Virus, breast tissue disease and skin disease). Consequently, this study can be taken as a key insight for guidance to identify patient and would help the doctor in clinical decision support system. It advocates the recommended classification and class balancing technique with respect to specific data to able to detect the disease automatically with high accuracy. In our next study, we will investigate and build the most effective multi-class balancing method to handle the multi-class imbalance problem in data.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>45733</offset><text>Declaration of Competing Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>45767</offset><text>None.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>45773</offset><text>References</text></passage><passage><infon key="element-citation">https://www.who.int/news-room/fact-sheets/detail/hepatitis-c</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>45784</offset></passage><passage><infon key="name_0">surname:Dubey;given-names:A.K.</infon><infon key="name_1">surname:Mohbey;given-names:K.K.</infon><infon key="pub-id_doi">10.1080/07391102.2022.2034668</infon><infon key="section_type">REF</infon><infon key="source">J Bimol Struct Dyn</infon><infon key="type">ref</infon><infon key="year">2022</infon><offset>45785</offset><text>Enabling CT-scans for COVID detection using transfer learning-based neural networks</text></passage><passage><infon key="name_0">surname:Schmid;given-names:S.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>45869</offset></passage><passage><infon key="fpage">57</infon><infon key="issue">1</infon><infon key="lpage">75</infon><infon key="name_0">surname:Kratzsch;given-names:J.</infon><infon key="name_1">surname:Pulzer;given-names:F.</infon><infon key="pub-id_pmid">18279780</infon><infon key="section_type">REF</infon><infon key="source">Best Pract Res Clin Endocrinol Metab</infon><infon key="type">ref</infon><infon key="volume">22</infon><infon key="year">2008</infon><offset>45870</offset><text>Thyroid gland development and defects</text></passage><passage><infon key="fpage">83</infon><infon key="issue">2</infon><infon key="lpage">92</infon><infon key="name_0">surname:Sheehan;given-names:M.T.</infon><infon key="pub-id_pmid">27231117</infon><infon key="section_type">REF</infon><infon key="source">Clin Med Res</infon><infon key="type">ref</infon><infon key="volume">14</infon><infon key="year">2016</infon><offset>45908</offset><text>Biochemical testing of the thyroid: TSH is the best and, oftentimes, only test needed-a review for primary care</text></passage><passage><infon key="element-citation">hhttps://www.who.int/news/item/03-02-2021-breast-cancer-now-most-common-form-of-cancer-who-taking-action</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>46020</offset></passage><passage><infon key="fpage">271</infon><infon key="issue">3</infon><infon key="lpage">283</infon><infon key="name_0">surname:Basra;given-names:M.K.A.</infon><infon key="name_1">surname:Shahrukh;given-names:M.</infon><infon key="pub-id_pmid">19527100</infon><infon key="section_type">REF</infon><infon key="source">Expert Rev Pharmacoecon Outcomes Res</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2009</infon><offset>46021</offset><text>Burden of skin diseases</text></passage><passage><infon key="element-citation">http://archive.ics.uci.edu/ml</infon><infon key="name_0">surname:Frank;given-names:A.</infon><infon key="name_1">surname:Asuncion;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">and Computer Science. Irvine</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>46045</offset></passage><passage><infon key="name_0">surname:Kumar;given-names:V.</infon><infon key="pub-id_doi">10.1007/s00521-020-05204-y</infon><infon key="section_type">REF</infon><infon key="source">Neural Comput Appl</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>46046</offset><text>Evaluation of computationally intelligent techniques for breast cancer diagnosis</text></passage><passage><infon key="fpage">2429</infon><infon key="issue">15</infon><infon key="lpage">2437</infon><infon key="name_0">surname:Li;given-names:T.</infon><infon key="name_1">surname:Zhang;given-names:C.</infon><infon key="name_2">surname:Ogihara;given-names:M.</infon><infon key="pub-id_doi">10.1093/bioinformatics/bth267</infon><infon key="pub-id_pmid">15087314</infon><infon key="section_type">REF</infon><infon key="source">Bioinformatics</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2004</infon><offset>46127</offset><text>A comparative study of feature selection and multiclass classfication methods for tissue classification based on gene expression</text></passage><passage><infon key="fpage">453</infon><infon key="issue">4</infon><infon key="lpage">472</infon><infon key="name_0">surname:Li;given-names:T.</infon><infon key="name_1">surname:Zhu;given-names:S.</infon><infon key="name_2">surname:Ogihara;given-names:M.</infon><infon key="pub-id_doi">10.1007/s10115-006-0013-y</infon><infon key="section_type">REF</infon><infon key="source">Knowl Inf Syst</infon><infon key="type">ref</infon><infon key="volume">10</infon><infon key="year">2006</infon><offset>46256</offset><text>Using discriminant analysis for multi-class classification: an experimental investigation</text></passage><passage><infon key="fpage">1</infon><infon key="issue">4</infon><infon key="lpage">12</infon><infon key="name_0">surname:Alickovic;given-names:E.</infon><infon key="name_1">surname:Subasi;given-names:A.</infon><infon key="pub-id_doi">10.1007/s10916-016-0467-8</infon><infon key="pub-id_pmid">26573639</infon><infon key="section_type">REF</infon><infon key="source">J Med Syst</infon><infon key="type">ref</infon><infon key="volume">40</infon><infon key="year">2016</infon><offset>46346</offset><text>Medical decision support system for diagnosis of heart arrhythmia using DWT and random forests classifier</text></passage><passage><infon key="pub-id_doi">10.48550/arXiv.2006.07879</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>46452</offset></passage><passage><infon key="name_0">surname:Mustaqeem;given-names:A.</infon><infon key="name_1">surname:Anwar;given-names:S.M.</infon><infon key="name_2">surname:Majid;given-names:M.</infon><infon key="pub-id_doi">10.1155/2018/7310496</infon><infon key="section_type">REF</infon><infon key="source">Comput Math Methods Med</infon><infon key="type">ref</infon><infon key="volume">2018</infon><infon key="year">2018</infon><offset>46453</offset><text>Multiclass classification of cardiac arrhythmia using improved feature selection and SVM invariants</text></passage><passage><infon key="fpage">414</infon><infon key="issue">5</infon><infon key="lpage">419</infon><infon key="name_0">surname:Khanna;given-names:D.</infon><infon key="name_1">surname:Sahu;given-names:R.</infon><infon key="name_2">surname:Baths;given-names:V.</infon><infon key="name_3">surname:Deshpande;given-names:B.</infon><infon key="pub-id_doi">10.7763/ijmlc.2015.v5.544</infon><infon key="section_type">REF</infon><infon key="source">Int J Mach Learn Comput</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2015</infon><offset>46553</offset><text>Comparative study of classification techniques (SVM, logistic regression and neural networks) to predict the prevalence of heart disease</text></passage><passage><infon key="fpage">1166</infon><infon key="issue">9</infon><infon key="name_0">surname:Al-Aidaroos;given-names:K.M.</infon><infon key="name_1">surname:Bakar;given-names:A.A.</infon><infon key="name_2">surname:Othman;given-names:Z.</infon><infon key="section_type">REF</infon><infon key="source">Inf Technol J</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2012</infon><offset>46690</offset><text>Medical data classification with Naive Bayes approach</text></passage><passage><infon key="fpage">321</infon><infon key="lpage">357</infon><infon key="name_0">surname:Chawla;given-names:N.V.</infon><infon key="name_1">surname:Bowyer;given-names:K.W.</infon><infon key="name_2">surname:Hall;given-names:L.O.</infon><infon key="name_3">surname:Kegelmeyer;given-names:W.P.</infon><infon key="pub-id_doi">10.1613/jair.953</infon><infon key="section_type">REF</infon><infon key="source">J Artif Intell Res</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">2002</infon><offset>46744</offset><text>SMOTE: synthetic minority over-sampling technique</text></passage><passage><infon key="comment">IJCNN 2008. (IEEE World Congr. Comput. Intell)</infon><infon key="fpage">1322</infon><infon key="lpage">1328</infon><infon key="name_0">surname:He;given-names:H.</infon><infon key="name_1">surname:Bai;given-names:Y.</infon><infon key="name_2">surname:Garcia;given-names:E.A.</infon><infon key="name_3">surname:Li;given-names:S.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the IEEE international joint conference on neural networks</infon><infon key="type">ref</infon><infon key="year">2008</infon><offset>46794</offset></passage><passage><infon key="fpage">228</infon><infon key="lpage">236</infon><infon key="name_0">surname:Shirai;given-names:K.</infon><infon key="name_1">surname:Xiang;given-names:Y.</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 33rd Pacific Asia conference on language, information and computation PACLIC, no. Paclic 33</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>46795</offset></passage><passage><infon key="issue">1</infon><infon key="name_0">surname:Nguyen;given-names:H.M.</infon><infon key="name_1">surname:Cooper;given-names:E.W.</infon><infon key="name_2">surname:Kamei;given-names:K.</infon><infon key="pub-id_doi">10.1504/ijkesdp.2011.039875</infon><infon key="section_type">REF</infon><infon key="source">Int J Knowl Eng Soft Data Paradig</infon><infon key="type">ref</infon><infon key="volume">3</infon><infon key="year">2011</infon><offset>46796</offset><text>Borderline over-sampling for imbalanced data classification</text></passage><passage><infon key="comment">(including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics) LNCS</infon><infon key="fpage">878</infon><infon key="lpage">887</infon><infon key="name_0">surname:Han;given-names:H.</infon><infon key="name_1">surname:Wang;given-names:W.Y.</infon><infon key="name_2">surname:Mao;given-names:B.H.</infon><infon key="pub-id_doi">10.1007/11538059_91</infon><infon key="section_type">REF</infon><infon key="source">Lect Notes Comput Sci</infon><infon key="type">ref</infon><infon key="volume">3644</infon><infon key="year">2005</infon><offset>46856</offset><text>Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning</text></passage><passage><infon key="name_0">surname:Kovács;given-names:G.</infon><infon key="pub-id_doi">10.1016/j.asoc.2019.105662</infon><infon key="section_type">REF</infon><infon key="source">Appl Soft Comput J</infon><infon key="type">ref</infon><infon key="volume">83</infon><infon key="year">2019</infon><offset>46934</offset><text>An empirical comparison and evaluation of minority oversampling techniques on a large number of imbalanced datasets</text></passage><passage><infon key="issue">1</infon><infon key="name_0">surname:Batista;given-names:G.E</infon><infon key="name_1">surname:Prati;given-names:R.C.</infon><infon key="name_2">surname:Monard;given-names:M.C.</infon><infon key="pub-id_doi">10.1145/1007730.1007735</infon><infon key="section_type">REF</infon><infon key="source">ACM SIGKDD Explor Newsl</infon><infon key="type">ref</infon><infon key="volume">6</infon><infon key="year">2004</infon><offset>47050</offset><text>A study of the behavior of several methods for balancing machine learning training data</text></passage><passage><infon key="fpage">221</infon><infon key="issue">3</infon><infon key="lpage">234</infon><infon key="name_0">surname:Quinlan;given-names:J.R.</infon><infon key="section_type">REF</infon><infon key="source">Int J Man Mach Stud</infon><infon key="type">ref</infon><infon key="volume">27</infon><infon key="year">1987</infon><offset>47138</offset><text>Simplifying Decision Trees</text></passage><passage><infon key="fpage">660</infon><infon key="issue">3</infon><infon key="lpage">674</infon><infon key="name_0">surname:Safavian;given-names:S.R.</infon><infon key="name_1">surname:Landgrebe;given-names:D.</infon><infon key="pub-id_doi">10.1109/21.97458</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans Syst Man Cybern</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">1991</infon><offset>47165</offset><text>A survey of Decision Tree classifier methodology</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">title_1</infon><offset>47214</offset><text>Data availability</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">paragraph</infon><offset>47232</offset><text>Data will be made available on request.</text></passage><passage><infon key="section_type">SUPPL</infon><infon key="type">footnote</infon><offset>47272</offset><text>Edited by: Sunil Kumar Singh</text></passage></document></collection>
