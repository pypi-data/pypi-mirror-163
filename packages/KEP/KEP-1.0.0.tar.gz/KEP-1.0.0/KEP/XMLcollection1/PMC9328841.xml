<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20220803</date><key>pmc.key</key><document><id>9328841</id><infon key="license">NO-CC CODE</infon><passage><infon key="article-id_doi">10.1016/j.knosys.2022.109539</infon><infon key="article-id_pii">S0950-7051(22)00775-4</infon><infon key="article-id_pmc">9328841</infon><infon key="article-id_pmid">35915642</infon><infon key="article-id_publisher-id">109539</infon><infon key="elocation-id">109539</infon><infon key="kwd">ML, Machine Learning CNN, Convolutional Neural Network DL, Deep Learning MFCC, Mel-frequency Cepstral Coefficients P, Positive subjects R, Recovered subjects H, Healthy control subjects NS, Nasal Swab PCR, Polymerase Chain Reaction-based molecular swabs 1E, Vowel /e/ vocal task 2S, Sentence vocal task 3C, Cough vocal task PvsH, Positive versus Healthy subjects comparison RvsH, Recovered versus Healthy subjects comparison SVM, Support Vector Machine CFS, Correlation-based Feature Selection RF, Random Forest ReLu, Rectified Linear Unit ROC, Receiver-Operating Curve COVID-19 Speech processing Classification Deep learning Adaboost</infon><infon key="license">Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</infon><infon key="name_0">surname:Costantini;given-names:Giovanni</infon><infon key="name_1">surname:Cesarini;given-names:Valerio</infon><infon key="name_10">surname:Cassaniti;given-names:Irene</infon><infon key="name_11">surname:Baldanti;given-names:Fausto</infon><infon key="name_12">surname:Saggio;given-names:Giovanni</infon><infon key="name_2">surname:Robotti;given-names:Carlo</infon><infon key="name_3">surname:Benazzo;given-names:Marco</infon><infon key="name_4">surname:Pietrantonio;given-names:Filomena</infon><infon key="name_5">surname:Di Girolamo;given-names:Stefano</infon><infon key="name_6">surname:Pisani;given-names:Antonio</infon><infon key="name_7">surname:Canzi;given-names:Pietro</infon><infon key="name_8">surname:Mauramati;given-names:Simone</infon><infon key="name_9">surname:Bertino;given-names:Giulia</infon><infon key="section_type">TITLE</infon><infon key="title">Abbreviations Keywords</infon><infon key="type">front</infon><infon key="year">2022</infon><offset>0</offset><text>Deep learning and machine learning-based voice analysis for the detection of COVID-19: A proposal and comparison of architectures</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>130</offset><text>Alongside the currently used nasal swab testing, the COVID-19 pandemic situation would gain noticeable advantages from low-cost tests that are available at any-time, anywhere, at a large-scale, and with real time answers. A novel approach for COVID-19 assessment is adopted here, discriminating negative subjects versus positive or recovered subjects. The scope is to identify potential discriminating features, highlight mid and short-term effects of COVID on the voice and compare two custom algorithms. A pool of 310 subjects took part in the study; recordings were collected in a low-noise, controlled setting employing three different vocal tasks. Binary classifications followed, using two different custom algorithms. The first was based on the coupling of boosting and bagging, with an AdaBoost classifier using Random Forest learners. A feature selection process was employed for the training, identifying a subset of features acting as clinically relevant biomarkers. The other approach was centred on two custom CNN architectures applied to mel-Spectrograms, with a custom knowledge-based data augmentation. Performances, evaluated on an independent test set, were comparable: Adaboost and CNN differentiated COVID-19 positive from negative with accuracies of 100% and 95% respectively, and recovered from negative individuals with accuracies of 86.1% and 75% respectively. This study highlights the possibility to identify COVID-19 positive subjects, foreseeing a tool for on-site screening, while also considering recovered subjects and the effects of COVID-19 on the voice. The two proposed novel architectures allow for the identification of biomarkers and demonstrate the ongoing relevance of traditional ML versus deep learning in speech analysis.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>1895</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>1908</offset><text>Voice production is a human skill relying on complex interactions between multiple systems, including air sources (lungs), a vibration mechanism (vocal folds), and resonant cavities (nasal, oral, pharyngeal and cranial). Such a skill is controlled by the brain and influenced by multiple surrounding factors, such as global health conditions, hydration and body temperature. Consequently, vocal samples may hold high informative content, since voice modifications may reflect the status of all the mentioned components. With these regard, distinctive vocal alterations have been identified and studied in various pathologies, mainly using machine learning (ML)-based methods, yielding encouraging outcomes. Understandably, research focused extensively on primary affections of the phonatory apparatus: specifically, Suppa et al.  investigated essential tremor, Teixeira et al.  assessed chronic laryngitis, Costa et al.  investigated vocal fold edema, Petrovic-Lazic et al.  revealed vocal polyps, and Alves et al.  reviewed the changes in voice quality related to hydration conditions and Zacharia et al.  explored head and neck cancer. Interestingly, promising results were also highlighted for pathologies leading to vocal alterations only secondarily, such as neurodegenerative pathologies (Parkinson’s, SLA) , Down syndrome , and even cardiovascular disorders  as well as movement disorders .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3324</offset><text>That being said, the COVID-19 pandemic represents an ideal and urgent field of application for this line of research. As a matter of fact, since SARS-CoV-2 can affect both the respiratory apparatus and the nervous system , it is possible to surmise that the disease may likely alter the sound of both voice and cough. Therefore, technologies able to detect vocal biomarkers of this infection could provide national health authorities with additional and non-invasive surveillance strategies.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>3817</offset><text>Deep learning (DL), especially based on Convolutional Neural Networks (CNN) applied to spectral images, is commonly considered as the major alternative to traditional ML pipeline methods for speech assessment. Indeed, it does offer advantages which include the extraction of very complex features – through repeated non-linear data transformation – and its completely data-driven nature, which allows to forgo data pre-processing. However, DL is also considered to perform unsatisfactorily on small datasets, requiring larger ones by its very nature , preferably in association with data augmentation procedures that inherently bring a degree of knowledge-based processing. Furthermore, DL are computational heavy and based on a large number of high-level parameters. Examples are studies by Sztahó et al.  and Nissar et al. , which highlighted the possibilities of DL for pathological speech assessment, although datasets and accuracies are comparable to those obtained using ML methods for the same pathologies, namely Parkinson’s disease  and dysphonia .</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>4889</offset><text>As for COVID-19, Laguarta  applied a CNN model on crowdsourced cough sounds, recognizing patients with a 97% accuracy. Similarly, Imran et al.  obtained an accuracy higher than 90% through principal component analysis (PCA) and data processing using mel-frequency cepstral coefficients (MFCC). The algorithms presented by Pinkas  and Shimon  yielded average accuracies around 83%, while Despotovic  achieved 88% in accuracy considering vocal, speech, cough and breathing crowdsourced sounds. To the best of our knowledge, the only study involving recovered subjects is the one by Suppakitjanusant et al. , using CNN with a mean accuracy of 74%. More recently, the Interspeech 2021 conference proposed the DiCOVA challenge, with teams testing algorithms for COVID-19 detection on crowdsourced voice samples , with a baseline mean accuracy of 73% and the highest one being 87%.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>5774</offset><text>All in all, we consider DL and traditional ML to be equally powerful and their usage to be significantly problem-dependent, so that here we employ fine-tuned versions of both to compare the results they can furnish. The chosen vocal tasks consisted in a sentence, a sustained vowel, and solicited coughing, so to gather somehow different informative content. The study population was comprised of three matched matched groups: COVID-19 positive patients, COVID-19 recovered individuals, and healthy subjects as controls.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>6295</offset><text>We adopted a multifaceted approach based on state-of-the-art ML algorithms, training an AdaBoost-based algorithm and a CNN-based one independently, albeit SVM classifiers  were considered as well. With these regards, we consider our innovation to be in the construction of a relatively homogeneous, polished dataset, with suitable domain-specific pre-processing and the usage of custom ML algorithm, which in turn detail and expand the existing state-of-the-art of voice analysis. In addition to the high accuracy and sensitivity results, we found acoustic vocal biomarkers for the identification and study of COVID-19, also taking into account the staging of the disease and its recovery, and began foreseeing a potential automatic tool for the real-time remote pre-screening. More on this will be detailed in the Discussion section.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>7131</offset><text>Materials</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>7141</offset><text>Study population</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7158</offset><text>Three groups of subjects were enrolled in the present study, namely #70 COVID-19 positive patients (group P, or P for short), #120 recovered COVID-19 individuals (R) who were initially proven positive and then negative, and #120 healthy control subjects (H) who never got infected. All of them were of Caucasian ethnicity. Average age and gender were: 57 yo (range 39–67), 57% male, for P subjects; 53 yo (range 39–69), 52% male, for R subjects; 50 yo (range 29–57), 54% male, for H subjects. Informed consent was obtained from all participants and all data was pseudonymized. Patient enrollment was carried out at three different Italian institutions: “San Matteo” University Hospital in Pavia (Otolaryngology Unit, ethical approval number 20200053388), “Tor Vergata” University Hospital in Rome (Otolaryngology Unit, ethical approval number 0012909/2020), and the “Dei Castelli” Hospital in Rome (General Medicine Unit, ethical approval number 0064181/2020).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8137</offset><text>Patients of group P were recruited within ten days from nasal swab (NS) positivity (RT-PCR), and COVID-19 pneumonia was diagnosed clinically and radiologically through a chest computed tomography (CT) scan. Subjects of group R were initially tested positive via RT-PCR NS and subsequently proved negative with two consecutive swabs. Finally, subjects of group H, recruited among hospital staff members and their acquaintances, had no COVID-19 symptoms, nor reported unprotected exposure to confirmed or suspected COVID-19 cases, with their serum samples, collected at least 20 days after the vocal tests, yielding negative results for both IgG and IgM antibodies.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8801</offset><text>To guarantee as homogeneous as possible a dataset for voice analysis, in addition to the average age and gender distribution being approximately matching among the three groups, data regarding clinical and demographic characteristics of the study population was also collected. Non-smokers represented almost half of participants in each study group (51% for group P, 54% for group R, 52% for group H). As far as clinical features are concerned, one or more COVID-19 symptoms (muscle pain, dyspnea, asthenia) were present in 78% and 75% of P and R subjects, respectively. Conversely, at the time of recording, cough was reported by 49% and 8% of P and R subjects, respectively. Finally, in order to minimize the heterogeneity of the dataset, more polarizing characteristics which could greatly affect breathing, articulation or voice emissions (like C-PAP therapy) were deemed as exclusion criteria. Table 1 depicts the main inclusion and exclusion criteria for all groups.</text></passage><passage><infon key="file">tbl1.xml</infon><infon key="id">tbl1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>9778</offset><text>Inclusion and exclusion criteria.</text></passage><passage><infon key="file">tbl1.xml</infon><infon key="id">tbl1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Inclusion criteria&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;P&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;R&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;H&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Exclusion criteria&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;P&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;R&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;H&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;18–80 yo age range&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Drugs acting on CNS&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;European ethnicity&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Head/neck cancer&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Italian native speaker&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Lung cancer&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Positive NS (&amp;lt; 10 days)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;NA&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;NA&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Chemoradiation therapy&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Two consecutive negative NS&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;NA&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;NA&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;C-PAP Therapy&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;LUS &lt;inline-formula&gt;&lt;mml:math id=&quot;d1e200&quot; display=&quot;inline&quot; altimg=&quot;si1.svg&quot;&gt;&lt;mml:mo&gt;≤&lt;/mml:mo&gt;&lt;/mml:math&gt;&lt;/inline-formula&gt; 3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;NA&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;NA&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Tracheal intubation&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Negative SS test (&amp;lt; 20 days)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;NA&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;NA&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Tracheostomy&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;✔&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>9812</offset><text>Inclusion criteria	P	R	H	Exclusion criteria	P	R	H	 	18–80 yo age range	✔	✔	✔	Drugs acting on CNS	✔	✔	✔	 	European ethnicity	✔	✔	✔	Head/neck cancer	✔	✔	✔	 	Italian native speaker	✔	✔	✔	Lung cancer	✔	✔	✔	 	Positive NS (&lt; 10 days)	NA	✔	NA	Chemoradiation therapy	✔	✔	✔	 	Two consecutive negative NS	NA	✔	NA	C-PAP Therapy	✔	✔	✔	 	LUS  3	NA	✔	NA	Tracheal intubation	✔	✔	✔	 	Negative SS test (&lt; 20 days)	NA	NA	✔	Tracheostomy	✔	✔	✔	 	</text></passage><passage><infon key="file">tbl1.xml</infon><infon key="id">tbl1</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>10311</offset><text>Abbreviations: NS: SARS-CoV-2 nasal swab for RNA detection; LUS: lung ultrasound score; SS: SARS-CoV-2 serum sample for IgM and IgG quantification; CNS: Central Nervous System; C-PAP: Continuous Positive Airway Pressure; LUS: Lung Ultrasound score; NA: not applicable.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>10580</offset><text>Vocal tasks and recordings</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10607</offset><text>We considered different vocal tasks to gather heterogeneous informative content, namely: the sustained vowel/e/ (1E); the popular Italian proverb “A caval donato non si guarda in bocca” (2S) and solicited cough (3C).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10828</offset><text>The vowel sound involves a quasi-periodic vibration of the vocal folds. Furthermore,/e/ is produced keeping the larynx in an almost neutral position, therefore avoiding artifacts due to excessive muscular contraction , while still being able to reflect possible pathological alterations of the lower respiratory tract. The sentence was necessary to study the vocal characteristics of speech (including prosody) and their deviations. Furthermore, the selected saying holds a prevalence of plosive consonants, the phonation of which is associated to the production and explosive emission of a relevant amount of air. Finally, cough sounds were selected since they can be reflective of possible alterations of the lungs and of the lower respiratory tract as a whole .</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11595</offset><text>One participant, at turn, was comfortably seated, with arms resting on the armrests at the centre of the room, and was asked to perform each vocal task twice, to select the best ones (one for each task) in terms of noise and intelligibility. Each participant was asked to sustain the vowel steadily for 2 to 5 s without straining, then to pronounce the sentence without pausing between words and at a natural speaking tone, and finally to cough for three consecutive times.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12070</offset><text>The recordings were captured through a smartphone (Y6s, by Huawei Technologies Co., Ltd., Shenzhen, China), kept at about 20 cm from the mouth, with the aid of a web-app (https://covid19.voicewise.it). Audio was recorded in .wav format, sampled at 44.1 kHz, with a resolution of 16-bit. We opted for a smartphone so that it could become an easily adoptable solution for a worldwide and low-cost adoption, as detailed in the Discussion. Recording sessions were held in rooms which were comparable in terms of acoustics and dimensions and had an appropriately quite environment (low-reverberation and low-noise levels). Recordings were only accepted when no hiss nor hum noises were detectable; additionally, no machines nor background voices were captured. The overall audio quality (background noise, reverberation, intelligibility) was subsequently assessed by ear by independent audio engineers.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>12970</offset><text>The vowel and sentence audio files were trimmed so to remove noises and silence at the beginning or the end. The cough files, originally comprehending three coughs in one recording, were split into three different files to isolate each single cough sound.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13226</offset><text>Trimming was performed automatically with custom-made routines in MATLAB (by Mathworks Inc., Natick, Massachusetts, USA ) based on compression and expansion followed by a “lowess” (locally weighted scatterplot) smoothing , band-pass filtering, and thresholds based on RMS Energy and MFCC. All recordings were also examined by sound experts to check the correct trimming; manual corrections were eventually applied when necessary using audio editing solutions available within the digital audio workstation REAPER (Cockos Inc., San Francisco, Calfornia, USA).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>13791</offset><text>Datasets for classification</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>13819</offset><text>Since three different vocal tasks were considered, three binary classifications were necessary for each comparison. For the comparison between groups P and H (PvsH), in order to have the same number of instances in each class, we down-sampled 70 out of the 120 subjects of group H with age-range and gender distributions similar to those of group P. Consequently, the PvsH comparison was based on datasets of 70 subjects per task. Therefore, 70 instances for both the tasks 1E and 2S, as well as 210 instances for the task 3C, were analyzed for each group.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14376</offset><text>For the comparison between groups R and H (RvsH), the final datasets consisted of 120 subjects in total. That translated to 120 instances for tasks 1E and 2S, and 360 instances for the task 3C.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>14570</offset><text>Finally, the total dataset was split randomly as follows: 85% as the training set, 15% as the validation set, which was never fed to any algorithm. Since the task 3C originally involved three instances per subject, only one of those was retained in the validation set. The other two were left out of the validation set as they would have been redundant, being representative of the same subject. For each binary classification (PvsH and RvsH) two different approaches were explored (Adaboost and CNN), each one encompassing three sub-classifiers, one for each speech task (1E, 2S, 3C).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>15156</offset><text>Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15164</offset><text>The following paragraphs describe the two different approaches used for both classifications: a Random Forest-based Adaboost approach applied on selected acoustic features, and two CNN architectures applied to augmented mel-spectrograms.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>15402</offset><text>Adaboost approach</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15420</offset><text>The Adaboost classifier-based approach is divided into four steps for each sub-classifier, as follows:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15523</offset><text>Audio feature extraction, with features reported into a single data matrix with two classes;</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15616</offset><text>Feature selection, using a Correlation-based Feature Selection algorithm (CFS) with a Forward Greedy Stepwise search method;</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15741</offset><text>Additional feature selection, with a wrapper-based ranker embedding a linear SVM classifier;</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15834</offset><text>Training of an AdaBoost classifier with Random Forest weak learners.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>15903</offset><text>All the Machine-Learning processes (steps 2 to 4) were implemented within the Weka platform (University of Waikato, New Zealand, GNU General Public License ), while the feature extraction was performed using OpenSMILE (Audeering GmbH, Gilching, Germany ).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>16161</offset><text>Feature extraction</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16180</offset><text>The feature set we chose had a total of 6373 acoustic features defined within the INTERSPEECH2016 Computational Paralinguistics Challenge (ComParE) , carrying several computational functionals (e.g. mean, position of peak, quartiles, delta coefficients) in the time, spectrum and cepstrum domains . Some of the relevant features include the relative spectral (RASTA) PLP coefficients , the voicing probability , and the spectral loudness summation .</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>16636</offset><text>Feature selection</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>16654</offset><text>To address the discrepancy between the number of features and the number of training instances, we performed a feature reduction, according to , using a correlation-based feature selection (CFS) algorithm. Specifically, this algorithm takes into account the redundancy of the features in a subset, and their eventual correlation with the class itself . The basic principle is the computation of a merit factor for subsets of features, according to the equation:  where k is the number of features in the subset S,  is the average correlation between each feature in the subset and the class, and  is the average cross-correlation between all the features one with each other.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>17338</offset><text>A forward greedy stepwise search method, chosen as a good trade-off between computational time and exhaustiveness , allowed for the selection of the subset, resulting in a number of features which spans from 1% to 3% of the full 6373-features set. A further reduction was applied to the feature subsets, which was also useful to work with a homogeneous number of features throughout all sub-classifications. A wrapper-based feature selector employing a soft-margins linear SVM , trained with Platt’s SMO Optimizer  on a single feature at a time, was used to perform ranking. We empirically considered the first 50 ranked features, and retained them as the final set to train the Adaboost classifier.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>18043</offset><text>Classification</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18058</offset><text>According to literature, SVM and tree-based classifiers – Random Forests (RF) in particular – are the most common solutions for audio-related classification, especially when it comes to speech analysis . For the present study, we adopted RF as a boosted learner in reason of its strength when it comes to non-linear classification problems . The AdaBoost M1 method was applied employing RF internal classifiers (“weak learners”). This approach was chosen for its proven effectiveness in voice classification . In addition, basing on literature  and on our experience, boosting proved beneficial to voice analysis when using more complex weak learners.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>18722</offset><text>Adaboost M1 is a “boosting” technique aimed at generalizing ensembles of weak learners by running on various weight distributions over the training data, finally combining the obtained classifiers into one . Weights are updated so that training examples which are difficult to classify get assigned a higher weight. Ultimately, among all the possible alternatives, the final predicted label is the one which maximizes the sum of the (logarithmic) reciprocals of the prediction error. In particular, the predicted label of the RF classifier is decided by majority voting over simple decision trees trained on different subsets of the training set (“bags” sampled with repetition) and features .</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>19426</offset><text>AdaBoost with RF has proven effective especially for problems where RF itself represents a suitable solution, providing further improvement in error rates, mostly in case of non-linear dependencies and dataset-related complexities . Although both bagging and boosting aim at producing a low variance hypothesis combining higher variance ones, they actually succeed in the task in different ways: the former creates subsets of the training data and works in parallel on them, while the latter manages the training space as a whole, repeatedly weighing it with different distributions.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20011</offset><text>For the present study, in order to emphasize the different dynamics of boosting and bagging, each bag for the RF bootstrapping was built using 80% of the whole training set on each bag. A number of 1000 iterations was selected for both AdaBoost and the internal RF.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20277</offset><text>The main steps for the whole process, exemplified for the PvsH comparison, are displayed in Fig. 1. The same process was also carried out for the RvsH comparison.</text></passage><passage><infon key="file">gr1_lrg.jpg</infon><infon key="id">fig1</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>20443</offset><text>Flowchart describing the complete pipeline of the Machine Learning approach based on the Adaboost classifier (exemplified for the PvsH comparison).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>20591</offset><text>Convolutional Neural Networks (CNN) approach</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>20636</offset><text>Convolutional Neural Networks (CNN) and DL have gradually become gold standards for voice analysis . Most problems regarding voice analysis for health-related issues involve CNN at some point, and COVID-19 studies are no exception . As a matter of fact, most of the studies described in the Introduction achieved the best results either with CNN or RF classifiers. However, as suggested by Cummins et al. , DL is still unable to outclass the efficacy of traditional ML in voice analysis for multiple limitations, such as the complexity of acoustic features and the scarcity of datasets.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>21227</offset><text>Mel-spectrograms as image inputs</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>21260</offset><text>CNN are mostly employed on images in reason of their filtering nature. Indeed, they effectively identify local graphical features, to the point that the logical process behind CNN may somehow recall human sight . Therefore, even for audio applications, graphic plots are preferred as inputs. Mel-spectrograms of all audio recording were therefore produced and exported as grayscale .png images. Subsequently, 4096 points FFT mel-spectrograms were generated with a 50% overlapped Hamming window. Since invaluable information for vocal tasks is found in frequency bands extending up to a few kHz , the frequency range for the spectrograms was limited to 20–12000 Hz, also limiting the complexity of the problem. These steps, as well as the whole CNN procedure, was implemented in MATLAB.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>22051</offset><text>Data augmentation</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22069</offset><text>Since one of the main drawbacks of DL is the requirement for a large pool of data , augmentation procedures have become common practice to improve the generalization and the accuracy of models . Successful results in augmenting biometric data for the detection of COVID-19 were obtained by Barshooi and Amirkani  with a novel approach based on synthetic GAN-generated data, pre-processed by a Gabor filter. However, the images fed to our CNN are in fact time plots, and most graphical artifacts and/or synthesis methods would result in unrealistic augmented data which would bring in the risk of biasing the net.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22685</offset><text>All the employed augmentations were either a modelling of a real-world audio effect, or appropriate mathematical/graphical artifacts on the spectrograms. Specifically, the selected data augmentation methods were:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22898</offset><text>Pink noise addition;</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22919</offset><text>Frequency masking on the spectrogram;</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22957</offset><text>Time masking on the spectrogram;</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>22990</offset><text>Frequency and time masking used together.</text></passage><passage><infon key="file">gr2_lrg.jpg</infon><infon key="id">fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>23032</offset><text>Visualization of the data augmentation techniques applied to mel-frequency spectrograms.</text></passage><passage><infon key="file">gr2_lrg.jpg</infon><infon key="id">fig2</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>23121</offset><text>Top left: original sample spectrogram, top right: pink noise addition, bottom left: time masking, bottom right: frequency masking. .</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>23254</offset><text>Pink noise was preferred over white noise for multiple reasons, such as the invariance of its energy content with respect to pitch and its resemblance to real-world noise able to affect the data . Frequency and time masking are artifacts proven effective by Park in the SpecAugment study : they involve the “masking” of a random range of frequency or time on the spectrogram, which are represented by a dark horizontal and vertical band, respectively. All data augmentation methods, whose results on the spectrogram images can be seen in Fig. 2, were implemented using random values for both the starting point and the width of the masked bands, for which the values of the spectrogram were brought very close to zero (10−15). A SNR of 35 dB was chosen for pink noise addition, whose signal was randomly generated and then added on MATLAB. Audio files were normalized again after the addition of noise to avoid clipping.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24187</offset><text>Data augmentation was carried out only on the training set, and it resulted in new training sets five times larger than the original ones. Specifically, for the PvsH comparison, data augmentation produced 300 instances for 1E and 2S, and 900 instances for 3C. For the RvsH comparison, the process led to 510 instances for 1E and 2S, and 1530 instances for 3C.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_3</infon><offset>24547</offset><text>Proposed architectures</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>24570</offset><text>Although transfer learning proved effective in voice analysis , custom-built and simpler architectures were preferred for the present research project. This choice was mainly dictated by our knowledge of the complexity of audio classification tasks, as well as by the chance of a faster and more controllable framework for future implementations of the tool. However, we did also experiment on transfer-training several popular CNNs (namely AlexNet and ResNet50), finding no improvements over our custom architectures.</text></passage><passage><infon key="file">gr3_lrg.jpg</infon><infon key="id">fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>25090</offset><text>CNN1 architecture.</text></passage><passage><infon key="file">gr3_lrg.jpg</infon><infon key="id">fig3</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>25109</offset><text>A “Conv Block” is described in the higher box and is comprised of a convolutional layer followed by a batch normalization layer and a ReLu (Rectified Linear Unit) activation function. The number after “Conv Block” indicates the number of parallel convolutional filters/neurons in the layer. Max Pool: max pooling layer; FC: Fully connected layer: the number in the round brackets indicates the number of neurons.</text></passage><passage><infon key="file">gr4_lrg.jpg</infon><infon key="id">fig4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>25530</offset><text>CNN2 architecture (for the sole 3C – Cough vocal task).</text></passage><passage><infon key="file">gr4_lrg.jpg</infon><infon key="id">fig4</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>25588</offset><text>A “Conv Block” is described in the higher box and is comprised of a convolutional layer followed by a batch normalization layer and a ReLu (Rectified Linear Unit) activation function. The number after “Conv Block” indicates the number of parallel convolutional filters/neurons in the layer. Max Pool: max pooling layer; FC: Fully connected layer: the number in the round brackets indicates the number of neurons. .</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26011</offset><text>Two different CNN architectures were built: one (CNN1) was employed for the two “vocalized” tasks (1E and 2S), the other (CNN2) for the cough task (3C). These architectures were used in both comparisons (PvsH and RvsH). Square, grayscale images (257 × 257 pixels) were used as inputs. As depicted in Fig. 3, CNN1 encompasses a total of 6 convolutional layers with a growing number of 3 × 3 filters, from 16 to 128. Each convolutional layer is followed by a batch normalization layer and a ReLu activation function. This ensemble is represented as “Conv Block” in the figure. Max Pooling layers are 2 × 2 in size with a Stride of 2. An additional fully connected (FC) layer containing 128 neurons, followed by batch Normalization and ReLu, precedes the last FC layer before the classification output.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>26831</offset><text>CNN2, displayed in Fig. 4, is a simplification of CNN1 used for the task 3C, containing three convolutional layers with 16, 64 and 128 filters respectively, before the two fully connected layers.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>27030</offset><text>For the training of both nets the ADAM Optimizer was used , with a gradient decay factor of 0.8, employing L2-Regularization  and a piecewise learning rate decaying with a factor of 0.8 every 10 epochs.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>27239</offset><text>Results</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>27247</offset><text>Accuracies</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27258</offset><text>For both classification approaches, a total of three sub-classifiers per comparison (PvsH and RvsH) was built, that is, one for each different vocal task. The final predictions on the validation set were unified by means of a majority voting method, considering each of the three sub-classifiers having the same weight. Thus, two or three errors on an instance lead to misclassification in the final result (each instance in the validation set corresponds to a different person).</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>27738</offset><text>For the PvsH comparison, instances 1 to 10 are of healthy subjects, while instances 11 to 20 correspond to COVID-19 positive ones. For the RvsH comparison, healthy subjects go from 1 to 18, while the remaining two are recovered ones.</text></passage><passage><infon key="file">tbl2.xml</infon><infon key="id">tbl2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>27972</offset><text>Confusion matrices for the PvsH comparison over the two classification approaches (Adaboost and CNN).</text></passage><passage><infon key="file">tbl2.xml</infon><infon key="id">tbl2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;#Inst&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Real class&lt;/th&gt;&lt;th colspan=&quot;4&quot; align=&quot;left&quot;&gt;Adaboost&lt;hr/&gt;&lt;/th&gt;&lt;th colspan=&quot;4&quot; align=&quot;left&quot;&gt;CNN&lt;hr/&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;1E&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;2S&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;3C&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Final&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;1E&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;2S&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;3C&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Final&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;4&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;9&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;10&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;11&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;12&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;13&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;14&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;15&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;16&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;17&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;18&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;19&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;20&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;10&quot;&gt;&lt;hr/&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot; align=&quot;left&quot;&gt;&lt;bold&gt;Accuracy (%)&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;95&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;80&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;75&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;100&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;90&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;85&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;85&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;95&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>28074</offset><text>#Inst	Real class	Adaboost	CNN	 			1E	2S	3C	Final	1E	2S	3C	Final	 	1	H	–	–	X	ok	–	–	–	ok	 	2	H	–	–	X	ok	–	–	–	ok	 	3	H	–	–	–	ok	–	–	–	ok	 	4	H	–	–	–	ok	–	–	–	ok	 	5	H	X	–	–	ok	X	X	–	X	 	6	H	–	–	–	ok	–	–	–	ok	 	7	H	–	–	–	ok	–	–	X	ok	 	8	H	–	–	–	ok	–	X	–	ok	 	9	H	–	–	–	ok	–	–	–	ok	 	10	H	–	X	–	ok	–	–	–	ok	 	11	P	–	–	–	ok	–	–	X	ok	 	12	P	–	X	–	ok	–	X	–	ok	 	13	P	–	–	X	ok	–	–	–	ok	 	14	P	–	–	–	ok	–	–	–	ok	 	15	P	–	–	X	ok	X	–	–	ok	 	16	P	–	X	–	ok	–	–	–	ok	 	17	P	–	–	X	ok	–	–	–	ok	 	18	P	–	X	–	ok	–	–	–	ok	 	19	P	–	–	–	ok	–	–	–	ok	 	20	P	–	–	–	ok	–	–	X	ok	 		 	Accuracy (%)	95	80	75	100	90	85	85	95	 	</text></passage><passage><infon key="file">tbl2.xml</infon><infon key="id">tbl2</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>28878</offset><text>Abbreviations: #Inst: Number of test instance; H: Healthy group; P: Positive group; 1E: Sustained vowel /e/ vocal task sub-classifier; 2S: Sentence vocal task sub-classifier; 3C: Cough vocal task sub-classifier; CNN: Convolutional Neural Network approach; -: No error in sub-classifier; X: Classification error; Final: Final classification output obtained by means of majority voting of the three (1E, 2S, 3C) sub-classifiers; ok: No final classification error.</text></passage><passage><infon key="file">tbl3.xml</infon><infon key="id">tbl3</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>29340</offset><text>Confusion matrices for the RvsH comparison over the two classification approaches (Adaboost and CNN).</text></passage><passage><infon key="file">tbl3.xml</infon><infon key="id">tbl3</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;#Inst&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Real class&lt;/th&gt;&lt;th colspan=&quot;4&quot; align=&quot;left&quot;&gt;Adaboost&lt;hr/&gt;&lt;/th&gt;&lt;th colspan=&quot;4&quot; align=&quot;left&quot;&gt;CNN&lt;hr/&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;1E&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;2S&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;3C&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Final&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;1E&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;2S&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;3C&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Final&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;1&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;3&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;4&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;5&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;6&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;8&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;9&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;10&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;11&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;12&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;13&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;14&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;15&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;16&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;17&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;18&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;19&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;20&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;21&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;22&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;23&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;24&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;25&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;26&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;27&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;28&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;29&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;30&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;31&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;32&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;33&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;34&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;35&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;X&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;36&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;–&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;ok&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;10&quot;&gt;&lt;hr/&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot; align=&quot;left&quot;&gt;&lt;bold&gt;Accuracy (%)&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;66.7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;88.9&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;72.2&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;86.1&lt;/bold&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;63.9&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;91.7&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;69.4&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;&lt;bold&gt;75.0&lt;/bold&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>29442</offset><text>#Inst	Real class	Adaboost	CNN	 			1E	2S	3C	Final	1E	2S	3C	Final	 	1	H	X	–	X	X	X	–	X	X	 	2	H	X	–	–	ok	X	–	–	ok	 	3	H	X	–	–	ok	–	–	X	ok	 	4	H	X	–	X	X	X	–	X	X	 	5	H	X	–	–	ok	X	–	X	X	 	6	H	X	–	–	ok	X	–	–	ok	 	7	H	–	–	–	ok	–	X	X	X	 	8	H	–	–	X	ok	X	–	X	X	 	9	H	–	–	–	ok	X	–	X	X	 	10	H	–	–	–	ok	–	–	–	ok	 	11	H	–	–	–	ok	X	–	–	ok	 	12	H	–	–	–	ok	–	–	–	ok	 	13	H	–	X	X	X	X	–	X	X	 	14	H	–	–	X	ok	–	–	X	ok	 	15	H	X	X	–	X	X	X	X	X	 	16	H	X	X	–	X	X	–	–	ok	 	17	H	X	–	–	ok	–	–	–	ok	 	18	H	X	–	–	ok	X	X	–	X	 	19	R	–	–	X	ok	–	–	–	ok	 	20	R	–	–	–	ok	–	–	–	ok	 	21	R	–	–	–	ok	–	–	X	ok	 	22	R	X	–	–	ok	–	–	–	ok	 	23	R	–	–	–	ok	–	–	–	ok	 	24	R	X	–	–	ok	–	–	–	ok	 	25	R	–	–	–	ok	–	–	–	ok	 	26	R	–	X	–	ok	–	–	–	ok	 	27	R	–	–	–	ok	–	–	–	ok	 	28	R	–	–	X	ok	–	–	–	ok	 	29	R	–	–	–	ok	–	–	–	ok	 	30	R	–	–	X	ok	–	–	–	ok	 	31	R	–	–	–	ok	–	–	–	ok	 	32	R	–	–	–	ok	–	–	–	ok	 	33	R	–	–	–	ok	X	–	–	ok	 	34	R	–	–	X	ok	–	–	–	ok	 	35	R	–	–	X	ok	–	–	–	ok	 	36	R	–	–	–	ok	–	–	–	ok	 		 	Accuracy (%)	66.7	88.9	72.2	86.1	63.9	91.7	69.4	75.0	 	</text></passage><passage><infon key="file">tbl3.xml</infon><infon key="id">tbl3</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>30770</offset><text>Abbreviations: #Inst: Number of test instance; H: Healthy group; R: Recovered group; 1E: Sustained vowel /e/ vocal task sub-classifier; 2S: Sentence vocal task sub-classifier; 3C: Cough vocal task sub-classifier; CNN: Convolutional Neural Network approach; -: No error in sub-classifier; X: Classification error; Final: Final classification output obtained by means of majority voting of the three (1E, 2S, 3C) sub-classifiers; ok: No final classification error.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31233</offset><text>Table 2 and Table 3 show the results of Adaboost and CNN respectively, with confusion matrices highlighting the errors in the single sub-classifiers and presenting the final output obtained by majority voting.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>31447</offset><text>In order to better interpret the results, the concepts of sensitivity and specificity must be introduced, as they represent useful measures for binary classifications, especially in the biomedical field. Sensitivity – or true positive rate – is the ratio of positive subjects correctly identified as such (TP) versus all the positives (), following the formula . The specificity – or true negative rate – refers to the correctly identified negative subjects (TN) versus all the negatives (), according to . In the PvsH comparison, positive subjects are indeed the individuals with an ongoing COVID-19 infection; whereas in the RvsH comparison, the R class is considered as “positive” for the scope of calculating sensitivity and specificity. For the purposes of the present study, a high sensitivity was considered to be a priority objective, especially for the PvsH comparison.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>32342</offset><text>As far as the PvsH comparison is concerned, an accuracy of 100% was obtained with the Adaboost approach; the CNN approach reached a 95% accuracy, which translates to one misclassification. It is also worth noting that the only misclassified subject is a healthy one, which means that the CNN-based approach also has a sensitivity of 100%. The RvsH comparison yielded less accurate results, with the AdaBoost-based approach reaching 86.1% and the CNN reaching 75%. However, both classifiers interestingly yielded 100% sensitivity in identifying COVID-19 recovered patients.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>32915</offset><text>Acoustic features for the adaboost approach</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>32959</offset><text>The top ranked features employed for the AdaBoost-based approach can be considered as clinical “biomarkers” for COVID-19 identification through voice analysis.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>33123</offset><text>Most works based on similar vocal tasks are based on different acoustic features, including MFCC, HNR, jitter, shimmer and fundamental frequency (F0) . According to our results, relevant features for determining the positivity to COVID-19 include the RASTA-PLP Coefficients, which ranked higher than MFCC. The RASTA-PLP processing can be considered somehow similar to MFCC , and it’s especially aimed for speech signals due to its insensitiveness to slowly varying background noises . In fact, this processing is especially sensitive to background voices, which we carefully and intentionally avoided. RASTA is widely used in speech recognition and, to the best of our knowledge, it has yet to be solidly introduced in studies regarding voice analysis for healthcare. This is also in line with other studies carried out by our study group on different diseases, especially regarding the implications of higher-numbered RASTA windows and “rough” voices .</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>34087</offset><text>Voicing Probability-related features appear to be the most relevant for the vowel task, while different frequency domain features are present in all the sets.</text></passage><passage><infon key="file">gr5_lrg.jpg</infon><infon key="id">fig5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34246</offset><text>ROC curves.</text></passage><passage><infon key="file">gr5_lrg.jpg</infon><infon key="id">fig5</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34258</offset><text>Above: ROC curve for the PvsH (Positive versus Healthy) comparison. Below: ROC curve for the RvsH (Recovered versus Healthy) comparison. Red line refers to the 1E – vowel/e/ vocal task sub-classifier; blue line refers to the 2S – sentence vocal task sub-classifier; green line refers to the 3C – cough vocal task sub-classifier. Axes span from 0 to 1. AUC (Area Under the Curve) values are reported in the manuscript.</text></passage><passage><infon key="file">gr6_lrg.jpg</infon><infon key="id">fig6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34682</offset><text>Radar plot for the PvsH-3C sub-classifier.</text></passage><passage><infon key="file">gr6_lrg.jpg</infon><infon key="id">fig6</infon><infon key="section_type">FIG</infon><infon key="type">fig_caption</infon><offset>34725</offset><text>PvsH: Positive versus Healthy; 3C: Cough vocal task. Radar plot was built on the top 20 features (as ranked by the linear wrapped SVM ranker), averaged over all the subjects, and normalized by the H class. Blue unit circle (coloured area) represents the H class, red curve represents the P class. .</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>35024</offset><text>Receiver-operating curves (ROC) related to PvsH and RvsH for the AdaBoost classifiers are presented in Fig. 5. The area under the curve (AUC) values for the PvsH are 0.94 for task 1E (red), 0.90 for task 2S (blue) and 0.88 for task 3C (green), respectively. For the RvsH comparison they are 0.74 for 1E, 0.98 for 2S and 0.85 for 3C, respectively.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>35374</offset><text>Fig. 6 shows a radar plot for an interesting, exemplified view of the differences among the acoustic features. The plot shows the average, over all the subjects, of the top 20 features. The features are normalized by the average of the H-group class, which is consequently represented by a unit circle. between</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_1</infon><offset>35686</offset><text>Discussion</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>35697</offset><text>The human ear does not possess sufficient sensitivity to distinguish different pathological conditions just by listening to patients’ voices, even though well-trained and experienced clinicians may sometimes obtain precious hints for diagnosis from perceptual (by-ear) voice analysis, in particular for pathologies with very peculiar features of voice and speech . On the other hand, several studies demonstrate the possibility to identify vocal, respiratory and even neurological diseases from the automatic analysis of the speech signal.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>36240</offset><text>At the present time, a fair amount of studies have tried to identify COVID-19 from the human voice. However, common issues in voice analysis are exacerbated by the difficult pandemic situation, which makes it very hard to gather a reasonable amount of high-quality data, as well as the short timespans, and the limited knowledge on the disease.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>36585</offset><text>Our aim was to build a reliable framework based on a wide amount of information contained in high-quality data, with the best and most reproducible recording conditionsthat, working with subjects with proven clinical status. Since state-of-the-art methodologies contemplate both ML and CNN-based solutions, we chose to employ and to compare both with custom fine-tuned architectures and knowledge-based pre-processing on the input data. As of today, no reliable datasets exist for COVID-19 speech, which made the construction of an independent validation set one of the only possible choices (see Table 4).  </text></passage><passage><infon key="file">tbl4.xml</infon><infon key="id">tbl4</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>37199</offset><text>Most relevant feature domains as retained after the wrapper-based ranking step in the Adaboost-based ML pipeline.</text></passage><passage><infon key="file">tbl4.xml</infon><infon key="id">tbl4</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;/&gt;&lt;th align=&quot;left&quot;&gt;1E&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;2S&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;3C&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td rowspan=&quot;2&quot; align=&quot;left&quot;&gt;PvsH comparison&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Voicing Probability&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RASTA-PLP&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RASTA-PLP&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;RASTA-PLP&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Spectral Loudness&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Spectral Variation&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;MFCC&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;MFCC&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Spectral Loudness&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;4&quot;&gt;&lt;hr/&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td rowspan=&quot;2&quot; align=&quot;left&quot;&gt;RvsH comparison&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Spectral Variation&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;MFCC&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RASTA-PLP&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Energy&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RASTA-PLP&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;MFCC&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;/&gt;&lt;td align=&quot;left&quot;&gt;RASTA-PLP&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Energy&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Spectral Variation&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>37313</offset><text>	1E	2S	3C	 	PvsH comparison	Voicing Probability	RASTA-PLP	RASTA-PLP	 	RASTA-PLP	Spectral Loudness	Spectral Variation	 		MFCC	MFCC	Spectral Loudness	 		 	RvsH comparison	Spectral Variation	MFCC	RASTA-PLP	 	Energy	RASTA-PLP	MFCC	 		RASTA-PLP	Energy	Spectral Variation	 	</text></passage><passage><infon key="file">tbl4.xml</infon><infon key="id">tbl4</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>37582</offset><text>Abbreviations: PvsH: Positive versus Healthy; RvsH: Recovered versus Healthy; 1E: Sustained vowel /e/ vocal task sub-classifier; 2S: Sentence vocal task sub-classifier; 3C: Cough vocal task sub-classifier; RASTA-PLP: Features related to RASTA (Relative Spectral) Coefficients applied to the PLP domain (Perceptual Linear Predictive); Spectral Variation: Umbrella term for features related to variations in the spectrum, such as: slope, kurtosis, skewness, flux; MFCC: Mel-frequency Cepstral Coefficients.</text></passage><passage><infon key="file">tbl5.xml</infon><infon key="id">tbl5</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>38087</offset><text>Literature review.</text></passage><passage><infon key="file">tbl5.xml</infon><infon key="id">tbl5</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Study&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Input signals&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Recording specifications&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;COVID-19 screening and validation characteristics&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;No. of positive (P) subjects&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Classes considered&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Algorithm(s)&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Validation method&lt;/th&gt;&lt;th align=&quot;left&quot;&gt;Accuracy&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Ours&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Vowel, speech, cough&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Unique device (lossless)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;PCR, serology&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;70 (310 total)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P, H, R&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Adaboost, CNN (custom)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Independent test set&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100% 86% (R vs H)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Laguarta et al. &lt;xref rid=&quot;b20&quot; ref-type=&quot;bibr&quot;&gt;[20]&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Cough&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Crowdsourced&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;None (self-reported)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;2660 (5320 total)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P, H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;CNN (ResNet50)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Independent test set&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;97%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Imran et al. &lt;xref rid=&quot;b21&quot; ref-type=&quot;bibr&quot;&gt;[21]&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Cough&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Unspecified&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Unspecified&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;70 (543 total)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P, H, pertussis, bronchitis&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;SVM&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Cross-validation&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;92%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Pinkas et al. &lt;xref rid=&quot;b22&quot; ref-type=&quot;bibr&quot;&gt;[22]&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Vowel, speech, cough&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Multiple devices (lossy)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;PCR&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;29 (88 total)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P, H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;RNN + SVM&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Independent test set&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;79% (F1 score)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Shimon et al. &lt;xref rid=&quot;b23&quot; ref-type=&quot;bibr&quot;&gt;[23]&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Vowel, cough&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Multiple devices (lossy)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;PCR&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;69 (199 total)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P, H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;SVM, RF&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Independent test set&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;80% (mean)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Suppakitjanusant et al. &lt;xref rid=&quot;b25&quot; ref-type=&quot;bibr&quot;&gt;[25]&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Vowel, speech, cough&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Unique device&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Unspecified&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;None (76 recovered, 116 total)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;R, H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;CNN (transfer learning)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Cross-validation&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;74% (mean, R vs H)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Despotovic et al. &lt;xref rid=&quot;b24&quot; ref-type=&quot;bibr&quot;&gt;[24]&lt;/xref&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Vowel, speech, breath, cough&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Crowdsourced&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;None (self-reported)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;84 (1103 total)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P, H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Adaboost, Multilayer Perceptron, CNN&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Cross-validation&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;88%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Muguli et al. &lt;xref rid=&quot;b26&quot; ref-type=&quot;bibr&quot;&gt;[26]&lt;/xref&gt; – DiCOVA challenge&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Vowel, speech, breath, cough&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Crowdsourced&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;None (self-reported)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;60 (990 total)&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;P, H&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Various&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Various&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;73% (baseline) &lt;break/&gt;87% (best)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>38106</offset><text>Study	Input signals	Recording specifications	COVID-19 screening and validation characteristics	No. of positive (P) subjects	Classes considered	Algorithm(s)	Validation method	Accuracy	 	Ours	Vowel, speech, cough	Unique device (lossless)	PCR, serology	70 (310 total)	P, H, R	Adaboost, CNN (custom)	Independent test set	100% 86% (R vs H)	 	Laguarta et al. 	Cough	Crowdsourced	None (self-reported)	2660 (5320 total)	P, H	CNN (ResNet50)	Independent test set	97%	 	Imran et al. 	Cough	Unspecified	Unspecified	70 (543 total)	P, H, pertussis, bronchitis	SVM	Cross-validation	92%	 	Pinkas et al. 	Vowel, speech, cough	Multiple devices (lossy)	PCR	29 (88 total)	P, H	RNN + SVM	Independent test set	79% (F1 score)	 	Shimon et al. 	Vowel, cough	Multiple devices (lossy)	PCR	69 (199 total)	P, H	SVM, RF	Independent test set	80% (mean)	 	Suppakitjanusant et al. 	Vowel, speech, cough	Unique device	Unspecified	None (76 recovered, 116 total)	R, H	CNN (transfer learning)	Cross-validation	74% (mean, R vs H)	 	Despotovic et al. 	Vowel, speech, breath, cough	Crowdsourced	None (self-reported)	84 (1103 total)	P, H	Adaboost, Multilayer Perceptron, CNN	Cross-validation	88%	 	Muguli et al.  – DiCOVA challenge	Vowel, speech, breath, cough	Crowdsourced	None (self-reported)	60 (990 total)	P, H	Various	Various	73% (baseline) 87% (best)	 	</text></passage><passage><infon key="file">tbl5.xml</infon><infon key="id">tbl5</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>39447</offset><text>Abbreviations: PCR: Polymerase Chain Reaction-based molecular swab; P: COVID-19 Positive subjects; H: Healthy subjects; R: Recovered subjects; CNN: Convolutional Neural Network; SVM: Support Vector Machine; RNN: Recurrent Neural Network.</text></passage><passage><infon key="file">tbl5.xml</infon><infon key="id">tbl5</infon><infon key="section_type">TABLE</infon><infon key="type">table_footnote</infon><offset>39685</offset><text>“Lossless” refers to raw, unprocessed and uncompressed sound data, while “lossy” implies that compression and/or artifacts are present. “Accuracy” refers to the highest reported classification accuracy for the binary Positive VS Healthy classification, except when otherwise specified. Please note that the algorithms used in each study are greatly summarized in the Table. For studies which did not have a single, final, accuracy result, the mean accuracy has been reported, and specified as such.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>40196</offset><text>Table 5 reports the main characteristics of our study compared to most of the other published works. Although we assert the experimental and preliminary nature of these studies, we believe that ours is the first one to achieve such promising results with the use of non-crowdsourced, verified audio data, while also considering recovered subjects, bringing novel architectures and comparing the most technologically advanced methods for speech analysis.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>40651</offset><text>The chosen approaches exemplify the division between traditional ML pipelines allowing for a better control of every step of the inference, and CNNs, which act almost like a “black box” despite producing very deep features. All the architectures we adopted, despite being roughly based on state-of-the-art studies, are custom-made in reason of the difficult task of identifying a specific pathology from multiple sources of human voice signals. For the first approach (ML), bagged decision trees (Random Forests) are embedded within a boosting architecture. While bagging aims to cover many permutations of the training set to produce a generalized “tree of trees”, boosting the resulting models allows to build the best performing and least biased one. Customized feature sets, resulted from a preliminary correlation-based large-scale skimming followed by a more problem-specific wrapped SVM-based ranker, were used for the training of each Adaboost. The features were only selected on the basis of the training set, meaning that the final validation set was never analyzed by the Adaboost models. On the other hand, the proposed CNN architectures were custom-built, without being based on any specific previous study, and with the added aim to be reasonably “light” for ease of use and to avoid future implementation issues, also considering the absence of improvements when using transfer learning. The substantial acoustic differences between vocalized signals and cough led to the construction of a specific, shallower, CNN for the latter, considering the more straightforward and homogeneous characteristics of those specific recordings, which present less variation within the duration of the audio file and cough section. All of the proposed architectures were then re-applied on the independent validation set.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>42484</offset><text>The results of the Adaboost and CNN-based approaches appear similar for the PvsH comparison, with respective accuracies of 100% and 95%. The RvsH comparison showed Adaboost being significantly more accurate, with 86.1% versus the 75% obtained by the CNN. Interestingly, both approaches for PvsH and RvsH yielded 100% of sensitivity.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>42817</offset><text>The accuracy of the PvsH comparison might prove the feasibility of voice-based COVID-detection, which has solid clinical bases already since most pulmonary diseases have been demonstrated to have distinct and detectable effects on the speech. Moreover, the possibility to also discriminate recovered subjects is in line with the fact that COVID-19 may induce long-lasting damages to the phonatory system, as shown by Helding et al.  in a recent study.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>43271</offset><text>By considering the confusion matrices for the single sub-classifiers are concerned, it appeared that even sub-optimal results – like the 66.7% accuracy for the 1E task in the Adaboost-based RvsH comparison – can lead to satisfactory final accuracies when unified with other tasks. This confirms the potential of using more than one vocal indicator. It is also interesting to note how the vowel sound/e/ is the most effective at discriminating positive subjects from healthy ones, whereas it becomes the least promising for the RvsH comparison. This may suggest that the effects of COVID-19 on the voice are subject to change through the course of the disease and recovery. RASTA-PLP processing is assessed as the most recurring domain in the top-ranked features, corroborating and refining the existing approaches mainly based on MFCC, both in the definition of the features to extract for building classifiers and as an alternative to spectrograms for CNN’s. Thus, RASTA-PLP processing could be a very viable solution for voice analysis in healthcare and speech recognition, also due to its noise-robust nature . Thus, a more in-depth study of its potential is one of the aims of our future research, especially towards a solid employment of RASTA-temporal diagrams as inputs of a CNN.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>44565</offset><text>Other features like Jitter or HNR, widely used in voice analysis, leave space to more specific features in the frequency domain, here encompassed under the name of “spectral variation”, and generally referring to considerations on the slope, kurtosis, prominence of the spectral curves.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>44856</offset><text>In order to consider the features as reliable biomarkers, bias in selection and/or classification should be minimized. We tackled this with the gathering of a polished, well-organized dataset, and with a posterior confrontation of the features with those related to non-pathological effects  like age and gender  which, if present, could have represented a risk of bias. Despite the hierarchy of accuracies for each sub-classifier being the same between Adaboost and CNN, different subjects get mis-classified in the two approaches. This confirms that, although they probably rely on similar information for the inference, in the end the dynamics of the algorithms are different.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>45538</offset><text>With all these premises, the construction of a tool for remote screening can be reasonably foreseen. In fact, a project is currently ongoing, involving ML-based real-time voice analysis on-site . In these regards, a first concern could be represented by the choice of a suitable environment for voice recordings. The use of adequately quiet rooms for the present research project can easily be replicated in on-site screening facilities, by keeping the room devoid of noisy machinery, and choosing environments relatively protected from traffic and crowd noises. Moreover, we also consider quiet domestic rooms to be reasonably close to our experimental settings. The heavy usage of noise-robust acoustic features, like RASTA, also guarantee a certain insensitiveness of the Adaboost-based approach to changes in background noises and/or microphones.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>46390</offset><text>To build a reliable tool, the three classes considered in this study should also be unified in a single classifier. This could be done with a multi-class model or, especially for the ML approach, with the ensemble of three one-vs-one classifiers. However, since the identification of recovered subjects could arguably be less crucial in on-site screening situations, the merging of the H and R classes is also possible, leading to a single binary classification of positives versus non-positives. Both approaches are currently being tested, especially the latter, within the abovementioned national project.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>46998</offset><text>It is worth noting that the choice of recording through smartphones is justified by the need for a widespread and easily accessible strategy for remote screening. Moreover, smartphones have already been proved to be reliable tools for ML-based speech analyses .</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>47261</offset><text>The evolution of the disease could itself represent a limitation for the present study, as COVID-19 clinical spectrum is evolving, and pauci-symptomatic positive subjects are becoming more widespread. Besides, long-term effects of the disease on the voice could also be expected to evolve, and a further study of recovered subjects would also be helpful in this matter. Moreover, small datasets constitute a very common problem in bio-engineering ML tasks, and the collection of high-quality data is undoubtedly made difficult by the critical conditions of healthcare institutions. However, we believe that a knowledge-based approach accompanying the data-driven inference can compensate at least some of the typical critical issues of such a task.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">title_2</infon><offset>48010</offset><text>Future developments</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>48030</offset><text>A more thorough a-priori and posterior analysis of the acoustic features would be beneficial in the future, in order to identify not only the most powerful features for this task, but also a reasonably generalized, unbiased and more definitive set. Furthermore, many refined solutions exist in the neural network realm, especially with regards to pre-trained networks and node splitting, which could lead to better performing architectures, possibly addressing the problem with an even more knowledge-based approach without intensifying too much the calculation burden.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>48600</offset><text>In these regards, the collection of a clean and relatively homogeneous dataset, suitable pre-processing techniques, posterior analysis of features and the usage of two independent state-of-the-art algorithms (specifically chosen and tuned for this task) may hopefully dispel some skepticism towards this pioneering screening technology.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>48937</offset><text>Still, it is important to stress that a screening tool can only offer a preliminary result, suggesting a more extensive validation in case of a positive outcome through conventional diagnostics. This explains our preference for a high sensitivity, despite a certain prevalence for negatively tested subjects in the current diagnostic situation.</text></passage><passage><infon key="section_type">DISCUSS</infon><infon key="type">paragraph</infon><offset>49282</offset><text>All the above-mentioned steps, as already stressed, would be greatly supported by the collection of larger datasets, which is one of our main focuses for the immediate future.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>49458</offset><text>Conclusions</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>49470</offset><text>Our work concerns a novel approach in discriminating three groups of subjects with different COVID-19 status (positive, healthy and recovered), analyzing their vocal performances (sustained vowel, sentence, cough) employing ML algorithms. In order to minimize external biases in the classifiers, we focused on the acquisition of a professionally recorded voice dataset rather than crowdsourced data, which could not only guarantee the maximum reliability of the samples, but also a rigorous annotation and medically proven metadata. On the basis of current state-of-the-art technologies, two algorithms were used following specific fine-tuning and customization. Specifically, the first approach involves an AdaBoost algorithm with Random Forest weak learners applied to a selection of acoustic features reduced by a CFS followed by an SVM-wrapper-based ranking. The second approach is based on CNN applied to spectrogram images with a knowledge-based data augmentation.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>50441</offset><text>Two binary comparisons, COVID-19 positive versus healthy subjects (PvsH) and recovered versus healthy (RvsH) were considered, with three sub-classifiers per comparison, one for each speech task. Majority voting was used to determine the final results of the comparisons over the three sub-classifiers. Two custom CNN architectures are proposed, one strictly focused on the analysis of the cough sound.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>50843</offset><text>The accuracies are interestingly high, especially for the PvsH comparison, and the Adaboost approach scores higher in both comparisons. Furthermore, a 100% sensitivity for the identification of positive and recovered subjects is obtained by both approaches.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>51101</offset><text>According to our results, traditional ML algorithms proved to still be powerful tools in voice analysis, possibly leading the way for small datasets, over more complex DL solutions. Moreover, we stressed the importance of a knowledge-based approach in such tasks. Carefully built datasets in quiet environments, with adequate pre-processing and fine-tuning of the algorithms, are crucial for a more effective analysis. We observed that the voice sound may hold a COVID-19 “signature”, even when the infection is not detectable anymore, which leads to believe that vocal tests can represent a meaningful tool for multiple purposes, including mass-screening, identification of COVID-19 positive subjects and the study of mid and short-term effects of this dreadful disease on the voice.</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">title_1</infon><offset>51890</offset><text>Declaration of Competing Interest</text></passage><passage><infon key="section_type">COMP_INT</infon><infon key="type">paragraph</infon><offset>51924</offset><text>The authors declare the following financial interests/personal relationships which may be considered as potential competing interests: Authors Giovanni Costantini, Giovanni Saggio, and Antonio Pisani are advisory members of VoiceWise S.r.l., spin-off company of University of Rome Tor Vergata (Rome, Italy) developing voice analysis solutions for diagnostic purposes; Valerio Cesarini cooperates with VoiceWise and is employed by CloudWise S.r.l., a company developing cloud data storage and software solutions.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title</infon><offset>52436</offset><text>References</text></passage><passage><infon key="fpage">1401</infon><infon key="issue">6</infon><infon key="lpage">1410</infon><infon key="name_0">surname:Suppa;given-names:A.</infon><infon key="name_1">surname:Asci;given-names:F.</infon><infon key="name_2">surname:Saggio;given-names:G.</infon><infon key="name_3">surname:Di Leo;given-names:P.</infon><infon key="name_4">surname:Zarezadeh;given-names:Z.</infon><infon key="name_5">surname:Ferrazzano;given-names:G.</infon><infon key="name_6">surname:Ruoppolo;given-names:G.</infon><infon key="name_7">surname:Berardelli;given-names:A.</infon><infon key="name_8">surname:Costantini;given-names:G.</infon><infon key="pub-id_doi">10.1002/mds.28508</infon><infon key="section_type">REF</infon><infon key="source">Mov. Disorders: Off. J. Mov. Disorder Soc.</infon><infon key="type">ref</infon><infon key="volume">36</infon><infon key="year">2021</infon><offset>52447</offset><text>Voice analysis with machine learning: one step closer to an objective diagnosis of essential tremor</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52547</offset><text>J.P. Teixeira, J. Fernandes, F. Teixeira, P.O. Fernandes, Acoustic analysis of chronic laryngitis-statistical analysis of sustained speech parameters, in: 11th International Joint Conference on Biomedical Engineering Systems and Technologies, 2018, pp. 168–175.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>52811</offset><text>S.C. Costa, B.G.A. Neto, J.M. Fechine, M. Muppa, Short-Term Cepstral Analysis Applied to Vocal Fold Edema Detection, in: BIOSIGNALS (2), 2008, pp. 110–115.</text></passage><passage><infon key="fpage">241</infon><infon key="issue">2</infon><infon key="lpage">246</infon><infon key="name_0">surname:Petrovic-Lazic;given-names:M.</infon><infon key="name_1">surname:Jovanovic;given-names:N.</infon><infon key="name_2">surname:Kulic;given-names:M.</infon><infon key="name_3">surname:Babac;given-names:S.</infon><infon key="name_4">surname:Jurisic;given-names:V.</infon><infon key="pub-id_pmid">25301300</infon><infon key="section_type">REF</infon><infon key="source">J. Voice</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">2015</infon><offset>52969</offset><text>Acoustic and perceptual characteristics of the voice in patients with vocal polyps after surgery and voice therapy</text></passage><passage><infon key="fpage">125</infon><infon key="issue">1</infon><infon key="lpage">e13</infon><infon key="name_0">surname:Alves;given-names:M.</infon><infon key="name_1">surname:Krüger;given-names:E.</infon><infon key="name_2">surname:Pillay;given-names:B.</infon><infon key="name_3">surname:Van Lierde;given-names:K.</infon><infon key="name_4">surname:Van der Linde;given-names:J.</infon><infon key="section_type">REF</infon><infon key="source">J. Voice</infon><infon key="type">ref</infon><infon key="volume">33</infon><infon key="year">2019</infon><offset>53084</offset><text>The effect of hydration on voice quality in adults: A systematic review</text></passage><passage><infon key="fpage">193</infon><infon key="issue">4</infon><infon key="lpage">197</infon><infon key="name_0">surname:Zacharia;given-names:T.</infon><infon key="name_1">surname:Rao;given-names:S.</infon><infon key="name_2">surname:Hegde;given-names:S.K.</infon><infon key="name_3">surname:D’souza;given-names:P.</infon><infon key="name_4">surname:James;given-names:J.</infon><infon key="name_5">surname:Baliga;given-names:M.S.</infon><infon key="section_type">REF</infon><infon key="source">Middle East J. Cancer</infon><infon key="type">ref</infon><infon key="volume">7</infon><infon key="year">2016</infon><offset>53156</offset><text>Evaluation of voice parameters in people with head and neck cancers: an investigational study</text></passage><passage><infon key="fpage">19835</infon><infon key="lpage">19841</infon><infon key="name_0">surname:Alhussein;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">IEEE Access</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2017</infon><offset>53250</offset><text>Monitoring Parkinson’s disease in smart cities</text></passage><passage><infon key="fpage">130</infon><infon key="lpage">138</infon><infon key="name_0">surname:Gómez-Vilda;given-names:P.</infon><infon key="name_1">surname:Londral;given-names:A.R.M.</infon><infon key="name_2">surname:Rodellar-Biarge;given-names:V.</infon><infon key="name_3">surname:Ferrández-Vicente;given-names:J.M.</infon><infon key="name_4">surname:de Carvalho;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Neurocomputing</infon><infon key="type">ref</infon><infon key="volume">151</infon><infon key="year">2015</infon><offset>53299</offset><text>Monitoring amyotrophic lateral sclerosis by biomechanical modeling of speech production</text></passage><passage><infon key="fpage">995</infon><infon key="issue">5</infon><infon key="lpage">1001</infon><infon key="name_0">surname:Albertini;given-names:G.</infon><infon key="name_1">surname:Bonassi;given-names:S.</infon><infon key="name_2">surname:Dall’Armi;given-names:V.</infon><infon key="name_3">surname:Giachetti;given-names:I.</infon><infon key="name_4">surname:Giaquinto;given-names:S.</infon><infon key="name_5">surname:Mignano;given-names:M.</infon><infon key="pub-id_pmid">20488659</infon><infon key="section_type">REF</infon><infon key="source">Res. Dev. Disabil.</infon><infon key="type">ref</infon><infon key="volume">31</infon><infon key="year">2010</infon><offset>53387</offset><text>Spectral analysis of the voice in Down syndrome</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">6</infon><infon key="name_0">surname:Pareek;given-names:V.</infon><infon key="name_1">surname:Sharma;given-names:R.K.</infon><infon key="section_type">REF</infon><infon key="source">2016 IEEE Students’ Conference on Electrical, Electronics and Computer Science</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>53435</offset></passage><passage><infon key="fpage">190</infon><infon key="issue">2</infon><infon key="lpage">194</infon><infon key="name_0">surname:Oh;given-names:J.E.</infon><infon key="name_1">surname:Choi;given-names:Y.M.</infon><infon key="name_2">surname:Kim;given-names:S.J.</infon><infon key="name_3">surname:Joo;given-names:C.U.</infon><infon key="section_type">REF</infon><infon key="source">Korean J. Pediatr.</infon><infon key="type">ref</infon><infon key="volume">53</infon><infon key="year">2010</infon><offset>53436</offset><text>Acoustic variations associated with congenital heart disease</text></passage><passage><infon key="fpage">39</infon><infon key="issue">7</infon><infon key="lpage">43</infon><infon key="name_0">surname:Sakai;given-names:M.</infon><infon key="section_type">REF</infon><infon key="source">Int. J. Comput. Appl.</infon><infon key="type">ref</infon><infon key="volume">109</infon><infon key="year">2015</infon><offset>53497</offset><text>Feasibility study on blood pressure estimations from voice spectrum analysis</text></passage><passage><infon key="fpage">1041</infon><infon key="issue">4</infon><infon key="name_0">surname:Asci;given-names:F.</infon><infon key="name_1">surname:Costantini;given-names:G.</infon><infon key="name_2">surname:Saggio;given-names:G.</infon><infon key="name_3">surname:Suppa;given-names:A.</infon><infon key="section_type">REF</infon><infon key="source">Mov. Disorders</infon><infon key="type">ref</infon><infon key="volume">36</infon><infon key="year">2021</infon><offset>53574</offset><text>Fostering voice objective analysis in patients with movement disorders</text></passage><passage><infon key="fpage">169</infon><infon key="issue">3</infon><infon key="lpage">170</infon><infon key="name_0">surname:Todisco;given-names:M.</infon><infon key="name_1">surname:Alfonsi;given-names:E.</infon><infon key="name_2">surname:Arceri;given-names:S.</infon><infon key="name_3">surname:Bertino;given-names:G.</infon><infon key="name_4">surname:Robotti;given-names:C.</infon><infon key="name_5">surname:Albergati;given-names:M.</infon><infon key="name_6">surname:Gastaldi;given-names:M.</infon><infon key="name_7">surname:Tassorelli;given-names:C.</infon><infon key="name_8">surname:Cosentino;given-names:G.</infon><infon key="pub-id_doi">10.1016/S1474-4422(21)00025-9</infon><infon key="pub-id_pmid">33609467</infon><infon key="section_type">REF</infon><infon key="source">Lancet. Neurol.</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2021</infon><offset>53645</offset><text>Isolated bulbar palsy after SARS-CoV-2 infection</text></passage><passage><infon key="fpage">293</infon><infon key="issue">1</infon><infon key="lpage">303</infon><infon key="name_0">surname:Hu;given-names:G.</infon><infon key="name_1">surname:Peng;given-names:X.</infon><infon key="name_2">surname:Yang;given-names:Y.</infon><infon key="name_3">surname:Hospedales;given-names:T.M.</infon><infon key="name_4">surname:Verbeek;given-names:J.</infon><infon key="pub-id_doi">10.1109/TIP.2017.2756450</infon><infon key="pub-id_pmid">28952941</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Image Process.</infon><infon key="type">ref</infon><infon key="volume">27</infon><infon key="year">2018</infon><offset>53694</offset><text>Frankenstein: learning deep face representations using small data</text></passage><passage><infon key="fpage">135</infon><infon key="lpage">141</infon><infon key="name_0">surname:Sztahó;given-names:D.</infon><infon key="name_1">surname:Gábor;given-names:K.</infon><infon key="name_2">surname:Gábriel;given-names:T.</infon><infon key="pub-id_doi">10.5220/0010193101350141</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the 14th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 2: BIOSIGNALS</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>53760</offset></passage><passage><infon key="fpage">898</infon><infon key="lpage">905</infon><infon key="name_0">surname:Nissar;given-names:I.</infon><infon key="name_1">surname:Mir;given-names:W.A.</infon><infon key="name_2">surname:Izharuddin;given-names:T.</infon><infon key="name_3">surname:Shaikh;given-names:T.A.</infon><infon key="pub-id_doi">10.1109/ICACCS51430.2021.9441885</infon><infon key="section_type">REF</infon><infon key="source">2021 IEEE, 7th International Conference on Advanced Computing and Communication Systems</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>53761</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>53762</offset><text>A. Benba, A. Jilbab, A. Hammouch, S. Sandabad, Voiceprints analysis using MFCC and SVM for detecting patients with Parkinson’s disease, in: 2015 International Conference on Electrical and Information Technologies, ICEIT, 2015, pp. 300–304.</text></passage><passage><infon key="fpage">23</infon><infon key="lpage">30</infon><infon key="name_0">surname:Suppa;given-names:A.</infon><infon key="name_1">surname:Asci;given-names:F.</infon><infon key="name_2">surname:Saggio;given-names:G.</infon><infon key="name_3">surname:Marsili;given-names:L.</infon><infon key="name_4">surname:Casali;given-names:D.</infon><infon key="name_5">surname:Zarezadeh;given-names:Z.</infon><infon key="name_6">surname:Ruoppolo;given-names:G.</infon><infon key="name_7">surname:Berardelli;given-names:A.</infon><infon key="name_8">surname:Costantini;given-names:G.</infon><infon key="pub-id_doi">10.1016/j.parkreldis.2020.03.012</infon><infon key="section_type">REF</infon><infon key="source">Parkinsonism Rel. Disord.</infon><infon key="type">ref</infon><infon key="volume">73</infon><infon key="year">2020</infon><offset>54006</offset><text>Voice analysis in adductor spasmodic dysphonia: Objective diagnosis and response to botulinum toxin</text></passage><passage><infon key="name_0">surname:Laguarta;given-names:J.</infon><infon key="name_1">surname:Hueto;given-names:F.</infon><infon key="name_2">surname:Subirana;given-names:B.</infon><infon key="section_type">REF</infon><infon key="source">IEEE Open J. Eng. Med. Biol.</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>54106</offset><text>COVID-19 artificial intelligence diagnosis using only cough recordings</text></passage><passage><infon key="comment">arXiv preprint arXiv:2004.01275</infon><infon key="name_0">surname:Imran;given-names:A.</infon><infon key="name_1">surname:Posokhova;given-names:I.</infon><infon key="name_2">surname:Qureshi;given-names:H.N.</infon><infon key="name_3">surname:Masood;given-names:U.</infon><infon key="name_4">surname:Riaz;given-names:S.</infon><infon key="name_5">surname:Ali;given-names:K.</infon><infon key="name_6">surname:Nabeel;given-names:M.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>54177</offset></passage><passage><infon key="comment">2020</infon><infon key="fpage">268</infon><infon key="lpage">274</infon><infon key="name_0">surname:Pinkas;given-names:G.</infon><infon key="name_1">surname:Karny;given-names:Y.</infon><infon key="name_2">surname:Malachi;given-names:A.</infon><infon key="name_3">surname:Barkai;given-names:G.</infon><infon key="name_4">surname:Bachar;given-names:G.</infon><infon key="name_5">surname:Aharonson;given-names:V.</infon><infon key="section_type">REF</infon><infon key="source">IEEE Open J. Eng. Med.</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2020</infon><offset>54178</offset><text>SARS-CoV-2 detection from voice</text></passage><passage><infon key="fpage">1120</infon><infon key="issue">2</infon><infon key="name_0">surname:Shimon;given-names:C.</infon><infon key="name_1">surname:Shafat;given-names:G.</infon><infon key="name_2">surname:Dangoor;given-names:I.</infon><infon key="name_3">surname:Ben-Shitrit;given-names:A.</infon><infon key="pub-id_pmid">33639822</infon><infon key="section_type">REF</infon><infon key="source">J. Acoust. Soc. Am.</infon><infon key="type">ref</infon><infon key="volume">149</infon><infon key="year">2021</infon><offset>54210</offset><text>Artificial intelligence enabled preliminary diagnosis for COVID-19 from voice cues and questionnaires</text></passage><passage><infon key="name_0">surname:Despotovic;given-names:V.</infon><infon key="name_1">surname:Ismael;given-names:M.</infon><infon key="name_2">surname:Cornil;given-names:M.</infon><infon key="name_3">surname:Call;given-names:R.M.</infon><infon key="name_4">surname:Fagherazzi;given-names:G.</infon><infon key="section_type">REF</infon><infon key="source">Comput. Biol. Med.</infon><infon key="type">ref</infon><infon key="volume">138</infon><infon key="year">2021</infon><offset>54312</offset><text>Detection of COVID-19 from voice, cough and breathing patterns: Dataset and preliminary results</text></passage><passage><infon key="fpage">19149</infon><infon key="name_0">surname:Suppakitjanusant;given-names:P.</infon><infon key="name_1">surname:Sungkanuparph;given-names:S.</infon><infon key="name_2">surname:Wongsinin;given-names:T.</infon><infon key="pub-id_doi">10.1038/s41598-021-98742-x</infon><infon key="pub-id_pmid">34580407</infon><infon key="section_type">REF</infon><infon key="source">Sci. Rep.</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">2021</infon><offset>54408</offset><text>Identifying individuals with recent COVID-19 through voice classification using deep learning</text></passage><passage><infon key="comment">Pre-print within the Interspeech 2021 challenge</infon><infon key="element-citation">arXiv:2103.09148</infon><infon key="name_0">surname:Muguli;given-names:A.</infon><infon key="name_1">surname:Pinto;given-names:L.</infon><infon key="name_10">surname:Ramoji;given-names:S.</infon><infon key="name_11">surname:Viral;given-names:N.</infon><infon key="name_2">surname:Nirmala;given-names:R.</infon><infon key="name_3">surname:Sharma;given-names:N.</infon><infon key="name_4">surname:Krishnan;given-names:P.</infon><infon key="name_5">surname:Ghosh;given-names:P.K.</infon><infon key="name_6">surname:Kumar;given-names:R.</infon><infon key="name_7">surname:Bhat;given-names:S.</infon><infon key="name_8">surname:Chetupalli;given-names:S.R.</infon><infon key="name_9">surname:Ganapathy;given-names:S.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>54502</offset></passage><passage><infon key="comment">in press</infon><infon key="name_0">surname:Robotti;given-names:C.</infon><infon key="name_1">surname:Costantini;given-names:G.</infon><infon key="name_10">surname:Cassaniti;given-names:I.</infon><infon key="name_11">surname:Baldanti;given-names:F.</infon><infon key="name_12">surname:Gravina;given-names:A.</infon><infon key="name_13">surname:Sakib;given-names:A.</infon><infon key="name_14">surname:Alessi;given-names:E.</infon><infon key="name_15">surname:Pascucci;given-names:M.</infon><infon key="name_16">surname:Casali;given-names:D.</infon><infon key="name_17">surname:Zarezadeh;given-names:Z.</infon><infon key="name_18">surname:Del Zoppo;given-names:V.</infon><infon key="name_19">surname:Pisani;given-names:A.</infon><infon key="name_2">surname:Saggio;given-names:G.</infon><infon key="name_20">surname:Benazzo;given-names:M.</infon><infon key="name_3">surname:Cesarini;given-names:V.</infon><infon key="name_4">surname:Calastri;given-names:A.</infon><infon key="name_5">surname:Maiorano;given-names:E.</infon><infon key="name_6">surname:Piloni;given-names:D.</infon><infon key="name_7">surname:Perrone;given-names:T.</infon><infon key="name_8">surname:Sabatini;given-names:U.</infon><infon key="name_9">surname:Ferretti;given-names:V.V.</infon><infon key="pub-id_doi">10.1016/j.jvoice.2021.11.004</infon><infon key="section_type">REF</infon><infon key="source">J. Voice</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>54503</offset><text>Machine learning-based voice assesment for the detection of positive and recovered COVID-19 patients</text></passage><passage><infon key="name_0">surname:G.;given-names:Fant</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">1960</infon><offset>54604</offset></passage><passage><infon key="name_0">surname:MATLAB;given-names:Fant</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>54605</offset></passage><passage><infon key="comment">https://www.statisticshowto.com/lowess-smoothing/</infon><infon key="name_0">surname:Glen;given-names:S.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2013</infon><offset>54606</offset></passage><passage><infon key="name_0">surname:Eibe;given-names:F.</infon><infon key="name_1">surname:Hall;given-names:M.</infon><infon key="name_2">surname:Witten;given-names:I.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>54607</offset></passage><passage><infon key="comment">25.-29.10.2010</infon><infon key="fpage">1459</infon><infon key="lpage">1462</infon><infon key="name_0">surname:Eyben;given-names:F.</infon><infon key="name_1">surname:Wöllmer;given-names:M.</infon><infon key="name_2">surname:Björn Schuller;given-names:M</infon><infon key="section_type">REF</infon><infon key="source">Proc. ACM Multimedia (MM)</infon><infon key="type">ref</infon><infon key="year">2010</infon><offset>54608</offset></passage><passage><infon key="name_0">surname:Schuller;given-names:B.</infon><infon key="name_1">surname:Steidl;given-names:S.</infon><infon key="name_2">surname:Batliner;given-names:A.</infon><infon key="name_3">surname:Hirschberg;given-names:J.</infon><infon key="name_4">surname:Burgoon;given-names:J.</infon><infon key="name_5">surname:Baird;given-names:A.</infon><infon key="name_6">surname:Elkins;given-names:A.</infon><infon key="name_7">surname:Zhang;given-names:Y.</infon><infon key="name_8">surname:Coutinho;given-names:E.</infon><infon key="name_9">surname:Evanini;given-names:K.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>54609</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54610</offset><text>B.P. Bogert, M.J.R. Healy, J.W. Tukey, The Quefrency Alanysis [sic] of Time Series for Echoes: Cepstrum, Pseudo Autocovariance, Cross-Cepstrum and Saphe Cracking, in: M. Rosenblatt (Ed.), Proceedings of the Symposium on Time Series Analysis, 1963.</text></passage><passage><infon key="fpage">578</infon><infon key="lpage">589</infon><infon key="name_0">surname:Hermansky;given-names:H.</infon><infon key="name_1">surname:Morgan;given-names:N.</infon><infon key="pub-id_doi">10.1109/89.32661</infon><infon key="section_type">REF</infon><infon key="source">IEEE Trans. Speech Audio Process.</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">1994</infon><offset>54858</offset><text>RASTA processing of speech</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>54885</offset><text>S. Yeldener, Method of determining the voicing probability of speech signals - united states patent USOO637792OB2, Patent No.: US 6, 377, 920 B2, Apr. 23, 2002.</text></passage><passage><infon key="fpage">2919</infon><infon key="issue">5 Pt 1</infon><infon key="lpage">2928</infon><infon key="name_0">surname:Anweiler;given-names:A.K.</infon><infon key="name_1">surname:Verhey;given-names:J.L.</infon><infon key="pub-id_doi">10.1121/1.2184224</infon><infon key="pub-id_pmid">16708949</infon><infon key="section_type">REF</infon><infon key="source">J. Acoust. Soc. Am.</infon><infon key="type">ref</infon><infon key="volume">119</infon><infon key="year">2006</infon><offset>55046</offset><text>Spectral loudness summation for short and long signals as a function of level</text></passage><passage><infon key="name_0">surname:Köppen;given-names:M.</infon><infon key="pub-id_doi">10.1007/978-0-387-39940-9_133</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>55124</offset></passage><passage><infon key="fpage">27</infon><infon key="lpage">36</infon><infon key="name_0">surname:Salimi;given-names:A.</infon><infon key="name_1">surname:Ziaii;given-names:M.</infon><infon key="name_2">surname:Amiri;given-names:A.</infon><infon key="name_3">surname:Zadeh;given-names:M.H.</infon><infon key="name_4">surname:Karimpouli;given-names:S.</infon><infon key="name_5">surname:Moradkhani;given-names:M.</infon><infon key="pub-id_doi">10.1016/j.ejrs.2017.02.003</infon><infon key="section_type">REF</infon><infon key="source">Egypt. J. Remote Sens. Space Sci.</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">2018</infon><offset>55125</offset><text>Using a feature subset selection method and support vector machine to address curse of dimensionality and redundancy in Hyperion hyperspectral data classification</text></passage><passage><infon key="name_0">surname:Hall;given-names:M.A.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">1999</infon><offset>55288</offset></passage><passage><infon key="name_0">surname:Cormen;given-names:T.H.</infon><infon key="name_1">surname:Leiserson;given-names:C.E.</infon><infon key="name_2">surname:Rivest;given-names:R.L.</infon><infon key="name_3">surname:Stein;given-names:C.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2009</infon><offset>55289</offset></passage><passage><infon key="fpage">273</infon><infon key="issue">3</infon><infon key="lpage">297</infon><infon key="name_0">surname:Cortes;given-names:C.</infon><infon key="name_1">surname:Vapnik;given-names:V.N.</infon><infon key="section_type">REF</infon><infon key="source">Mach. Learn.</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">1995</infon><offset>55290</offset><text>Support-vector networks</text></passage><passage><infon key="name_0">surname:Platt;given-names:J.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">1998</infon><offset>55314</offset></passage><passage><infon key="fpage">662</infon><infon key="lpage">666</infon><infon key="name_0">surname:Hammami;given-names:I.</infon><infon key="name_1">surname:Salhi;given-names:L.</infon><infon key="name_2">surname:Labidi;given-names:S.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2016</infon><offset>55315</offset></passage><passage><infon key="name_0">surname:Godino-Llorente;given-names:J.I.</infon><infon key="name_1">surname:Gómez-Vilda;given-names:P.</infon><infon key="name_2">surname:Sáenz-Lechón;given-names:N.</infon><infon key="name_3">surname:Blanco-Velasco;given-names:M.</infon><infon key="name_4">surname:Cruz-Roldán;given-names:F.</infon><infon key="name_5">surname:Ferrer-Ballester;given-names:M.A.</infon><infon key="name_6">surname:V.;given-names:Faundez-Zanuy M. Janer L. Esposito A. Satue-Villar A. Roure J. Espinosa-Duro</infon><infon key="section_type">REF</infon><infon key="series">Lecture Notes in Computer Science</infon><infon key="source">Nonlinear Analyses and Algorithms for Speech Processing</infon><infon key="type">ref</infon><infon key="year">2006</infon><offset>55316</offset></passage><passage><infon key="fpage">5022</infon><infon key="issue">18</infon><infon key="name_0">surname:Asci;given-names:F.</infon><infon key="name_1">surname:Costantini;given-names:G.</infon><infon key="name_2">surname:Di Leo;given-names:P.</infon><infon key="name_3">surname:Zampogna;given-names:A.</infon><infon key="name_4">surname:Ruoppolo;given-names:G.</infon><infon key="name_5">surname:Berardelli;given-names:A.</infon><infon key="name_6">surname:Saggio;given-names:G.</infon><infon key="name_7">surname:Suppa;given-names:A.</infon><infon key="pub-id_doi">10.3390/s20185022</infon><infon key="section_type">REF</infon><infon key="source">Sensors (Basel, Switzerland)</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">2020</infon><offset>55317</offset><text>Machine-learning analysis of voice samples recorded through smartphones: the combined effect of ageing and gender</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55431</offset><text>X. Zhang, L. Zhang, Z. Tao, H. Zhao, Acoustic Characteristics of Normal and Pathological Voices Analysis and Recognition, in: 2019 6th International Conference on Systems and Informatics, ICSAI, 2019, pp. 1423–1427.</text></passage><passage><infon key="fpage">3133</infon><infon key="lpage">3181</infon><infon key="name_0">surname:Delgado;given-names:M.F.</infon><infon key="name_1">surname:Cernadas;given-names:E.</infon><infon key="name_2">surname:Barro;given-names:S.</infon><infon key="name_3">surname:Amorim;given-names:D.G.</infon><infon key="section_type">REF</infon><infon key="source">J. Mach. Learn. Res.</infon><infon key="type">ref</infon><infon key="volume">15</infon><infon key="year">2014</infon><offset>55649</offset><text>Do we need hundreds of classifiers to solve real world classification problems?</text></passage><passage><infon key="comment">Published 2019 Oct 24</infon><infon key="fpage">221</infon><infon key="issue">4</infon><infon key="lpage">226</infon><infon key="name_0">surname:Sheibani;given-names:R.</infon><infon key="name_1">surname:Nikookar;given-names:E.</infon><infon key="name_2">surname:Alavi;given-names:S.E.</infon><infon key="pub-id_doi">10.4103/jmss.JMSS_57_18</infon><infon key="pub-id_pmid">31737550</infon><infon key="section_type">REF</infon><infon key="source">J. Med. Signals Sens.</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2019</infon><offset>55729</offset><text>An ensemble method for diagnosis of parkinson’s disease based on voice measurements</text></passage><passage><infon key="comment">ArXiv, abs/1504.07676</infon><infon key="name_0">surname:Wyner;given-names:A.J.</infon><infon key="name_1">surname:Olson;given-names:M.L.</infon><infon key="name_2">surname:Bleich;given-names:J.</infon><infon key="name_3">surname:Mease;given-names:D.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2017</infon><offset>55815</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>55816</offset><text>J. Thongkam, G. Xu, Y. Zhang, AdaBoost algorithm with random forests for predicting breast cancer survivability, in: 2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence), 2008, pp. 3062–3069.</text></passage><passage><infon key="name_0">surname:Freund;given-names:Y.</infon><infon key="name_1">surname:Schapire;given-names:R.E.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">1996</infon><offset>56069</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>56070</offset><text>H. Parmar, S. Bhanderi, G. Shah, Sentiment Mining of Movie Reviews using Random Forest with Tuned Hyperparameters, in: Conference: International Conference on Information Science, Kerala, 2014.</text></passage><passage><infon key="fpage">5</infon><infon key="lpage">32</infon><infon key="name_0">surname:Breiman;given-names:L.</infon><infon key="pub-id_doi">10.1023/A:1010933404324</infon><infon key="section_type">REF</infon><infon key="source">Mach. Learn.</infon><infon key="type">ref</infon><infon key="volume">45</infon><infon key="year">2001</infon><offset>56264</offset><text>Random forests</text></passage><passage><infon key="comment">2007</infon><infon key="fpage">1305</infon><infon key="lpage">6417</infon><infon key="name_0">surname:Leshem;given-names:G.</infon><infon key="name_1">surname:Ritov;given-names:Y.</infon><infon key="section_type">REF</infon><infon key="source">J. Int. J. Intell. Technol.</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">2007</infon><offset>56279</offset><text>Traffic flow prediction using adaboost algorithm with random forests as a weak learner</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>56366</offset><text>G. Gosztolya, R. Busa-Fekete, T. Grósz, L. Tóth, DNN-Based Feature Extraction and Classifier Combination for Child-Directed Speech, Cold and Snoring Identification, in: INTERSPEECH, 2017.</text></passage><passage><infon key="fpage">604</infon><infon key="lpage">608</infon><infon key="name_0">surname:Bansal;given-names:V.</infon><infon key="name_1">surname:Pahwa;given-names:G.</infon><infon key="name_2">surname:Kannan;given-names:N.</infon><infon key="pub-id_doi">10.1109/GUCON48875.2020.9231094</infon><infon key="section_type">REF</infon><infon key="source">2020 IEEE International Conference on Computing, Power and Communication Technologies</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>56556</offset></passage><passage><infon key="fpage">41</infon><infon key="issue">2018</infon><infon key="lpage">54</infon><infon key="name_0">surname:Cummins;given-names:N.</infon><infon key="name_1">surname:Baird;given-names:A.</infon><infon key="name_2">surname:Schuller;given-names:B.W.</infon><infon key="pub-id_doi">10.1016/j.ymeth.2018.07.007</infon><infon key="pub-id_pmid">30099083</infon><infon key="section_type">REF</infon><infon key="source">Methods</infon><infon key="type">ref</infon><infon key="volume">151</infon><infon key="year">2018</infon><offset>56557</offset><text>Speech analysis for health: Current state-of-the-art and the increasing impact of deep learning</text></passage><passage><infon key="name_0">surname:Nguyen;given-names:H.</infon><infon key="name_1">surname:Nguyen;given-names:C.</infon><infon key="name_2">surname:Ino;given-names:T.</infon><infon key="name_3">surname:Indurkhya;given-names:B.</infon><infon key="name_4">surname:Nakagawa;given-names:M.</infon><infon key="pub-id_doi">10.1016/j.patrec.2018.07.022</infon><infon key="section_type">REF</infon><infon key="source">Pattern Recognit. Lett.</infon><infon key="type">ref</infon><infon key="volume">121</infon><infon key="year">2018</infon><offset>56653</offset><text>Text-independent writer identification using convolutional neural network</text></passage><passage><infon key="fpage">587</infon><infon key="name_0">surname:Monson;given-names:B.B.</infon><infon key="name_1">surname:Hunter;given-names:E.J.</infon><infon key="name_2">surname:Lotto;given-names:A.J.</infon><infon key="name_3">surname:Story;given-names:B.H.</infon><infon key="pub-id_doi">10.3389/fpsyg.2014.00587</infon><infon key="pub-id_pmid">24982643</infon><infon key="section_type">REF</infon><infon key="source">Front. Psychol.</infon><infon key="type">ref</infon><infon key="volume">5</infon><infon key="year">2014</infon><offset>56727</offset><text>The perceptual significance of high-frequency energy in the human voice</text></passage><passage><infon key="comment">ArXiv, abs/1801.00631</infon><infon key="name_0">surname:Marcus;given-names:G.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2018</infon><offset>56799</offset></passage><passage><infon key="comment">ArXiv, abs/1912.07756</infon><infon key="name_0">surname:Nanni;given-names:L.</infon><infon key="name_1">surname:Maguolo;given-names:G.</infon><infon key="name_2">surname:Paci;given-names:M.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>56800</offset></passage><passage><infon key="name_0">surname:Barshooi;given-names:A.H.</infon><infon key="name_1">surname:Amirkhani;given-names:A.</infon><infon key="pub-id_doi">10.1016/j.bspc.2021.103326</infon><infon key="section_type">REF</infon><infon key="source">Biomed. Signal Process. Control</infon><infon key="type">ref</infon><infon key="volume">72</infon><infon key="year">2022</infon><offset>56801</offset><text>A novel data augmentation based on gabor filter and convolutional deep learning for improving the classification of COVID-19 chest X-ray images</text></passage><passage><infon key="name_0">surname:Chenou;given-names:J.</infon><infon key="name_1">surname:Hsieh;given-names:G.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2019</infon><offset>56945</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>56946</offset><text>D.S. Park, W. Chan, Y. Zhang, C. Chiu, B. Zoph, E.D. Cubuk, Q.V. Le, SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition, in: INTERSPEECH, 2019.</text></passage><passage><infon key="comment">arXiv:2012.01926v1 [cs.SD]</infon><infon key="name_0">surname:Pahar;given-names:M.</infon><infon key="name_1">surname:Klopper;given-names:M.</infon><infon key="name_2">surname:Warren;given-names:R.</infon><infon key="name_3">surname:Niesler;given-names:T.</infon><infon key="section_type">REF</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>57119</offset></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>57120</offset><text>D. Kingma, J. Ba, Adam: A Method for Stochastic Optimization, in: International Conference on Learning Representations, 2014.</text></passage><passage><infon key="name_0">surname:Bühlmann;given-names:P.</infon><infon key="name_1">surname:Van De Geer;given-names:S</infon><infon key="pub-id_doi">10.1007/978-3-642-20192-9</infon><infon key="section_type">REF</infon><infon key="series">Springer Series in Statistics</infon><infon key="source">Statistics for High-Dimensional Data</infon><infon key="type">ref</infon><infon key="volume">vol. 9</infon><infon key="year">2011</infon><offset>57246</offset></passage><passage><infon key="fpage">654</infon><infon key="lpage">661</infon><infon key="name_0">surname:Fernandes;given-names:J.</infon><infon key="name_1">surname:Silva;given-names:L.</infon><infon key="name_2">surname:Teixeira;given-names:F.</infon><infon key="name_3">surname:Guedes;given-names:V.</infon><infon key="name_4">surname:Santos;given-names:J.</infon><infon key="name_5">surname:Teixeira;given-names:J.</infon><infon key="pub-id_doi">10.1016/j.procs.2019.12.232</infon><infon key="section_type">REF</infon><infon key="source">Procedia Comput. Sci.</infon><infon key="type">ref</infon><infon key="volume">164</infon><infon key="year">2019</infon><offset>57247</offset><text>Parameters for vocal acoustic analysis - cured database</text></passage><passage><infon key="fpage">4377</infon><infon key="issue">6</infon><infon key="name_0">surname:Bartl-Pokorny;given-names:K.D.</infon><infon key="name_1">surname:Pokorny;given-names:F.B.</infon><infon key="name_10">surname:Schuller;given-names:B.W.</infon><infon key="name_2">surname:Batliner;given-names:A.</infon><infon key="name_3">surname:Amiriparian;given-names:S.</infon><infon key="name_4">surname:Semertzidou;given-names:A.</infon><infon key="name_5">surname:Eyben;given-names:F.</infon><infon key="name_6">surname:Kramer;given-names:E.</infon><infon key="name_7">surname:Schmidt;given-names:F.</infon><infon key="name_8">surname:Schönweiler;given-names:R.</infon><infon key="name_9">surname:Wehler;given-names:M.</infon><infon key="pub-id_doi">10.1121/10.0005194</infon><infon key="pub-id_pmid">34241490</infon><infon key="section_type">REF</infon><infon key="source">J. Acoust. Soc. Am.</infon><infon key="type">ref</infon><infon key="volume">149</infon><infon key="year">2021</infon><offset>57303</offset><text>The voice of COVID-19: Acoustic correlates of infection in sustained vowels</text></passage><passage><infon key="name_0">surname:Tamazin;given-names:M.</infon><infon key="name_1">surname:Gouda;given-names:A.</infon><infon key="name_2">surname:Khedr;given-names:M.</infon><infon key="pub-id_doi">10.3390/app9102166</infon><infon key="section_type">REF</infon><infon key="source">Appl. Sci.</infon><infon key="type">ref</infon><infon key="volume">9</infon><infon key="year">2019</infon><offset>57379</offset><text>Enhanced automatic speech recognition system based on enhancing power-normalized cepstral coefficients</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>57482</offset><text>R. Singh, P. Rao, Spectral Subtraction Speech Enhancement with RASTA Filtering, in: Proceding of National Conference on Communications, NCC, Kanpur, India, 2007, 2007.</text></passage><passage><infon key="fpage">407</infon><infon key="lpage">411</infon><infon key="name_0">surname:Cesarini;given-names:V.</infon><infon key="name_1">surname:Casiddu;given-names:N.</infon><infon key="name_2">surname:Porfirione;given-names:C.</infon><infon key="name_3">surname:Massazza;given-names:G.</infon><infon key="name_4">surname:Saggio;given-names:G.</infon><infon key="name_5">surname:Costantini;given-names:G.</infon><infon key="pub-id_doi">10.1109/MetroInd4.0IoT51437.2021.9488503</infon><infon key="section_type">REF</infon><infon key="source">2021 IEEE International Workshop on Metrology for Industry 4.0 IoT (MetroInd4.0 IoT)</infon><infon key="type">ref</infon><infon key="year">2021</infon><offset>57650</offset></passage><passage><infon key="fpage">1269</infon><infon key="issue">6</infon><infon key="lpage">1275</infon><infon key="name_0">surname:Kent;given-names:R.D.</infon><infon key="name_1">surname:Sufit;given-names:R.L.</infon><infon key="name_2">surname:Rosenbeck;given-names:J.C.</infon><infon key="name_3">surname:Kent;given-names:J.F.</infon><infon key="name_4">surname:Weismer;given-names:G.</infon><infon key="name_5">surname:Martin;given-names:R.E.</infon><infon key="name_6">surname:Brooks;given-names:B.R.</infon><infon key="pub-id_doi">10.1044/jshr.3406.1269</infon><infon key="pub-id_pmid">1787708</infon><infon key="section_type">REF</infon><infon key="source">J. Speech Hear. Res.</infon><infon key="type">ref</infon><infon key="volume">34</infon><infon key="year">1991</infon><offset>57651</offset><text>Speech deterioration in amyotrophic lateral sclerosis: a case study</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>57719</offset><text>G. Saggio, Are Sensors and Data Processing Paving the Way to Completely Non-invasive and Not-painful Medical Tests for Widespread Screening and Diagnosis Purposes?, in: BIODEVICES, 2020, pp. 207–214.</text></passage><passage><infon key="comment">S0892-1997(20)30281-2. Advance online publication</infon><infon key="name_0">surname:Helding;given-names:L.</infon><infon key="name_1">surname:Carroll;given-names:T.L.</infon><infon key="name_2">surname:Nix;given-names:J.</infon><infon key="name_3">surname:Johns;given-names:M.M.</infon><infon key="name_4">surname:LeBorgne;given-names:W.D.</infon><infon key="name_5">surname:Meyer;given-names:D.</infon><infon key="pub-id_doi">10.1016/j.jvoice.2020.07.032</infon><infon key="section_type">REF</infon><infon key="source">J. Voice: Off. J. Voice Found.</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>57921</offset><text>COVID-19 after effects: concerns for singers</text></passage><passage><infon key="name_0">surname:Saggio;given-names:G.</infon><infon key="name_1">surname:Costantini;given-names:G.</infon><infon key="section_type">REF</infon><infon key="source">J. Voice</infon><infon key="type">ref</infon><infon key="year">2020</infon><offset>57966</offset><text>Worldwide healthy adult voice baseline parameters: a comprehensive review</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>58040</offset><text>HERMES Project - https://www.leonardocompany.com/en/news-and-stories-detail/-/detail/hermes-the-telespazio-and-e-geos-solution-responding-to-healthcare-needs.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">ref</infon><offset>58199</offset><text>V. Uloza, E. Padervinskis, A. Vegiene, R. Pribuisiene, V. Saferis, E. Vaiciukynas, A. Gelzinis, A. Verikas, Exploring the feasibility of smart phone microphone for measurement of acoustic voice parameters and voice pathology screening, in: European Archives of Oto-Rhino-Laryngology: Official Journal of the European Federation of Oto-Rhino-Laryngological Societies, EUFOS, 2015.</text></passage><passage><infon key="section_type">REF</infon><infon key="type">title_1</infon><offset>58579</offset><text>Data statement</text></passage><passage><infon key="section_type">REF</infon><infon key="type">paragraph</infon><offset>58594</offset><text>Clinical data and audio files are not publicly available due to privacy and consent restrictions. Moreover, data contain potentially identifying or sensitive patient information. However, they may be made available to research institutions by the authors upon reasonable request.</text></passage></document></collection>
